{"title": "Extracting Opinions Opinion Holders And Topics Expressed In Online News Media Text", "abstract": "This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline. ", "introduction": "The challenge of automatically identifying opinions in text automatically has been the focus of attention in recent years in many different domains such as news articles and product reviews. Various approaches have been adopted in subjectivity detection, semantic orientation detection, review classification and review mining. Despite the successes in identifying opinion expressions and subjective words/phrases, there has been less achievement on the factors closely related to subjectivity and polarity, such as opinion holder, topic of opinion, and inter-topic/inter-opinion relationships. This paper addresses the problem of identifying not only opinions in text but also holders and topics of opinions from online news articles. Identifying opinion holders is important especially in news articles. Unlike product reviews in which most opinions expressed in a review are likely to be opinions of the author of the review, news articles contain different opinions of different opinion holders (e.g. people, organizations, and countries). By grouping opinion holders of different stance on diverse social and political issues, we can have a better understanding of the relationships among countries or among organizations. An opinion topic can be considered as an object an opinion is about. In product reviews, for example, opinion topics are often the product itself or its specific features, such as design and quality (e.g. \u201cI like the design of iPod video\u201d, \u201cThe sound quality is amazing\u201d). In news articles, opinion topics can be social issues, government\u2019s acts, new events, or someone\u2019s opinions. (e.g., \u201cDemocrats in Congress accused vice president Dick Cheney\u2019s shooting accident.\u201d, \u201cShiite leaders accused Sunnis of a mass killing of Shiites in Madaen, south of Baghdad.\u201d) As for opinion topic identification, little research has been conducted, and only in a very limited domain, product reviews. In most approaches in product review mining, given a product (e.g. mp3 player), its frequently mentioned features (e.g. sound, screen, and design) are first collected and then used as anchor points. In this study, we extract opinion topics from news articles. Also, we do not pre-limit topics in advance. We first identify an opinion and then find its holder and topic. We define holder as an entity who holds an opinion, and topic, as what the opinion is about. In this paper, we propose a novel method that employs Semantic Role Labeling, a task of identifying semantic roles given a sentence. We deProceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1\u20138, Sydney, July 2006. c\ufffd2006 Association for Computational Linguistics compose the overall task into the following steps: In this paper, we focus on the first three subtasks. The main contribution of this paper is to present a method that identifies not only opinion holders but also opinion topics. To achieve this goal, we utilize FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics, and then use them for system training. We demonstrate that investigating semantic relations between an opinion and its holder and topic is crucial in opinion holder and topic identification. This paper is organized as follows: Section 2 briefly introduces related work both in sentiment analysis and semantic role labeling. Section 3 describes our approach for identifying opinions and labeling holders and topics by utilizing FrameNet1 data for our task. Section 4 reports our experiments and results with discussions and finally Section 5 concludes. ", "conclusion": "This paper presented a methodology to identify an opinion with its holder and topic given a sentence in online news media texts. We introduced an approach of exploiting semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. Our method first identifies an opinion-bearing word, labels semantic roles related to the word in the sentence, and then finds a holder and a topic of the opinion word among labeled semantic roles. There has been little previous study in identifying opinion holders and topics partly because it requires a great amount of annotated data. To overcome this barrier, we utilized FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics. However, FrameNet has a limited number of words in its annotated corpus. For a broader coverage, we used a clustering technique to predict a most probable frame for an unseen word. Our experimental results showed that our system performs significantly better than the baseline. The baseline system results imply that opinion holder and topic identification is a hard task. We believe that there are many complicated semantic relations between opinion-bearing words and their holders and topics which simple relations such as subject and object relations are not able to capture. In the future, we plan to extend our list of opinion-bearing verbs and adjectives so that we can discover and apply more opinion-related frames. Also, it would be interesting to see how other types of part of speech such as adverbs and nouns affect the performance of the system. ", "summary_sents": ["This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts.", "We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.", "This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet.", "We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.", "For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.", "Our experimental results show that our system performs significantly better than the baseline.", "we identify opinion holders and targets with semantic role labeling."]}
{"title": "Accurate Unlexicalized Parsing", "abstract": "Figure 3: Size and devset performance of the cumulatively annotated models, starting with the markovized baseline. The two columns show the change in the baseline for each annotation introduced, both cumulatively and for each single annotation applied to the baseline in isolation. history models similar in intent to those described in Ron et al. (1994). For variable horizontal histories, we did not split intermediate states below 10 occurrences of a symbol. For example, if the symbol too rare, we would colit to For vertical histories, we used a cutoff which included both frequency and mutual information between the history and the expansions (this was not appropriate for the horizontal because unreliable at such low counts). Figure 2 shows parsing accuracies as well as the number of symbols in each markovization. These symbol counts include all the intermediate states which represent partially completed constituents. The general trend is that, in the absence of further annotation, more vertical annotation is better \u2013 even exhaustive grandparent annotation. This is not true for horizontal markovization, where the variableorder second-order model was superior. The best has an 79.74, already a substantial improvement over the baseline. In the remaining sections, we discuss other annotations which increasingly split the symbol space. Since we expressly do not smooth the grammar, not all splits are guaranteed to be beneficial, and not all sets of useful splits are guaranteed to co-exist well. particular, while markovization is good on its own, it has a large number of states and does not tolerate further splitting well. Therefore, base all further exploration on the ROOT S&quot;ROOT 4: An error which can be resolved with the (incorrect baseline parse shown). grammar. Although it does not necessarily jump out of the grid at first glance, this point represents the best compromise between a compact grammar and useful markov histories. 3 External vs. Internal Annotation The two major previous annotation strategies, parent annotation and head lexicalization, can be seen as instances of external and internal annotation, respectively. Parent annotation lets us indicate an important feature of the external environment of a node which influences the internal expansion of that node. On the other hand, lexicalization is a (radical) method of marking a distinctive aspect of the otherwise hidden internal contents of a node which influence the external distribution. Both kinds of annotation can be useful. To identify split states, we suffixes of the form mark internal content and mark external features. To illustrate the difference, consider unary productions. In the raw grammar, there are many unaries, and once any major category is constructed over a span, most others become constructible as well using unary chains (see Klein and Manning (2001) for discussion). Such chains are rare in real treebank trees: unary rewrites only appear in very specific for example of verbs where an empty, controlled subject. Figure 4 shows an erroneous output of the parser, using the baseline markovized grammar. Intuitively, there are several reasons this parse should be ruled out, but is that the lower which is intended prifor of communication verbs, is not a unary rewrite position (such complements usually have subjects). It would therefore be natural to annotate the trees so as to confine unary productions to the contexts in which they are actually ap- We tried two annotations. First, . NP&quot;S VP&quot;S NP&quot;VP VBD , NN S\u02c6VP . VP\u02c6S QP , NP&quot;VP $ CD CD VBG 444.9 million including , CONJP NP&quot;NP NP&quot;NP $ QP JJ NN , RB RB IN net interest down slightly from CD $ 450.7 million was Revenue $ CD (with a any nonterminal node which has only one child. In isolation, this resulted in an absolute gain of 0.55% (see figure 3). The same sentence, parsed using only the baseline and is parsed correctly, because the in the incorrect parse ends with an very low marked nodes had no siblings with It was similar to solo benefit (0.01% worse), but provided far less marginal benefit on top of later features (none at all on top of our top models), and was One restricted place where external unary annotation was very useful, however, was at the preterminal level, where internal annotation was meaningless. One distributionally salient tag conflation in the Penn treebank is the identification of demonstraand regular determiners based on whether they were only captured this distinction. The same external unary annotation was even more efwhen applied to adverbs disfor example, well Beyond these cases, unary tag marking was detrimen- The 78.86%. 4 Tag Splitting The idea that part-of-speech tags are not fine-grained enough to abstract away from specific-word behaviour is a cornerstone of lexicalization. The for example, showed that the determiners which occur alone are usefully distinguished from those which occur with other nomimaterial. This marks the with a single bit about their immediate external context: whether there are sisters. Given the success of parent annotation for nonterminals, it makes sense to parent antags, as well In fact, as figure 3 shows, exhaustively marking all preterminals with their parent category was the most effective single annotation we tried. Why should this be useful? Most tags have a canonical category. For example, occur under (only 234 of 70855 do not, mostly mistakes). However, when a tag that when we show such trees, we generally only show one annotation on top of the baseline at a time. Moreover, we do not explicitly show the binarization implicit by the horizontal markovization. two are not equivalent even given infinite data. 5: An error resolved with the (of the (a) the incorrect baseline parse and (b) the correct resolves this error. somewhat regularly occurs in a non-canonical position, its distribution is usually distinct. For example, most common adverbs directly under and Under they are and Under and and so on. substantially, to 80.62%. In addition to the adverb case, the Penn tag set conflates various grammatical distinctions that are commonly made in traditional and generative grammar, and from which a parser could hope to get useful information. For example, subordinating conas, complementizers prepositions in, all get the tag of these distinctions are captured by conjunctions occur under under but are not (both subordinating conjunctions and complementizers appear Also, there are exclusively nounprepositions predominantly verbones and so on. The annotation a linguistically motivated 6-way split the and brought the total to 81.19%. Figure 5 shows an example error in the baseline is equally well fixed by either In this case, the more common nominal of preferred unless the is annoto allow prefer We also got value from three other annotations which subcategorized tags for specific lexemes. we split off auxiliary verbs with the which appends all forms all forms of More miconjunction tags to indicate is an extended uniform version of the partial auxiliary annotation of Charniak (1997), wherein all auxiliaries are as a added to gerund auxiliaries and VP&quot;S VP&quot;S TO VP&quot;VP TO&quot;VP VP&quot;VP to VB PP&quot;VP to VB&quot;VP SBAR&quot;VP see NP&quot;PP see IN&quot;SBAR S&quot;SBAR IN (a) (b) NNS NN if VP&quot;S VBZ&quot;VP works works NN&quot;NP advertising advertising or not they were the strings &, each of which have distinctly different distributions from other conjunctions. Finally, we gave the percent sign (%) its own tag, in line with the dollar sign ($) already having its own. Together these three anbrought the 81.81%. 5 What is an Unlexicalized Grammar? Around this point, we must address exactly what we by an To the extent that go about subcategorizing many of them might come to represent a single word. One might thus feel that the approach of this paper is to walk down a slippery slope, and that we are merely arguing degrees. However, we believe that there is a fundamental qualitative distinction, grounded in linguistic practice, between what we see as permitted an unlexicalized against what one finds hopes to exploit in lexicalized The division rests on the traditional distinction between words closed-class words) and open class or lexical words). It is standard practice in linguistics, dating back decades, to annotate phrasal nodes with important functiondistinctions, for example to have a a whereas content words are not part of grammatical structure, and one would not have sperules or constraints for an for example. We follow this approach in our model: various closed classes are subcategorized to better represent important distinctions, and important features commonly expressed by function words are annotated phrasal nodes (such as whether a finite, or a participle, or an infinitive clause). However, no use is made of lexical class words, to provide either or bilexical At any rate, we have kept ourselves honest by estimating our models exclusively by maximum likelihood estimation over our subcategorized grammar, without any form of interpolation or shrinkage to unsubcategorized categories (although we do rules, as explained above). This effecshould be noted that we started with four tags in the Penn tagset that rewrite as a single word: and some of the punctuation tags, which rewrite as barely more. To the extent that we subcategorize tags, there will be more such cases, but many of them already exist in other tag sets. For instance, many tag sets, such as the Brown and tagsets give a separate sets of tags to each form of verbal auxiliaries and most of which rewrite as only a single word (and any corresponding contractions). 6: An error resolved with the (a) incorrect baseline parse and (b) the correct tively means that the subcategories that we break off must themselves be very frequent in the language. In such a framework, if we try to annotate categories with any detailed lexical information, many sentences either entirely fail to parse, or have only extremely weird parses. The resulting battle against sparsity means that we can only afford to make a few distinctions which have major distributional impact. Even with the individual-lexeme annotations in this section, the grammar still has only 9255 states compared to the 7619 of the baseline model. 6 Annotations Already in the Treebank At this point, one might wonder as to the wisdom of stripping off all treebank functional tags, only to heuristically add other such markings back in to the grammar. By and large, the treebank out-of-the tags, such as have negative utility. Recall that the raw treebank gramwith no annotation or markovization, had an of 72.62% on our development set. With the functional annotation left in, this drops to 71.49%. The v markovization baseline of 77.77% dropped even further, all the way to 72.87%, when these annotations were included. Nonetheless, some distinctions present in the raw trees were valuable. For example, an an could be either a temporal a For the annotation we retained the on and, furthermore, propathe tag down to the tag of the head of the This is illustrated in figure 6, which also shows an of its utility, clarifying that last night is not a plausible compound and facilitating the othunusual high attachment of the smaller the cumulative 82.25%. Note that this technique of pushing the functional tags down to preterminals might be useful more generfor example, locative expand roughly the VP\u02c6VP VP\u02c6VP VB to NP\u02c6VP appear NN\u02c6TMP NP\u02c6NP PP\u02c6NP JJ PP\u02c6NP NP\u02c6NP appear NP\u02c6VP VB NP-TMP\u02c6VP to IN NNS last night JJ times three NNP NN on CD on times three last CNN night NP\u02c6PP NNP CNN NP\u02c6PP IN NNS CD (a) (b) ROOT Distance S\u02c6ROOT S\u02c6ROOT 7: An error resolved with the (a) incorrect baseline parse and (b) the correct way as all other (usually as but do tend to have different prepositions below A second kind of information in the original trees is the presence of empty elements. Following (1999), the annotation nodes which have an empty subject (i.e., raising and constructions). This brought 82.28%. 7 Head Annotation The notion that the head word of a constituent can affect its behavior is a useful one. However, often the head tag is as good (or better) an indicator of how constituent will We found several head annotations to be particularly effective. First, poshave a very different distribution than \u2013 in particular, are only used in the treebank when the leftmost child is possessive (as opposed to other imaginable uses like for York which is left flat). To address this, all possessive This brought total 83.06%. Second, the is very overloaded in the Penn treebank, most severely in that there is no distinction between finite and in- An example of the damage this conflation can do is given in figure 7, where one needs to capture the fact that present-tense verbs do not take bare infinitive To allow the finite/non-finite distinction, and other verb distinctions, all with their head tag, merging all finite forms to a sintag In particular, this also accomplished This was extremely bringing the cumulative 85.72%, 2.66% absolute improvement (more than its solo improvement over the baseline). is part of the explanation of why (Charniak, 2000) finds that early generation of head tags as in (Collins, 1999) is so beneficial. The rest of the benefit is presumably in the availability of the tags for smoothing purposes. Error analysis at this point suggested that many remaining errors were attachment level and conjunction scope. While these kinds of errors are undoubtedly profitable targets for lexical preference, most attachment mistakes were overly high attachments, indicating that the overall right-branching tendency of English was not being captured. Indeed, this tenis a difficult trend to capture in a because often the high and low attachments involve the very same rules. Even if not, attachment height is modeled by a it is somehow explicitly encoded into category labels. More complex parsing models have indirectly overcome this by modeling distance (rather than height). distance is difficult to encode in a \u2013 marking nodes with the size of their yields masmultiplies the state Therefore, we wish to find indirect indicators that distinguish high from low ones. In the case of two a with the question of whether the a second modifier of the leftmost should attach lower, inside the first the important distinction is usually that the lower site is a base Collins (1999) captures this by introducing the notion of a base in any dominates only preterminals is with a Further, if an not have non-base it is given one with a unary production. This was helpful, but substantially less than marking base the unary, whose presence actually erased a useful indicator \u2013 base are more frequent in subject position than object position, for example. In isolation, the Collins method actually hurt the base- (absolute cost to 0.37%), while skipping the unary insertion added an absolute 0.73% to the and brought the cumulative 86.04%. the case of attachment of a an eiabove or inside a relative clause, the high is distinct from the low one in that the already modified one contains a verb (and the low one may be base well). This is a partial explanation of the utility of verbal distance in Collins (1999). To inability to encode distance naturally in a naive somewhat ironic. In the heart of any the fundamental table entry or chart item is a label over a span, for exan position 0 to position 5. The concrete use of a grammar rule is to take two adjacent span-marked labels and them (for example and into Yet, only the labels are used to score the combination. \u201c DT \u201c NP\u02c6S VBZ VP\u02c6VP VP\u02c6S ! . \u201d \u201d \u201c \u201c NP\u02c6S DT NP\u02c6VP \u201d . ! \u201d VP\u02c6S-VBF VBZ (a) (b) NP\u02c6VP buying This is This NN NN panic buying LP LR Exact CB 0 CB Magerman (1995) 84.9 84.6 1.26 56.6 Collins (1996) 86.3 85.8 1.14 59.9 this paper 86.9 85.7 86.3 30.9 1.10 60.3 Charniak (1997) 87.4 87.5 1.00 62.1 Collins (1999) 88.7 88.6 0.90 67.1 Figure 8: Results of the final model on the test set (section 23). this, all nodes which any verbal node with a This the cumulative 86.91%. We also tried marking nodes which dominated prepositions and/or conjunctions, but these features did not help the cumulative hill-climb. The final distance/depth feature we used was an explicit attempt to model depth, rather than use distance and linear intervention as a proxy. With we marked all which contained their right periphery (i.e., as a rightmost descendant). This captured some further attachment trends, and brought us to a final develop- 87.04%. 9 Final Results We took the final model and used it to parse section 23 of the treebank. Figure 8 shows the re- The test set 86.32% for words, already higher than early lexicalized models, though of course lower than the state-of-the-art parsers. 10 Conclusion The advantages of unlexicalized grammars are clear enough \u2013 easy to estimate, easy to parse with, and timeand space-efficient. However, the dismal performance of basic unannotated unlexicalized grammars has generally rendered those advantages irrelevant. Here, we have shown that, surprisingly, the maximum-likelihood estimate of a compact unlexiparse on par with early lexicalized parsers. We do not want to argue that lexical selection is not a worthwhile component of a state-ofthe-art parser \u2013 certain attachments, at least, require it \u2013 though perhaps its necessity has been overstated. Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well. ", "introduction": "", "conclusion": "The advantages of unlexicalized grammars are clear enough \u2013 easy to estimate, easy to parse with, and time- and space-efficient. However, the dismal performance of basic unannotated unlexicalized grammars has generally rendered those advantages irrelevant. Here, we have shown that, surprisingly, the maximum-likelihood estimate of a compact unlexicalized PCFG can parse on par with early lexicalized parsers. We do not want to argue that lexical selection is not a worthwhile component of a state-ofthe-art parser \u2013 certain attachments, at least, require it \u2013 though perhaps its necessity has been overstated. Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well. ", "summary_sents": ["We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar.", "Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art.", "This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.", "We also present a manual symbol refinement method."]}
{"title": "Towards History-Based Grammars: Using Richer Models For Probabilistic Parsing", "abstract": "While a different order for these predictions is possible, we only experimented with this one. Parameter Estimation We only have built a decision tree to the rule probability component (3) of the model. For the mowe are using with the usual interpolation smoothing for the other four components of the model. We have assigned bit strings to the syntactic and semantic categories and to the rules manually. Our intention is that bit strings differing in the least significant bit positions correspond to categories of non-terminals or rules that are similar. We also have assigned bitstrings for the words in the vocabulary (the lexical heads) using automatic clustering algorithms using the bigram mutual information clustering algorithm (see (5)). Given the bitsting of a history, we then designed a decision tree for modeling the probability that a rule will be used for rewriting a node in the parse tree. Since the grammar produces parses which may be more detailed than the Treebank, the decision tree was built using a training set constructed in the following manner. Using the grammar with the P-CFG model we determined the most likely parse that is consistent with the Treebank and considered the resulting sentence-tree pair as an event. Note that the grammar parse will also provide the lexical head structure of the parse. Then, we extracted using leftmost derivation order tuples of a history (truncated to the definition of a history in the HBG model) and the corresponding rule used in expanding a node. Using the resulting data set we built a decision tree by classifying histories to locally minimize the entropy of the rule template. With a training set of about 9000 sentencetree pairs, we had about 240,000 tuples and we grew a tree with about 40,000 nodes. This required 18 hours on a 25 MIPS RISC-based machine and the resulting decision tree was nearly 100 megabytes. Immediate vs. Functional Parents model employs two types of parents, the and the The a list Figure 3: Sample representation of &quot;with a list&quot; in HBG model. R: PP1 Syn: PP H1: list with R: NBAR4 Syn: NP Sem: Data H1: list H2: a R: N1 Syn: N Sem: Data H1: list H2: * 35 immediate parent is the constituent that immediately dominates the constituent being predicted. If the immediate parent of a constituent has a different syntactic type from that of the constituent, then the immediate parent is also the functional parent; otherwise, the functional parent is the functional parent of the immediate parent. The distinction between functional parents and immediate parents arises primarily to cope with unit productions. When unit productions of the form XP2 ---> XP1 occur, the immediate parent of XP1 is XP2. But, in general, the constituent XP2 does not contain enough useful information for ambiguity resolution. In particular, when considering only immediate parents, unit rules such as NP2 \u2014\u25a0 NP1 prevent the probabilistic model from allowing the NP1 constituent to interact with the VP rule which is the functional parent of NP1. When the two parents are identical as it often happens, the duplicate information will be ignored. However, when they differ, the decision tree will select that parental context which best resolves ambiguities. Figure 3 shows an example of the representation of a history in HBG for the prepositional phrase &quot;with a list.&quot; In this example, the immediate parent of the Ni node is the NBAR4 node and the functional parent of Ni is the PP1 node. Results We compared the performance of HBG to the &quot;broad-coverage&quot; probabilistic context-free gram- P-CFG. The of the grammar is 90% on test sentences of 7 to 17 words. The of P-CFG is 60% on the same test corpus of 760 sentences used in our experiments. On the same test sentences, the HBG model has a of 75%. This is a reduction of 37% in error rate. Accuracy P-CFG 59.8% HBG 74.6% Error Reduction 36.8% Figure 4: Parsing accuracy: P-CFG vs. HBG In developing HBG, we experimented with similar models of varying complexity. One discovery made during this experimentation is that models which incorporated more context than HBG performed slightly worse than HBG. This suggests that the current training corpus may not contain enough sentences to estimate richer models. Based on the results of these experiments, it appears likely that significantly increasing the size of the training corpus should result in a corresponding improvement in the accuracy of HBG and richer HBG-like models. To check the value of the above detailed history, we tried the simpler model: 1. 2. 3. p(Syn p(Sem ISyn, p(R ISyn, Sem, This model corresponds to a P-CFG with NTs that are the crude syntax and semantic categories with the lexical heads. The in this case was 66%, a small improvement over the P-CFG model indicating the value of using more context from the derivation tree. Conclusions The success of the HBG model encourages future development of general history-based grammars as a more promising approach than the usual P-CFG. More experimentation is needed with a larger Treebank than was used in this study and with different aspects of the derivation history. In addition, this paper illustrates a new approach to grammar development where the parsing problem is divided (and hopefully conquered) into two subproblems: one of grammar coverage for the grammarian to address and the other of statistical modeling to increase the probability of picking the correct parse of a sentence. ", "introduction": "", "conclusion": "The success of the HBG model encourages future development of general history-based grammars as a more promising approach than the usual P-CFG. More experimentation is needed with a larger Treebank than was used in this study and with different aspects of the derivation history. In addition, this paper illustrates a new approach to grammar development where the parsing problem is divided (and hopefully conquered) into two subproblems: one of grammar coverage for the grammarian to address and the other of statistical modeling to increase the probability of picking the correct parse of a sentence. ", "summary_sents": ["We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity.", "HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.", "We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence.", "This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse.", "In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."]}
{"title": "NLTK: The Natural Language Toolkit", "abstract": "NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset. ", "introduction": "Teachers of introductory courses on computational linguistics are often faced with the challenge of setting up a practical programming component for student assignments and projects. This is a difficult task because different computational linguistics domains require a variety of different data structures and functions, and because a diverse range of topics may need to be included in the syllabus. A widespread practice is to employ multiple programming languages, where each language provides native data structures and functions that are a good fit for the task at hand. For example, a course might use Prolog for parsing, Perl for corpus processing, and a finite-state toolkit for morphological analysis. By relying on the built-in features of various languages, the teacher avoids having to develop a lot of software infrastructure. An unfortunate consequence is that a significant part of such courses must be devoted to teaching programming languages. Further, many interesting projects span a variety of domains, and would require that multiple languages be bridged. For example, a student project that involved syntactic parsing of corpus data from a morphologically rich language might involve all three of the languages mentioned above: Perl for string processing; a finite state toolkit for morphological analysis; and Prolog for parsing. It is clear that these considerable overheads and shortcomings warrant a fresh approach. Apart from the practical component, computational linguistics courses may also depend on software for in-class demonstrations. This context calls for highly interactive graphical user interfaces, making it possible to view program state (e.g. the chart of a chart parser), observe program execution step-by-step (e.g. execution of a finite-state machine), and even make minor modifications to programs in response to \u201cwhat if\u201d questions from the class. Because of these difficulties it is common to avoid live demonstrations, and keep classes for theoretical presentations only. Apart from being dull, this approach leaves students to solve important practical problems on their own, or to deal with them less efficiently in office hours. In this paper we introduce a new approach to the above challenges, a streamlined and flexible way of organizing the practical component of an introductory computational linguistics course. We describe NLTK, the Natural Language Toolkit, which we have developed in conjunction with a course we have taught at the University of Pennsylvania. The Natural Language Toolkit is available under an open source license from http://nltk.sf.net/. NLTK runs on all platforms supported by Python, including Windows, OS X, Linux, and Unix. ", "conclusion": "NLTK provides a simple, extensible, uniform framework for assignments, projects, and class demonstrations. It is well documented, easy to learn, and simple to use. We hope that NLTK will allow computational linguistics classes to include more hands-on experience with using and building NLP components and systems. NLTK is unique in its combination of three factors. First, it was deliberately designed as courseware and gives pedagogical goals primary status. Second, its target audience consists of both linguists and computer scientists, and it is accessible and challenging at many levels of prior computational skill. Finally, it is based on an object-oriented scripting language supporting rapid prototyping and literate programming. We plan to continue extending the breadth of materials covered by the toolkit. We are currently working on NLTK modules for Hidden Markov Models, language modeling, and tree adjoining grammars. We also plan to increase the number of algorithms implemented by some existing modules, such as the text classification module. Finding suitable corpora is a prerequisite for many student assignments and projects. We are therefore putting together a collection of corpora containing data appropriate for every module defined by the toolkit. NLTK is an open source project, and we welcome any contributions. Readers who are interested in contributing to NLTK, or who have suggestions for improvements, are encouraged to contact the authors. ", "summary_sents": ["NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware.", "NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora.", "Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.", "NLTK, the Natural Language Toolkit, is a suite of Python modules providing many NLP data types, processing tasks, corpus samples and readers, together with animated algorithms, tutorials and problem sets."]}
{"title": "Manual And Automatic Evaluation Of Machine Translation Between European Languages", "abstract": "Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy . 0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt ", "introduction": "", "conclusion": "We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs. While many systems had similar performance, the results offer interesting insights, especially about the relative performance of statistical and rule-based systems. Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics. The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data. The manual evaluation of scoring translation on a graded scale from 1\u20135 seems to be very hard to perform. Replacing this with an ranked evaluation seems to be more suitable. Human judges also pointed out difficulties with the evaluation of long sentences. ", "summary_sents": ["We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back.", "Evaluation was done automatically using the BLEU score and manually on fluency and adequacy.", "The results of the workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems.", "We report and analyze several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric."]}
{"title": "Anaphora For Everyone: Pronominal Anaphora Resolution Without A Parser", "abstract": "We present an algorithm for anaphora res- olutkm which is a modified and extended version of that developed by (Lappin and Leass,/994). In contrast to that work, our al- gorithm does not require in-depth, full, syn.. tactic parsing of text. Instead, with minimal compromise in output quality, the modifica- tions enable the resolution process to work from tile output of a part of speech tag- ge~; enriched only with annotations of gram- matica\\] functkm of lexical items in the in- put text stream. Evaluation of the results of our in-tplementation demonstrates that ac- curate anaphora resolution can be realized within natural anguage processing fl'ame- works which do not--~,)r cannot- employ ro- bust and rcqiable parsing components. ", "introduction": "", "conclusion": "", "summary_sents": ["We present an algorithm for anaphora resolution which is a modified and extended version of that developed by (Lappin and Leass, 1994).", "In contrast to that work, our algorithm does not require in-depth, full, syntactic parsing of text.", "Instead, with minimal compromise in output quality, the modifications enable the resolution process to work from tile output of a part of speech tagger, enriched only with annotations of grammatical function of lexical items in the in-put text stream.", "Evaluation of the results of our implementation demonstrates that accurate anaphora resolution can be realized within natural language processing frameworks which do not -- cannot -- employ robust and reliable parsing components.", "We also suggest that anaphora resolution is part of the discourse referents resolution."]}
{"title": "Long-Distance Dependency Resolution In Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations", "abstract": "This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks. We extract LFG subcategorisation frames and paths linking LDDreentrancies from f-structures generated automati cally for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text.Unlike (Collins, 1999; Johnson, 2002), in our ap proach resolution of LDDs is done at f-structure (attribute-value structure representations of basicpredicate-argument or dependency structure) with out empty productions, traces and coindexation in CFG parse trees. Currently our best automaticallyinduced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU1051 and 80.24% against the PARC 700 Depen dency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004). ", "introduction": "The determination of syntactic structure is an important step in natural language processing as syntactic structure strongly determines semantic interpretation in the form of predicate-argument struc ture, dependency relations or logical form. For a substantial number of linguistic phenomena such as topicalisation, wh-movement in relative clausesand interrogative sentences, however, there is an important difference between the location of the (surface) realisation of linguistic material and the location where this material should be interpreted semantically. Resolution of such long-distance dependencies (LDDs) is therefore crucial in the determination of accurate predicate-argument struc1Manually constructed f-structures for 105 randomly se lected trees from Section 23 of the WSJ section of the Penn-II Treebankture, deep dependency relations and the construction of proper meaning representations such as log ical forms (Johnson, 2002). Modern unification/constraint-based grammarssuch as LFG or HPSG capture deep linguistic information including LDDs, predicate-argument structure, or logical form. Manually scaling rich uni fication grammars to naturally occurring free text, however, is extremely time-consuming, expensiveand requires considerable linguistic and computa tional expertise. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-craftedLFG is successfully scaled to parse the Penn-II treebank (Marcus et al, 1994) with discriminative (log linear) parameter estimation techniques.The last 20 years have seen continuously increas ing efforts in the construction of parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebankshave been enormously influential in the develop ment of robust, state-of-the-art parsing technology:grammars (or grammatical information) automat ically extracted from treebank resources providethe backbone of many state-of-the-art probabilis tic parsing approaches (Charniak, 1996; Collins, 1999; Charniak, 1999; Hockenmaier, 2003; Kleinand Manning, 2003). Such approaches are attractive as they achieve robustness, coverage and performance while incurring very low grammar devel opment cost. However, with few notable exceptions(e.g. Collins? Model 3, (Johnson, 2002), (Hocken maier, 2003) ), treebank-based probabilistic parsersreturn fairly simple ?surfacey? CFG trees, with out deep syntactic or semantic information. Thegrammars used by such systems are sometimes re 2Or dependency banks.ferred to as ?half? (or ?shallow?) grammars (Johnson, 2002), i.e. they do not resolve LDDs but interpret linguistic material purely locally where it oc curs in the tree. Recently (Cahill et al, 2002) showed how wide-coverage, probabilistic unification grammarresources can be acquired automatically from fstructure-annotated treebanks. Many second gen eration treebanks provide a certain amount of deep syntactic or dependency information (e.g. in the form of Penn-II functional tags and traces) supporting the computation of representations ofdeep linguistic information. Exploiting this in formation (Cahill et al, 2002) implement an automatic LFG f-structure annotation algorithmthat associates nodes in treebank trees with f structure annotations in the form of attribute-valuestructure equations representing abstract predicate argument structure/dependency relations. From the f-structure annotated treebank they automatically extract wide-coverage, robust, PCFG-based LFG approximations that parse new text into trees and f-structure representations. The LFG approximations of (Cahill et al, 2002), however, are only ?half? grammars, i.e. like most of their probabilistic CFG cousins (Charniak, 1996; Johnson, 1999; Klein and Manning, 2003) they do not resolve LDDs but interpret linguistic material purely locally where it occurs in the tree.In this paper we show how finite approxima tions of long distance dependency resolution can be obtained automatically for wide-coverage, robust, probabilistic LFG resources automatically acquired from treebanks. We extract LFG subcategorisation frames and paths linking LDD reentrancies fromf-structures generated automatically for the PennII treebank trees and use them in an LDD resolu tion algorithm to parse new text. Unlike (Collins, 1999; Johnson, 2002), in our approach LDDs are resolved on the level of f-structure representation,rather than in terms of empty productions and coindexation on parse trees. Currently we achieve f structure/dependency f-scores of 80.24 and 80.97for parsing section 23 of the WSJ part of the Penn II treebank, evaluating against the PARC 700 and DCU 105 respectively. The paper is structured as follows: we give a brief introduction to LFG. We outline the automatic f-structure annotation algorithm, PCFG-based LFG grammar approximations and parsing architecturesof (Cahill et al, 2002). We present our subcategorisation frame extraction and introduce the treebank based acquisition of finite approximations of LFG functional uncertainty equations in terms of LDD paths. We present the f-structure LDD resolution algorithm, provide results and extensive evaluation.We compare our method with previous work. Fi nally, we conclude. ", "conclusion": "We presented and extensively evaluated a finiteapproximation of LDD resolution in automatically constructed, wide-coverage, robust, PCFG based LFG approximations, effectively turning the ?half?(or ?shallow?)-grammars presented in (Cahill et al, 2002) into ?full? or ?deep? grammars. In our approach, LDDs are resolved in f-structure, not trees. The method achieves a preds-only f-score of 80.97% for f-structures with the PA-PCFG in the integrated architecture against the DCU 105and 78.4% against the 2,416 automatically gener ated f-structures for the original Penn-II treebanktrees. Evaluating against the PARC 700 Depen dency Bank, the P-PCFG achieves an f-score of 80.24%, an overall improvement of approximately 0.6% on the result reported for the best hand-crafted grammars in (Kaplan et al, 2004). AcknowledgementsThis research was funded by Enterprise Ireland Ba sic Research Grant SC/2001/186 and IRCSET. ", "summary_sents": ["This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks.", "We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text.", "Unlike (Collins, 1999; Johnson, 2002), in our approach resolution of LDDs is done at f-structure (attribute-value structure representations of basic predicate-argument or dependency structure) without empty productions, traces and coindexation in CFG parse trees.", "Currently our best automatically induced grammars achieve 80.97% f-score for f-structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU 1051 and 80.24% against the PARC 700 Dependency Bank (King et al., 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al., 2004).", "Our f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English uses configurational, categorial, function tag and trace information.", "We automatically map c-structures to f-structures by assigning grammatical functions to tree nodes based on their phrasal category, the category of the mother node and their position relative to the local head.", "Our parser automatically annotates input text with c-structure trees and f-structure dependencies, reaching high precision and recall rates."]}
{"title": "Empirical Lower Bounds On The Complexity Of Translational Equivalence", "abstract": "This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts. The study found that the complexity of these patterns in every bitext was higher than suggested in the literature. These findings shed new light on why \u201csyntactic\u201d constraints have not helped to improve statistical translation models, including finitestate phrase-based models, tree-to-string models, and tree-to-tree models. The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order. Instructions for replicating our experiments are at ", "introduction": "Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning. The most common explicit representations of this relation are word alignments between sentences that are translations of each other. The complexity of a given word alignment can be measured by the difficulty of decomposing it into its atomic units under certain constraints detailed in Section 2. This paper describes a study of the distribution of alignment complexity in a variety of bitexts. The study considered word alignments both in isolation and in combination with independently generated parse trees for one or both sentences in each pair. Thus, the study * Thanks to David Chiang, Liang Huang, the anonymous reviewers, and members of the NYU Proteus Project for helpful feedback. This research was supported by NSF grant #\u2019s 0238406 and 0415933. \u2020 SW made most of her contribution while at NYU. is relevant to finite-state phrase-based models that use no parse trees (Koehn et al., 2003), tree-tostring models that rely on one parse tree (Yamada and Knight, 2001), and tree-to-tree models that rely on two parse trees (Groves et al., 2004, e.g.). The word alignments that are the least complex on our measure coincide with those that can be generated by an inversion transduction grammar (ITG). Following Wu (1997), the prevailing opinion in the research community has been that more complex patterns of word alignment in real bitexts are mostly attributable to alignment errors. However, the experiments in Section 3 show that more complex patterns occur surprisingly often even in highly reliable alignments in relatively simple bitexts. As discussed in Section 4, these findings shed new light on why \u201csyntactic\u201d constraints have not yet helped to improve the accuracy of statistical machine translation. Our study used two kinds of data, each controlling a different confounding variable. First, we wanted to study alignments that contained as few errors as possible. So unlike some other studies (Zens and Ney, 2003; Zhang et al., 2006), we used manually annotated alignments instead of automatically generated ones. The results of our experiments on these data will remain relevant regardless of improvements in technology for automatic word alignment. Second, we wanted to measure how much of the complexity is not attributable to systematic translation divergences, both in the languages as a whole (SVO vs. SOV), and in specific constructions (English not vs. French ne... pas). To eliminate this source of complexity of translational equivalence, we used English/English bitexts. We are not aware of any previous studies of word alignments in monolingual bitexts. Even manually annotated word alignments vary in their reliability. For example, annotators sometimes link many words in one sentence to many words in the other, instead of making the effort to tease apart more fine-grained distinctions. A study of such word alignments might say more about the annotation process than about the translational equivalence relation in the data. The inevitable noise in the data motivated us to focus on lower bounds, complementary to Fox (2002), who wrote that her results \u201cshould be looked on as more of an upper bound.\u201d (p. 307) As explained in Section 3, we modified all unreliable alignments so that they cannot increase the complexity measure. Thus, we arrived at complexity measurements that were underestimates, but reliably so. It is almost certain that the true complexity of translational equivalence is higher than what we report. ", "conclusion": "This paper presented evidence of phenomena that can lead to complex patterns of translational equivalence in bitexts of any language pair. There were surprisingly many examples of such patterns that could not be analyzed using binary-branching structures without discontinuities. Regardless of the languages involved, the translational equivalence relations in most real bitexts of non-trivial size cannot be generated by an inversion transduction grammar. The low coverage rates without gaps under the constraints of independently generated monolingual parse trees might be the main reason why \u201csyntactic\u201d constraints have not yet increased the accuracy of SMT systems. Allowing a single gap in bilingual phrases or other types of constituent can improve coverage dramatically. ", "summary_sents": ["This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts.", "The study found that the complexity of these patterns in every bitext was higher than suggested in the literature.", "These findings shed new light on why syntactic constraints have not helped to improve statistical translation models, including finite-state phrase-based models, tree-to-string models, and tree-to-tree models.", "The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order.", "Our methodology measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans to succeed in parsing.", "Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning.", "We argue for the necessity of discontinuous spans (i.e., for a formalism beyond Synchronous CFG) in order for synchronous parsing to cover human-annotated word alignment data under the constraint that rules have a rank of no more than two."]}
{"title": "Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data", "abstract": "This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition. We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data. Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL\u201900 and \u201903 shared task data for the above three NLP tasks, respectively. We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement. In addition, our results are superior to the best reported results for all of the above test collections. ", "introduction": "Today, we can easily find a large amount of unlabeled data for many supervised learning applications in Natural Language Processing (NLP). Therefore, to improve performance, the development of an effective framework for semi-supervised learning (SSL) that uses both labeled and unlabeled data is attractive for both the machine learning and NLP communities. We expect that such SSL will replace most supervised learning in real world applications. In this paper, we focus on traditional and important NLP tasks, namely part-of-speech (POS) tagging, syntactic chunking, and named entity recognition (NER). These are also typical supervised learning applications in NLP, and are referred to as sequential labeling and segmentation problems. In some cases, these tasks have relatively large amounts of labeled training data. In this situation, supervised learning can provide competitive results, and it is difficult to improve them any further by using SSL. In fact, few papers have succeeded in showing significantly better results than state-of-theart supervised learning. Ando and Zhang (2005) reported a substantial performance improvement compared with state-of-the-art supervised learning results for syntactic chunking with the CoNLL\u201900 shared task data (Tjong Kim Sang and Buchholz, 2000) and NER with the CoNLL\u201903 shared task data (Tjong Kim Sang and Meulder, 2003). One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible. This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data. To achieve this, it is paramount to make the SSL method scalable with regard to the size of unlabeled data. We first propose a scalable model for SSL. Then, we apply our model to widely used test collections, namely Penn Treebank (PTB) III data (Marcus et al., 1994) for POS tagging, CoNLL\u201900 shared task data for syntactic chunking, and CoNLL\u201903 shared task data for NER. We used up to 1G-words (one billion tokens) of unlabeled data to explore the performance improvement with respect to the unlabeled data size. In addition, we investigate the performance improvement for \u2018unseen data\u2019 from the viewpoint of unlabeled data coverage. Finally, we compare our results with those provided by the best current systems. The contributions of this paper are threefold. First, we present a simple, scalable, but powerful task-independent model for semi-supervised sequential labeling and segmentation. Second, we report the best current results for the widely used test collections described above. Third, we confirm that the use of more unlabeled data in SSL can really lead to further improvements. ", "conclusion": "We proposed a simple yet powerful semi-supervised conditional model, which we call JESS-CM. It is applicable to large amounts of unlabeled data, for example, at the giga-word level. Experimental results obtained by using JESS-CM incorporating 1Gwords of unlabeled data have provided the current best performance as regards POS tagging, syntactic chunking, and NER for widely used large test collections such as PTB III, CoNLL\u201900 and \u201903 shared task data, respectively. We also provided evidence that the use of more unlabeled data in SSL can lead to further improvements. Moreover, our experimental analysis revealed that it may also induce an improvement in the expected performance for unseen data in terms of the unlabeled data coverage. Our results may encourage the adoption of the SSL method for many other real world applications. ", "summary_sents": ["This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition.", "We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data.", "Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL\u201900 and \u201903 shared task data for the above three NLP tasks, respectively.", "We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement.", "In addition, our results are superior to the best reported results for all of the above test collections.", "We run a baseline discriminative classifier on unlabeled data to generate pseudo examples, which are then used to train a different type of classifier for the same problem.", "We use the automatically labeled corpus to train HMMs."]}
{"title": "An Efficient Implementation Of A New DOP Model", "abstract": "Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. This paper proposes an integration of the two models which outperforms each of them separately. Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence. ", "introduction": "The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments, without imposing any constraints on the size of these fragments. Fragments include, for instance, subtrees of depth 1 (corresponding to context-free rules) as well as entire trees. To appreciate these innovations, it should be noted that the model was radically different from all other statistical parsing models at the time. Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g. in Fujisaki et al. 1989; Black et al. 1992, 1993; Briscoe and I Thanks to Ivan Sag for this pun. Waegner 1992; Pereira and Schabes 1992). The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar. This approach has now gained wide usage, as exemplified by the work of Collins (1996, 1999), Charniak (1996, 1997), Johnson (1998), Chiang (2000), and many others. The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset. This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a). The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999; Charniak 2000; Goodman 1998). And Collins (2000) argues for &quot;keeping track of counts of arbitrary fragments within parse trees&quot;, which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992). Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a). One instantiation of DOP which has received considerable interest is the model known as DOP12 (Bod 1992). DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition). Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules. However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations. Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002). Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse. Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar. While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the &quot;maximum constituents parse&quot;, i.e. the parse tree which is most likely to have the largest number of correct constituents. Johnson (1998b, 2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent. Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization. This resulted in a statistically consistent model dubbed ML-DOP. However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from. Cross-validation is needed to avoid this problem. But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b). Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees. As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains. Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data. Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003). Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees, reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ. Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions. This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ. We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t. Bod (2001, 2003). But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996). In the second part of this paper, we extend our experiments with a new notion of the best parse tree. Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation. We show that a combination of a probabilistic and a simplicity metric, which chooses the simplest parse from the n likeliest parses, outperforms the use of these metrics alone. Compared to Bod (2001), our results show an 11% improvement in terms of relative error reduction and a speedup which reduces the processing time from 220 to 3.6 seconds per WSJ sentence. ", "conclusion": "As our second experimental goal, we compared the models SL-DOP and LS-DOP explained in Section 3.2. Recall that for n=1, SL-DOP is equal to the PCFG-reduction of Bod (2001) (which we also called Likelihood-DOP) while LS-DOP is equal to Simplicity-DOP. Table 2 shows the results for sentences 100 words for various values of n. Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1. Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. While the first feature has been generally adopted in statistical NLP, the second feature has for a long time been a serious bottleneck, as it results in exponential processing time when the most probable parse tree is computed. This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank. This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000). ", "summary_sents": ["Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.", "This paper proposes an integration of the two models which outperforms each of them separately.", "Together with a PCFG-reduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank.", "Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.", "We note that it is the highest ranking parse, not derivation, that is desired.", "We show that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well.", "We redress subtree probabilit by a simple correction factor."]}
{"title": "Better Word Alignments with Supervised ITG Models", "abstract": "This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. ", "introduction": "Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG approaches offer several advantages over general matchings. First, the additional structural constraint can result in superior alignments. We confirm and extend this result, showing that one-toone ITG models can perform as well as, or better than, general one-to-one matching models, either using heuristic weights or using rich, learned features. A second advantage of ITG approaches is that they admit a range of training options. As with general one-to-one matchings, we can optimize margin-based objectives. However, unlike with general matchings, we can also efficiently compute expectations over the set of ITG derivations, enabling the training of conditional likelihood models. A major challenge in both cases is that our training alignments are often not one-to-one ITG alignments. Under such conditions, directly training to maximize margin is unstable, and training to maximize likelihood is ill-defined, since the target alignment derivations don\u2019t exist in our hypothesis class. We show how to adapt both margin and likelihood objectives to learn good ITG aligners. In the case of likelihood training, two innovations are presented. The simple, two-rule ITG grammar exponentially over-counts certain alignment structures relative to others. Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. We extend this normal form to null productions and give the first extensive empirical comparison of simple and normal form ITGs, for posterior decoding under our likelihood models. Additionally, we show how to deal with training instances where the gold alignments are outside of the hypothesis class by instead optimizing the likelihood of a set of minimum-loss alignments. Perhaps the greatest advantage of ITG models is that they straightforwardly permit blockstructured alignments (i.e. phrases), which general matchings cannot efficiently do. The need for block alignments is especially acute in ChineseEnglish data, where oracle AERs drop from 10.2 without blocks to around 1.2 with them. Indeed, blocks are the primary reason for gold alignments being outside the space of one-to-one ITG alignments. We show that placing linear potential functions on many-to-one blocks can substantially improve performance. Finally, to scale up our system, we give a combination of pruning techniques that allows us to sum ITG alignments two orders of magnitude faster than naive inside-outside parsing. All in all, our discriminatively trained, block ITG models produce alignments which exhibit the best AER on the NIST 2002 Chinese-English alignment data set. Furthermore, they result in a 1.1 BLEU-point improvement over GIZA++ alignments in an end-to-end Hiero (Chiang, 2007) machine translation system. ", "conclusion": "This work presented the first large-scale application of ITG to discriminative word alignment. We empirically investigated the performance of conditional likelihood training of ITG word aligners under simple and normal form grammars. We showed that through the combination of relaxed learning objectives, many-to-one block alignment potential, and efficient pruning, ITG models can yield state-of-the art word alignments, even when the underlying gold alignments are highly nonITG. Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance. ", "summary_sents": ["This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints.", "We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations.", "Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives.", "For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing.", "Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models.", "Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.", "We describe a pruning heuristic that results in average case runtime of O (n 3)."]}
{"title": "Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models", "abstract": "This article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar. The models are \u201cfull\u201d parsing models in the sense that probabilities are defined for complete parses, rather than for independent events derived by decomposing the parse tree. Discriminative training is used to estimate the models, which requires incorrect parses for each sentence in the training data as well as the correct parse. The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank. The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement (up to 25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm running on a Beowulf cluster. Dynamic programming over a packed chart, in combination with the parallel implementation, allows us to solve one of the largest-scale estimation problems in the statistical parsing literature in under three hours. A key component of the parsing system, for both training and testing, is a Maximum Entropy supertagger which assigns CCG lexical categories to words in a sentence. The supertagger makes the discriminative training feasible, and also leads to a highly efficient parser. Surprisingly, given CCG\u2019s \u201cspurious ambiguity,\u201d the parsing speeds are significantly higher than those reported for comparable parsers in the literature. We also extend the existing parsing techniques for CCG by developing a new model and efficient parsing algorithm which exploits all derivations, including CCG\u2019s nonstandard derivations. This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate\u2013argument dependencies from CCGbank. The parser is also evaluated on DepBank and compared against the RASP parser, outperforming RASP overall and on the majority of relation types. The evaluation on DepBank raises a number of issues regarding parser evaluation. This article provides a comprehensive blueprint for building a wide-coverage CCG parser. We demonstrate that both accurate and highly efficient parsing is possible with CCG. ", "introduction": "", "conclusion": "", "summary_sents": ["This article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar.", "The models are \"full\" parsing models in the sense that probabilities are defined for complete parses, rather than for independent events derived by decomposing the parse tree.", "Discriminative training is used to estimate the models, which requires incorrect parses for each sentence in the training data as well as the correct parse.", "The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank.", "The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement (up to 25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm running on a Beowulf cluster.", "Dynamic programming over a packed chart, in combination with the parallel implementation, allows us to solve one of the largest-scale estimation problems in the statistical parsing literature in under three hours.", "A key component of the parsing system, for both training and testing, is a Maximum Entropy supertagger which assigns CCG lexical categories to words in a sentence.", "The supertagger makes the discriminative training feasible, and also leads to a highly efficient parser.", "Surprisingly, given CCG\u2019s 'spurious ambiguity,' the parsing speeds are significantly higher than those reported for comparable parsers in the literature.", "We also extend the existing parsing techniques for CCG by developing a new model and efficient parsing algorithm which exploits all derivations, including CCG\u2019s nonstandard derivations.", "This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate\u2013argument dependencies from CCGbank.", "The parser is also evaluated on DepBank and compared against the RASP parser, outperforming RASP overall and on the majority of relation types.", "The evaluation on DepBank raises a number of issues regarding parser evaluation.", "This article provides a comprehensive blueprint for building a wide-coverage CCG parser.", "We demonstrate that both accurate and highly efficient parsing is possible with CCG.", "From a parsing perspective, the C & C parser has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al, 2009)."]}
{"title": "Formalism-Independent Parser Evaluation with CCG and DepBank", "abstract": "A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output. Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. this paper we evaluate a on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations. In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%. We compare the against the outperformover 5% overall and on the majority of dependency types. ", "introduction": "Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot be compared with the dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the output of a parser from one representation to another. Second, we develop a method for measuring how effective the conversion process is, which also provides an upper bound for the performance of the parser, given the conversion process being used; this method can be adapted by other researchers to strengthen their own parser comparisons. And third, we provide the first evaluation of a widecoverage CCG parser outside of CCGbank, obtaining impressive results on DepBank and outperforming the RASP parser (Briscoe et al., 2006) by over 5% overall and on the majority of dependency types. ", "conclusion": "A contribution of this paper has been to highlight the difficulties associated with cross-formalism parser comparison. Note that the difficulties are not unique to CCG, and many would apply to any crossformalism comparison, especially with parsers using automatically extracted grammars. Parser evaluation has improved on the original Parseval measures (Carroll et al., 1998), but the challenge remains to develop a representation and evaluation suite which can be easily applied to a wide variety of parsers and formalisms. Despite the difficulties, we have given the first evaluation of a CCG parser outside of CCGbank, outperforming the RASP parser by over 5% overall and on the majority of dependency types. Can the CCG parser be compared with parsers other than RASP? Briscoe and Carroll (2006) give a rough comparison of RASP with the Parc LFG parser on the different versions of DepBank, obtaining similar results overall, but they acknowledge that the results are not strictly comparable because of the different annotation schemes used. Comparison with Penn Treebank parsers would be difficult because, for many constructions, the Penn Treebank trees and CCG derivations are different shapes, and reversing the mapping Hockenmaier used to create CCGbank would be very difficult. Hence we challenge other parser developers to map their own parse output into the version of DepBank used here. One aspect of parser evaluation not covered in this paper is efficiency. The CCG parser took only 22.6 seconds to parse the 560 sentences in DepBank, with the accuracy given earlier. Using a cluster of 18 machines we have also parsed the entire Gigaword corpus in less than five days. Hence, we conclude that accurate, large-scale, linguistically-motivated NLP is now practical with CCG. ", "summary_sents": ["A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output.", "Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance.", "In this paper we evaluate a CCG parser on DepBank, and demonstrate the difficulties in converting the parser output into DepBank grammatical relations.", "In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy.", "The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%.", "We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types.", "We develop a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998).", "We demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009)."]}
{"title": "Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars", "abstract": "One of the reasons nonparametric Bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities. Adaptor grammars are a framework for defining a variety of hierarchical nonparametric Bayesian models. This paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures, and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task. With appropriate adaptor grammars and inference procedures we achieve an 87% word token f-score on the standard Brent version of the Bernstein- Ratner corpus, which is an error reduction of over 35% over the best previously reported results for this corpus. ", "introduction": "Most machine learning algorithms used in computational linguistics are parametric, i.e., they learn a numerical weight (e.g., a probability) associated with each feature, where the set of features is fixed before learning begins. Such procedures can be used to learn features or structural units by embedding them in a \u201cpropose-and-prune\u201d algorithm: a feature proposal component proposes potentially useful features (e.g., combinations of the currently most useful features), which are then fed to a parametric learner that estimates their weights. After estimating feature weights and pruning \u201cuseless\u201d low-weight features, the cycle repeats. While such algorithms can achieve impressive results (Stolcke and Omohundro, 1994), their effectiveness depends on how well the feature proposal step relates to the overall learning objective, and it can take considerable insight and experimentation to devise good feature proposals. One of the main reasons for the recent interest in nonparametric Bayesian inference is that it offers a systematic framework for structural inference, i.e., inferring the features relevant to a particular problem as well as their weights. (Here \u201cnonparametric\u201d means that the models do not have a fixed set of parameters; our nonparametric models do have parameters, but the particular parameters in a model are learned along with their values). Dirichlet Processes and their associated predictive distributions, Chinese Restaurant Processes, are one kind of nonparametric Bayesian model that has received considerable attention recently, in part because they can be composed in hierarchical fashion to form Hierarchical Dirichlet Processes (HDP) (Teh et al., 2006). Lexical acquisition is an ideal test-bed for exploring methods for inferring structure, where the features learned are the words of the language. (Even the most hard-core nativists agree that the words of a language must be learned). We use the unsupervised word segmentation problem as a test case for evaluating structural inference in this paper. Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008). In a computational linguistics setting it is natural to try to align the HDP hierarchy with the hierarchy defined by a grammar. Adaptor grammars, which are one way of doing this, make it easy to explore a wide variety of HDP grammar-based models. Given an appropriate adaptor grammar, the features learned by adaptor grammars can correspond to linguistic units such as words, syllables and collocations. Different adaptor grammars encode different assumptions about the structure of these units and how they relate to each other. A generic adaptor grammar inference program infers these units from training data, making it easy to investigate how these assumptions affect learning (Johnson, 2008).1 However, there are a number of choices in the design of adaptor grammars and the associated inference procedure. While this paper studies the impact of these on the word segmentation task, these choices arise in other nonparametric Bayesian inference problems as well, so our results should be useful more generally. The rest of this paper is organized as follows. The next section reviews adaptor grammars and presents three different adaptor grammars for word segmentation that serve as running examples in this paper. Adaptor grammars contain a large number of adjustable parameters, and Section 3 discusses how these can be estimated using Bayesian techniques. Section 4 examines several implementation options within the adaptor grammar inference algorithm and shows that they can make a significant impact on performance. Cumulatively these changes make a significant difference in word segmentation accuracy: our final adaptor grammar performs unsupervised word segmentation with an 87% token f-score on the standard Brent version of the Bernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent and Cartwright, 1996), which is an error reduction of over 35% compared to the best previously reported results on this corpus. ", "conclusion": "This paper has examined adaptor grammar inference procedures and their effect on the word segmentation problem. Some of the techniques investigated here, such as batch versus incremental initialization, are quite general and may be applicable to a wide range of other algorithms, but some of the other techniques, such as table label resampling, are specialized to nonparametric hierarchical Bayesian inference. We\u2019ve shown that sampling adaptor hyperparameters is feasible, and demonstrated that this improves word segmentation accuracy of the collocation-syllable adaptor grammar by almost 10%, corresponding to an error reduction of over 35% compared to the best results presented in Johnson (2008). We also described and investigated table label resampling, which dramatically improves the effectiveness of Gibbs sampling estimators for complex adaptor grammars, and makes it possible to work with adaptor grammars with complex hierarchical structure. ", "summary_sents": ["One of the reasons nonparametric Bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities.", "Adaptor grammars are a framework for defining a variety of hierarchical nonparametric Bayesian models.", "This paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures, and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task.", "With appropriate adaptor grammars and inference procedures we achieve an 87% word token f-score on the standard Brent version of the Bernstein-Ratner corpus, which is an error reduction of over 35% over the best previously reported results for this corpus.", "We find that the adaptor grammar with syllable structure phontactic constraints and three levels of collocational structure yields the highest word segmentation token f-score."]}
{"title": "A Word-To-Word Model Of Translational Equivalence", "abstract": "Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of inducing or applying a full translation model. For these applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level . The model's precision/recall trade-off can be directly controlled via one threshold parameter. This feature makes the model more suitable for applications that are not fully statistical. The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as partof-speech, dictionaries, word order, etc.. Our model can link word tokens in parallel texts as well as other translation models in the literature. Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. ", "introduction": "Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a). However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply. Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including &quot;crummy&quot; MT on the World Wide Web (Church & Hovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard & Dorr, 1996). In this paper, we present a fast method for inducing accurate translation lexicons. The method assumes that words are translated one-to-one. This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models. Section 3.1 also shows how the oneto-one assumption enables us to use a new greedy competitive linking algorithm for re-estimating the model's parameters, instead of more expensive algorithms that consider a much larger set of word correspondence possibilities. The model uses two hidden parameters to estimate the confidence of its own predictions. The confidence estimates enable direct control of the balance between the model's precision and recall via a simple threshold. The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy. ", "conclusion": "Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of modeling the full range of translation phenomena. For these applications, we have designed a fast algorithm for estimating word-to-word models of translational equivalence. The estimation method uses a pair of hidden parameters to measure the model's uncertainty, and avoids making decisions that it's not likely to make correctly. The hidden parameters can be conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge. So far we have only implemented a two-class model, to exploit the differences in translation consistency between content words and function words. This relatively simple two-class model linked word tokens in parallel texts as accurately as other translation models in the literature, despite being trained on only one fifth as much data. Unlike other translation models, the word-to-word model can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy (Melamed, 1997). Another interesting extension is to broaden the definition of a &quot;word&quot; to include multi-word lexical units (Smadja, 1992). If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena. ", "summary_sents": ["Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of inducing or applying a full translation model.", "For these applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level. The model's precision/recall trade-off can be directly controlled via one threshold parameter.", "This feature makes the model more suitable for applications that are not fully statistical.", "The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as part-of-speech, dictionaries, word order, etc..", "Our model can link word tokens in parallel texts as well as other translation models in the literature.", "Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy.", "We propose the Competitive Linking Algorithm for linking word pairs and a method which calculates the optimized correspondence level between the word pairs by hill climbing.", "One problem that arises in word-to-word alignment is as follows: if e1 is the translation of f1 and f2 has a strong monolingual association with f1, e1 and f2 will also have a strong correlation."]}
{"title": "Named Entity Transliteration And Discovery From Multilingual Comparable Corpora", "abstract": "Named Entity recognition (NER) is an important part of many natural language processing tasks. Most current approaches employ machine learning techniques and require supervised data. However, many languages lack such resources. This paper presents an algorithm to automatically discover Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language. We observe that NEs have similar time distributions across such corpora, and that they are often transliterated, and develop an algorithm that exploits both iteratively. The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration. We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian. ", "introduction": "Named Entity recognition has been getting much attention in NLP research in recent years, since it is seen as a significant component of higher level NLP tasks such as information distillation and question answering, and an enabling technology for better information access. Most successful approaches to NER employ machine learning techniques, which require supervised training data. However, for many languages, these resources do not exist. Moreover, it is often difficult to find experts in these languages both for the expensive annotation effort and even for language specific clues. On the other hand, comparable multilingual data (such as multilingual news streams) are increasingly available (see section 4). In this work, we make two independent observations about Named Entities encountered in such corpora, and use them to develop an algorithm that extracts pairs of NEs across languages. Specifically, given a bilingual corpora that is weakly temporally aligned, and a capability to annotate the text in one of the languages with NEs, our algorithm identifies the corresponding NEs in the second language text, and annotates them with the appropriate type, as in the source text. The first observation is that NEs in one language in such corpora tend to co-occur with their counterparts in the other. E.g., Figure 1 shows a histogram of the number of occurrences of the word Hussein and its Russian transliteration in our bilingual news corpus spanning years 2001 through late 2005. One can see several common peaks in the two histograms, largest one being around the time of the beginning of the war in Iraq. The word Russia, on the other hand, has a distinctly different temporal signature. We can exploit such weak synchronicity of NEs across languages as a way to associate them. In order to score a pair of entities across languages, we compute the similarity of their time distributions. The second observation is that NEs are often transliterated or have a common etymological origin across languages, and thus are phonetically similar. Figure 2 shows an example list of NEs and their possible Russian transliterations. Approaches that attempt to use these two characteristics separately to identify NEs across languages would have significant shortcomings. Transliteration based approaches require a good model, typically handcrafted or trained on a clean set of transliteration pairs. On the other hand, time sequence similarity based approaches would incorrectly match words which happen to have similar time signatures (e.g. Taliban and Afghanistan in recent news). We introduce an algorithm we call co-ranking which exploits these observations simultaneously to match NEs on one side of the bilingual corpus to their counterparts on the other. We use a Discrete Fourier Transform (Arfken, 1985) based metric for computing similarity of time distributions, and we score NEs similarity with a linear transliteration model. For a given NE in one language, the transliteration model chooses a top ranked list of candidates in another language. Time sequence scoring is then used to re-rank the candidates and choose the one best temporally aligned with the NE. That is, we attempt to choose a candidate which is both a good transliteration (according to the current model) and is well aligned with the NE. Finally, pairs of NEs and the best candidates are used to iteratively train the transliteration model. A major challenge inherent in discovering transliterated NEs is the fact that a single entity may be represented by multiple transliteration strings. One reason is language morphology. For example, in Russian, depending on a case being used, the same noun may appear with various endings. Another reason is the lack of transliteration standards. Again, in Russian, several possible transliterations of an English entity may be acceptable, as long as they are phonetically similar to the source. Thus, in order to rely on the time sequences we obtain, we need to be able to group variants of the same NE into an equivalence class, and collect their aggregate mention counts. We would then score time sequences of these equivalence classes. For instance, we would like to count the aggregate number of occurrences of Herzegovina, Hercegovina on the English side in order to map it accurately to the equivalence class of that NE\u2019s variants we may see on the Russian side of our corpus (e.g. ). One of the objectives for this work was to use as little of the knowledge of both languages as possible. In order to effectively rely on the quality of time sequence scoring, we used a simple, knowledge poor approach to group NE variants for Russian. In the rest of the paper, whenever we refer to a Named Entity, we imply an NE equivalence class. Note that although we expect that better use of language specific knowledge would improve the results, it would defeat one of the goals of this work. ", "conclusion": "We have proposed a novel algorithm for cross lingual NE discovery in a bilingual weakly temporally aligned corpus. We have demonstrated that using two independent sources of information (transliteration and temporal similarity) together to guide NE extraction gives better performance than using either of them alone (see Figure 3). We developed a linear discriminative transliteration model, and presented a method to automatically generate features. For time sequence matching, we used a scoring metric novel in this domain. As supported by our own experiments, this method outperforms other scoring metrics traditionally used (such as cosine (Salton and McGill, 1986)) when corpora are not well temporally aligned. In keeping with our objective to provide as little language knowledge as possible, we introduced a simplistic approach to identifying transliteration equivalence classes, which sometimes produced erroneous groupings (e.g. an equivalence class for NE lincoln in Russian included both lincoln and lincolnshire on Figure 6). This approach is specific to Russian morphology, and would have to be altered for other languages. For example, for Arabic, a small set of prefixes can be used to group most NE variants. We expect that language specific knowledge used to discover accurate equivalence classes would result in performance improvements. Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP). ", "summary_sents": ["Named Entity recognition (NER) is an important part of many natural language processing tasks.", "Most current approaches employ machine learning techniques and require supervised data.", "However, many languages lack such resources.", "This paper presents an algorithm to automatically discover Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language.", "We observe that NEs have similar time distributions across such corpora, and that they are often transliterated, and develop an algorithm that exploits both iteratively.", "The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.", "We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian.", "Character unigrams and bigrams are used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model."]}
{"title": "Minimally Supervised Morphological Analysis By Multimodal Alignment", "abstract": "with genetic algorithms. Workshop on Empirical Learning of NLP Tasks. K. Koskenniemi, 1983. A general computation model word-form recognition and production. 11, of General Linguistics. of Helsinki. C.X. Ling, 1994. Learning the past tense of English verbs: The symbolic pattern associator vs. connecmodels. Art. Intel. Res., R. Mooney and M. Califf, 1995. Induction of firstorder decision lists: Results on learning the past of English verbs. Art. Intel. Res., K. Oflazer and S. Nirenburg, 1999. Practical bootof morphological analyzers. of the Conference on Natural Language Learning. D. Rumelhart and J. McClelland, 1986. On learning the past tense of English verbs. In J. McClel- D. Rumelhart, and the Group, Parallel distributed processing: Explorations in the of cognition, 2. MIT Press. P. Theron and I. Cloete, 1997. Automatic acquisition two-level morphological rules. of the Fifth Conference on Applied Natural Language Propages 103-110. A. Voutilainen, 1995. Morphological disambiguation. In F. Karlsson, A. Voutilainen, J. Heikkila, and A. (eds.) grammar - A language independent system for parsing unrestricted text, pages 165-284. The Hague: Mouton de Gruyter. ", "introduction": "", "conclusion": "This paper has presented an original algorithm capable of inducing the accurate morphological analysis of even highly irregular verbs, starting with no paired <inflection,root> examples for training and no prior seeding of legal morphological transformations. It does so by treating morphological analysis predominantly as an alignment task in a large corpus, performing the effective collaboration of four original similarity measures based on expected frequency distributions, context, morphologically-weighted Levenshtein similarity and an iteratively bootstrapped model of affixation and stem-change probabilities. This constitutes a significant achievement in that previous approaches to morphology acquisition have either focused on unsupervised induction of quasiregular concatenative affixation, or handled irregular forms with fully supervised training. In contrast, this paper's essentially unsupervised algorithm achieves over 80% accuracy on the most highly irregular forms, and 99.7% accuracy on analyses requiring some stem change, outperforming Mooney and Califf's fully supervised learning algorithm overall and on both of these measures. alignment principles used, as it creates nearly as many problems as it fixes. The overall performance advantage is slightly in its favor (with 59 misalignments avoided for 50 problems created), but the cost of this approach is borne heavily by the irregular verbs, and a probabilistic model of when variant forms should be expected/allowed is necessary to fix these cases while preserving the advantages of the principle in downweighting clashing analyses in the more regular verbs. Test True (Convg) CS+FS+LS+MS (Itr 1) CS+FS+LS CS+FS LS only Word Root Score (Itr 1) (Itr 1) (Itr 1) got get go 1.30 go go go gut knew know know 1.35 know know know know took take take 1.50 take take take toot blew blow blow 1.80 blow blow blow blow became become become 2.35 become become become become made make make 2.40 make make make mate clung cling cling 2.55 cling cling cling cling drew draw draw 2.65 draw draw draw draw swore swear swear 2.80 swear swear swear store wore wear wear 3.10 wear wear wear wire came come come 3.55 come come come come thought think think 3.60 think think think thump flung fling fling 4.60 fling fling fling fling brought bring bring 5.35 bring bring bring brighten strove strive strive 5.85 strive strive straddle strive stuck stick stick 6.00 stick stick stabilize stock swept sweep sweep 6.20 sweep sweep sweep swap shone shine shine 6.55 shine shine shine shine woke wake wake 6.95 wake wake wind wake clove cleave cleave 7.35 cleave cleave cleave close bore bear bear 7.75 bear bar bear bare meant mean mean 8.20 mean mean manage mount lent lend lend 9.25 lend lend lend lend slew slay slit 10.06 slit slight slight slow struck strike strike 11.60 strike strike strike strut bought buy buy 12.20 buy buy buy bound bit bite bite 13.60 bite bite betray bet dove dive dive 17.25 dive dive dash dive burnt burn burp 17.30 burp burp burp burn went go want 18.29 want want want want caught catch catch 18.35 catch cut catch cough dealt deal deal 21.45 deal deal disagree deal ", "summary_sents": ["This paper presents a corpus-based algorithm capable of inducing inflectional morphological analyses of both regular and highly irregular forms (such as brought\u2192bring) from distributional patterns in large monolingual text with no direct supervision.", "The algorithm combines four original alignment models based on relative corpus frequency, contextual similarity, weighted string similarity and incrementally retrained inflectional transduction probabilities.", "Starting with no paired <inflection,root> examples for training and no prior seeding of legal morphological transformations, accuracy of the induced analyses of 3888 past-tense test cases in English exceeds 99.2% for the set, with currently over 80% accuracy on the most highly irregular forms and 99.7% accuracy on forms exhibiting non-concatenative suffixation.", "We obtain outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language's inflectional parts of speech, and the canonical suffixes for each part of speech.", "We propose an algorithm that extracts morphological rules relating roots and inflected forms of verbs (but the algorithm can be extended to other morphological relations).", "The supervised morphological learner presented in this paper models lemmatization as a word-final stem change plus a suffix taken from a (possibly empty) list of potential suffixes."]}
{"title": "Inference In DATR", "abstract": "a declarative language for representing a restricted class of inheritance networks, permitting both multiple and default inheritance. The principal intended area of application is the representation of lexical entries for natural language processing, and we use examples from this domain throughout. In this paper we present the syntax and inference mechanisms for language. The goal of the is the design of a simple language that (i) has the necessary expressive power to encode the lexical entries presupposed by contemporary work in the unification grammar tradition, (ii) can express all the evident generalizations about such entries, (iii) has an explicit theory of inference, (iv) is computationally tractable, and (v) has an explicit declarative semantics. The present paper is primarily concerned with (iii), though the examples used may hint at our strategy in respect of (i) and (ii). ", "introduction": "Inheritance networks (&quot;semantic nets&quot;) provide an intuitively appealing way of thinking about the representation of various kinds of knowledge. This fact has not gone unnoticed by a number of researchers working on lexical knowledge representation, e.g. de Smedt (1984), Flickinger et al. (1985), Calder & te Linden (1987), Daelemans (1987a,1987b), Gazdar (1987) and Calder (1989). However, many such networks have been realized in the context of programming systems or programming languages that leave their precise meaning unclear. In the light of Braclunan (1985), Ether. ington (1988) and much other recent work, it ha become apparent that the formal properties oi notations intended to represent inheritance arc highly problematic. Although not discussec here, DATR has a formal semantics (Evans & Gazdar 1989) for which some completeness anc soundness results have been derived. These results, and others (on complexity, for example; will be provided in a subsequent paper. There are several prototype computational implementa. tions of the language, and non-trivial lexicor fragments for English, German and Latin have been developed and tested. ", "conclusion": "", "summary_sents": ["DATR is a declarative language for representing a restricted class of inheritance networks, permitting both multiple and default inheritance.", "The principal intended area of application is the representation of lexical entries for natural language processing, and we use examples from this domain throughout.", "In this paper we present the syntax and inference mechanisms for the language.", "The goal of the DATR enterprise is the design of a simple language that (i) has the necessary expressive power to encode the lexical entries presupposed by contemporary work in the unification grammar tradition, (ii) can express all the evident generalizations about such entries, (iii) has an explicit theory of inference, (iv) is computationally tractable, and (v) has an explicit declarative semantics.", "The present paper is primarily concerned with (iii), though the examples used may hint at our strategy in respect of (i) and (ii).", "we introduce DATR, a formal language for representing lexical knowledge."]}
{"title": "Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction", "abstract": "We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution. This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus. ", "introduction": "Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning\u2014both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family. Conjugacy implies a clean form for the posterior distribution over grammar probabilities (given the data and the prior), bestowing computational tractability. Following work by Blei and Lafferty (2006) for topic models, Cohen et al. (2008) proposed an alternative to Dirichlet priors for probabilistic grammars, based on the logistic normal (LN) distribution over the probability simplex. Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN. The prior encodes information about which grammar rules\u2019 weights are likely to covary, a more intuitive and expressive representation of knowledge than offered by Dirichlet distributions.1 The contribution of this paper is two-fold. First, from the modeling perspective, we present a generalization of the LN prior of Cohen et al. (2008), showing how to extend the use of the LN prior to tie between any grammar weights in a probabilistic grammar (instead of only allowing weights within the same multinomial distribution to covary). Second, from the experimental perspective, we show how such flexibility in parameter tying can help in unsupervised grammar learning in the well-known monolingual setting and in a new bilingual setting where grammars for two languages are learned at once (without parallel corpora). Our method is based on a distribution which we call the shared logistic normal distribution, which is a distribution over a collection of multinomials from different probability simplexes. We provide a variational EM algorithm for inference. The rest of this paper is organized as follows. In \u00a72, we give a brief explanation of probabilistic grammars and introduce some notation for the specific type of dependency grammar used in this paper, due to Klein and Manning (2004). In \u00a73, we present our model and a variational inference algorithm for it. In \u00a74, we report on experiments for both monolingual settings and a bilingual setting and discuss them. We discuss future work (\u00a75) and conclude in \u00a76. ", "conclusion": "We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results. We also showed how our model can be effectively used to simultaneously learn grammars in two languages from non-parallel multilingual data. ", "summary_sents": ["We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution.", "This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar.", "We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors.", "We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus.", "We see our largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing."]}
{"title": "Corpus Variation And Parser Performance", "abstract": "Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model. ", "introduction": "The past several years have seen great progress in the field of natural language parsing, through the use of statistical methods trained using large corpora of hand-parsed training data. The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data. In each case, the corpus used was the Penn Treebank's hand-annotated parses of Wall Street Journal articles. Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al. (1999) for results on Czech and Hwa (1999) for bootstrapping from WSJ to ATIS). The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora. In this paper we examine the following questions: Our investigation of these questions leads us to a surprising result about parsing the WSJ corpus: over a third of the model's parameters can be eliminated with little impact on performance. Aside from cross-corpus considerations, this is an important finding if a lightweight parser is desired or memory usage is a consideration. ", "conclusion": "Our results show strong corpus effects for statistical parsing models: a small amount of matched training data appears to be more useful than a large amount of unmatched data. The standard WSJ task seems to be simplified by its homogenous style. Adding training data from from an unmatched corpus doesn't hurt, but doesn't help a great deal either. In particular, lexical bigram statistics appear to be corpus-specific, and our results show that they are of no use when attempting to generalize to new training data. In fact, they are of surprisingly little benefit even for matched training and test data removing them from the model entirely reduces performance by less than 0.5% on the standard WSJ parsing task. Our selective pruning technique allows for a more fine grained tuning of parser model size, and would be particularly applicable to cases where large amounts of training data are available but memory usage is a consideration. In our implementation, pruning allowed models to run within 256MB that, unpruned, required larger machines. The parsing models of Charniak (2000) and Collins (2000) add more complex features to the parsing model that we use as our baseline. An area for future work is investigation of the degree to which such features apply across corpora, or, on the other hand, further tune the parser to the peculiarities of the Wall Street Journal. Of particular interest are the automatic clusterings of lexical co-occurrences used in Charniak (1997) and Magerman (1995). Cross-corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser's use. Acknowledgments This work was undertaken as part of the FrameNet project at ICSI, with funding from National Science Foundation grant ITR/HCI #0086132. ", "summary_sents": ["Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.", "While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora.", "We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.", "This leads us to a technique for pruning parameters to reduce the size of the parsing model.", "We show that the accuracy of parsers trained on the Penn Treebank degrades when applied to different genres and domains.", "We report results on sentences of 40 or less words on all the Brown corpus sections combined, for which we obtain 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus."]}
{"title": "Unsupervised Modeling of Twitter Conversations", "abstract": "We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain. Trained on a corpus of noisy Twitter conversations, our method discovers dialogue acts by clustering raw utterances. Because it accounts for the sequential behaviour of these acts, the learned model can provide insight into the shape of communication in a new medium. We address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task. This work is inspired by a corpus of 1.3 million Twitter conversations, which will be made publicly available. This huge amount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. ", "introduction": "Automatic detection of dialogue structure is an important first step toward deep understanding of human conversations. Dialogue acts1 provide an initial level of structure by annotating utterances with shallow discourse roles such as \u201cstatement\u201d, \u201cquestion\u201d and \u201canswer\u201d. These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009). Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004). Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000). This paradigm has been quite successful, but the labeling process is both slow and expensive, limiting the amount of data available for training. The expense is compounded as we consider new methods of communication, which may require not only new annotations, but new annotation guidelines and new dialogue acts. This issue becomes more pressing as the Internet continues to expand the number of ways in which we communicate, bringing us e-mail, newsgroups, IRC, forums, blogs, Facebook, Twitter, and whatever is on the horizon. Previous work has taken a variety of approaches to dialogue act tagging in new media. Cohen et al. (2004) develop an inventory of dialogue acts specific to e-mail in an office domain. They design their inventory by inspecting a large corpus of e-mail, and refine it during the manual tagging process. Jeong et al. (2009) use semi-supervised learning to transfer dialogue acts from labeled speech corpora to the Internet media of forums and e-mail. They manually restructure the source act inventories in an attempt to create coarse, domain-independent acts. Each approach relies on a human designer to inject knowledge into the system through the inventory of available acts. As an alternative solution for new media, we propose a series of unsupervised conversation models, where the discovery of acts amounts to clustering utterances with similar conversational roles. This avoids manual construction of an act inventory, and allows the learning algorithm to tell us something about how people converse in a new medium. There is surprisingly little work in unsupervised dialogue act tagging. Woszczyna and Waibel (1994) propose an unsupervised Hidden Markov Model (HMM) for dialogue structure in a meeting scheduling domain, but model dialogue state at the word level. Crook et al. (2009) use Dirichlet process mixture models to cluster utterances into a flexible number of acts in a travel-planning domain, but do not examine the sequential structure of dialogue.2 In contrast to previous work, we address the problem of discovering dialogue acts in an informal, open-topic domain, where an unsupervised learner may be distracted by strong topic clusters. We also train and test our models in a new medium: Twitter. Rather than test against existing dialogue inventories, we evaluate using qualitative visualizations and a novel conversation ordering task, to ensure our models have the opportunity to discover dialogue phenomena unique to this medium. ", "conclusion": "We have presented an approach that allows the unsupervised induction of dialogue structure from naturally-occurring open-topic conversational data. By visualizing the learned models, coherent patterns emerge from a stew of data that human readers find difficult to follow. We have extended a conversation sequence model to separate topic and dialogue words, resulting in an interpretable set of automatically generated dialogue acts. These discovered acts have interesting differences from those found in other domains, and reflect Twitter\u2019s nature as a micro-blog. We have introduced the task of conversation ordering as an intrinsic measure of conversation model quality. We found this measure quite useful in the development of our models and algorithms, although our experiments show that it does not necessarily correlate with interpretability. We have directly compared Bayesian inference to EM on our conversation ordering task, showing a clear advantage for Bayesian methods. Finally, we have collected a corpus of 1.3 million Twitter conversations, which we will make available to the research community, and which we hope will be useful beyond the study of dialogue. In the future, we wish to scale our models to the full corpus, and extend them with more complex notions of discourse, topic and community. Ultimately, we hope to put the learned conversation structure to use in the construction of a data-driven, conversational agent. ", "summary_sents": ["We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain.", "Trained on a corpus of noisy Twitter conversations, our method discovers dialogue acts by clustering raw utterances.", "Because it accounts for the sequential behaviour of these acts, the learned model can provide insight into the shape of communication in a new medium.", "We address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task.", "This work is inspired by a corpus of 1.3 million Twitter conversations, which will be made publicly available.", "This huge amount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium.", "We propose an evaluation based on rank correlation coefficient, which measures the degree of similarity between any two orderings over sequential data.", "We limit our dataset by choosing Twitter dialogues containing 3 to 6 posts (utterances), making it tractable to enumerate all permutations.", "We propose a probabilistic model to discover dialogue acts in Twitter conversations and to classify tweets in a conversation according to those acts.", "Under the block HMM, messages in a conversation flow according to a Markov process, where the words of messages are generated according to language models associated with a state in a hidden Markov model."]}
{"title": "Bayesian Learning of Non-Compositional Phrases with Synchronous Parsing", "abstract": "We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs. The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large. Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results. Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. ", "introduction": "Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments. These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al. (1996). As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments. Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process. While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems. A natural solution to several of these issues is unite the word-level and phrase-level models into one learning procedure. Ideally, such a procedure would remedy the deficiencies of word-level alignment models, including the strong restrictions on the form of the alignment, and the strong independence assumption between words. Furthermore it would obviate the need for heuristic combination of word alignments. A unified procedure may also improve the identification of non-compositional phrasal translations, and the attachment decisions for unaligned words. In this direction, Expectation Maximization at the phrase level was proposed by Marcu and Wong (2002), who, however, experienced two major difficulties: computational complexity and controlling overfitting. Computational complexity arises from the exponentially large number of decompositions of a sentence pair into phrase pairs; overfitting is a problem because as EM attempts to maximize the likelihood of its training data, it prefers to directly explain a sentence pair with a single phrase pair. In this paper, we attempt to address these two issues in order to apply EM above the word level. We attack computational complexity by adopting the polynomial-time Inversion Transduction Grammar framework, and by only learning small noncompositional phrases. We address the tendency of EM to overfit by using Bayesian methods, where sparse priors assign greater mass to parameter vectors with fewer non-zero values therefore favoring shorter, more frequent phrases. We test our model by extracting longer phrases from our model\u2019s alignments using traditional phrase extraction, and find that a phrase table based on our system improves MT results over a phrase table extracted from traditional word-level alignments. ", "conclusion": "We have presented an improved and more efficient method of estimating phrase pairs directly. By both changing the objective function to include a bias toward sparser models and improving the pruning techniques and efficiency, we achieve significant gains on test data with practical speed. In addition, these gains were shown without resorting to external models, such as GIZA++. We have shown that VB is both practical and effective for use in MT models. However, our best system does not apply VB to a single probability model, as we found an appreciable benefit from bootstrapping each model from simpler models, much as the IBM word alignment models are usually trained in succession. We find that VB alone is not sufficient to counteract the tendency of EM to prefer analyses with smaller trees using fewer rules and longer phrases. Both the tic-tac-toe pruning and the non-compositional constraint address this problem by reducing the space of possible phrase pairs. On top of these hard constraints, the sparse prior of VB helps make the model less prone to overfitting to infrequent phrase pairs, and thus improves the quality of the phrase pairs the model learns. Acknowledgments This work was done while the first author was at Microsoft Research; thanks to Xiaodong He, Mark Johnson, and Kristina Toutanova. The last author was supported by NSF IIS-0546554. ", "summary_sents": ["We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs.", "The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large.", "Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results.", "Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment.", "This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches.", "We suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed."]}
{"title": "Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis", "abstract": "Determining the polarity of a sentimentbearing expression requires more than a simple bag-of-words approach. In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity. In this paper, we view such interactions in light of composiand present a novel learningbased approach that incorporates structural inference motivated by compositional semantics into the learning procedure. Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%). We also find that \u201ccontentword negators\u201d, not widely employed in previous work, play an important role in determining expression-level polarity. Finally, in contrast to conventional wisdom, we find that expression-level classification accuracy additional, potentially disambiguating, context is considered. ", "introduction": "Determining the polarity of sentiment-bearing expressions at or below the sentence level requires more than a simple bag-of-words approach. One of the difficulties is that words or constituents within the expression can interact with each other to yield a particular overall polarity. To facilitate our discussion, consider the following examples: In the first example, \u201cdoubt\u201d in isolation carries a negative sentiment, but the overall polarity of the sentence is positive because there is a negator \u201cnot\u201d, which flips the polarity. In the second example, both \u201celiminated\u201d and \u201cdoubt\u201d carry negative sentiment in isolation, but the overall polarity of the sentence is positive because \u201celiminated\u201d acts as a negator for its argument \u201cdoubt\u201d. In the last example, there are effectively two negators \u2013 \u201cnot\u201d and \u201celiminated\u201d \u2013 which reverse the polarity of \u201cdoubt\u201d twice, resulting in the negative polarity for the overall sentence. These examples demonstrate that words or constituents interact with each other to yield the expression-level polarity. And a system that simply takes the majority vote of the polarity of individual words will not work well on the above examples. Indeed, much of the previous learning-based research on this topic tries to incorporate salient interactions by encoding them as features. One approach includes features based on contextual valence shifters1 (Polanyi and Zaenen, 2004), which are words that affect the polarity or intensity of sentiment over neighboring text spans (e.g., Kennedy and Inkpen (2005), Wilson et al. (2005), Shaikh et al. (2007)). Another approach encodes frequent subsentential patterns (e.g., McDonald et al. (2007)) as features; these might indirectly capture some of the subsentential interactions that affect polarity. However, both types of approach are based on learning models with a flat bag-of-features: some structural information can be encoded as higher order features, but the final representation of the input is still a flat feature vector that is inherently too limited to adequately reflect the complex structural nature of the underlying subsentential interactions. (Liang et al., 2008) Moilanen and Pulman (2007), on the other hand, handle the structural nature of the interactions more directly using the ideas from compositional semantics (e.g., Montague (1974), Dowty et al. (1981)). In short, the Principle of Compositionality states that the meaning of a compound expression is a function of the meaning of its parts and of the syntactic rules by which they are combined (e.g., Montague (1974), Dowty et al. (1981)). And Moilanen and Pulman (2007) develop a collection of composition rules to assign a sentiment value to individual expressions, clauses, or sentences. Their approach can be viewed as a type of structural inference, but their hand-written rules have not been empirically compared to learning-based alternatives, which one might expect to be more effective in handling some aspects of the polarity classification task. In this paper, we begin to close the gap between learning-based approaches to expression-level polarity classification and those founded on compositional semantics: we present a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure. Adopting the view point of compositional semantics, our working assumption is that the polarity of a sentiment-bearing expression can be determined in a two-step process: (1) assess the polarities of the constituents of the expression, and then (2) apply a relatively simple set of inference rules to combine them recursively. Rather than a rigid application of handwritten compositional inference rules, however, we hypothesize that an ideal solution to the expressionlevel polarity classification task will be a method that can exploit ideas from compositional semantics while providing the flexibility needed to handle the complexities of real-world natural language \u2014 exceptions, unknown words, missing semantic features, and inaccurate or missing rules. The learningbased approach proposed in this paper takes a first step in this direction. In addition to the novel learning approach, this paper presents new insights for content-word negators, which we define as content words that can negate the polarity of neighboring words or constituents. (e.g., words such as \u201celiminated\u201d in the example sentences). Unlike function-word negators, such as \u201cnot\u201d or \u201cnever\u201d, content-word negators have been recognized and utilized less actively in previous work. (Notable exceptions include e.g., Niu et al. (2005), Wilson et al. (2005), and Moilanen and Pulman (2007).2) In our experiments, we compare learning- and non-learning-based approaches to expression-level polarity classification \u2014 with and without compositional semantics \u2014 and find that (1) simple heuristics based on compositional semantics outperform (89.7% in accuracy) other reasonable heuristics that do not incorporate compositional semantics (87.7%); they can also perform better than simple learning-based methods that do not incorporate compositional semantics (89.1%), (2) combining learning with the heuristic rules based on compositional semantics further improves the performance (90.7%), (3) content-word negators play an important role in determining the expression-level polarity, and, somewhat surprisingly, we find that (4) expression-level classification accuracy uniformly decreases as additional, potentially disambiguating, context is considered. In what follows, we first explore heuristic-based approaches in \u00a72, then we present learning-based approaches in \u00a73. Next we present experimental results in \u00a74, followed by related work in \u00a75. ", "conclusion": "In this paper, we consider the task of determining the polarity of a sentiment-bearing expression, considering the effect of interactions among words or constituents in light of compositional semantics. We presented a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure. Our approach can be considered as a small step toward bridging the gap between computational semantics and machine learning methods. Our experimental results suggest that this direction of research is promising. Future research includes an approach that learns the compositional inference rules from data. ", "summary_sents": ["Determining the polarity of a sentiment-bearing expression requires more than a simple bag-of-words approach.", "In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity.", "In this paper, we view such subsentential interactions in light of compositional semantics, and present a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure.", "Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%).", "We also find that \u201ccontent-word negators\u201d, not widely employed in previous work, play an important role in determining expression-level polarity.", "Finally, in contrast to conventional wisdom, we find that expression-level classification accuracy uniformly decreases as additional, potentially disambiguating, context is considered.", "Content-word negators are words that are not function words, but act semantically as negators.", "We combine different kinds of negators with lexical polarity items through various compositional semantic models, both heuristic and machine learned, to improve phrasal sentiment analysiss.", "We propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (correct) phrase-level polarity.", "We hand-code compositional rules in order to model compositional effects of combining different words in the phrase.", "We categorized polarity reversing words into two categories: function-word negators such as not and content-word negators such as eliminate."]}
{"title": "Base Noun Phrase Translation Using Web Data And The EM Algorithm", "abstract": "We consider here the problem of Base Noun Phrase translation. We propose a new method to perform the task. For a given Base NP, we first search its translation candidates from the web. We next determine the possible translation(s) from among the candidates using one of the two methods that we have developed. In one method, we employ an ensemble of Na?ve Bayesian Classifiers constructed with the EM Algorithm. In the other method, we use TF-IDF vectors also constructed with the EM Algorithm. Experimental results indicate that the coverage and accuracy of our method are significantly better than those of the baseline methods relying on existing technologies. ", "introduction": "We address here the problem of Base NP translation, in which for a given Base Noun Phrase in a source language (e.g., ?information age? in English), we are to find out its possible translation(s) in a target language (e.g., ? in Chinese). We define a Base NP as a simple and non-recursive noun phrase. In many cases, Base NPs represent holistic and non-divisible concepts, and thus accurate translation of them from one language to another is extremely important in applications like machine translation, cross language information retrieval, and foreign language writing assistance. In this paper, we propose a new method for Base NP translation, which contains two steps: (1) translation candidate collection, and (2) translation selection. In translation candidate collection, for a given Base NP in the source language, we look for its translation candidates in the target language. To do so, we use a word-to-word translation dictionary and corpus data in the target language on the web. In translation selection, we determine the possible translation(s) from among the candidates. We use non-parallel corpus data in the two languages on the web and employ one of the two methods which we have developed. In the first method, we view the problem as that of classification and employ an ensemble of Na?ve Bayesian Classifiers constructed with the EM Algorithm. We will use ?EM-NBC-Ensemble? to denote this method, hereafter. In the second method, we view the problem as that of calculating similarities between context vectors and use TF-IDF vectors also constructed with the EM Algorithm. We will use ?EM-TF-IDF? to denote this method. Experimental results indicate that our method is very effective, and the coverage and top 3 accuracy of translation at the final stage are 91.4% and 79.8%, respectively. The results are significantly better than those of the baseline methods relying on existing technologies. The higher performance of our method can be attributed to the enormity of the web data used and the employment of the EM Algorithm. ", "conclusion": "This paper has proposed a new and effective method for Base NP translation by using web data and the EM Algorithm. Experimental results show that it outperforms the baseline methods based on existing techniques, mainly due to the employment of EM. Experimental results also show that the use of web data is more effective than non-use of it. Future work includes further applying the proposed method to the translation of other types of Base NPs and between other language pairs. Acknowledgements We thank Ming Zhou, Chang-Ning Huang, Jianfeng Gao, and Ashley Chang for many helpful discussions on this research project. We also acknowledge Shenjie Li for help with program coding. ", "summary_sents": ["We consider here the problem of Base Noun Phrase translation.", "We propose a new method to perform the task.", "For a given Base NP, we first search its translation candidates from the web.", "We next determine the possible translation(s) from among the candidates using one of the two methods that we have developed.", "In one method, we employ an ensemble of Naive Bayesian Classifiers constructed with the EM Algorithm.", "In the other method, we use TF-IDF vectors also constructed with the EM Algorithm.", "Experimental results indicate that the coverage and accuracy of our method are significantly better than those of the baseline methods relying on existing technologies.", "In our method, translation candidates of a term are compositionally generated by concatenating the translation of the constituents of the term and are re-ranked by measuring contextual similarity against the source language term."]}
{"title": "Robust Sentiment Detection on Twitter from Biased and Noisy Data", "abstract": "In this paper, we propose an approach toautomatically detect sentiments on Twit ter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages. Moreover, we leverage sources of noisy labels as our training data. These noisy labels were provided by a few sentiment detectionwebsites over twitter data. In our experi ments, we show that since our features areable to capture a more abstract representation of tweets, our solution is more ef fective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these sources. ", "introduction": "Twitter is one of the most popular social network websites and has been growing at a very fast pace. The number of Twitter users reached an estimated75 million by the end of 2009, up from approx imately 5 million in the previous year. Through the twitter platform, users share either informationor opinions about personalities, politicians, prod ucts, companies, events (Prentice and Huffman, 2008) etc. This has been attracting the attention of different communities interested in analyzing its content. Sentiment detection of tweets is one of the basicanalysis utility functions needed by various applications over twitter data. Many systems and ap proaches have been implemented to automatically detect sentiment on texts (e.g., news articles, Web reviews and Web blogs) (Pang et al, 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Glance et al, 2005; Wilson et al, 2005). Most of theseapproaches use the raw word representation (n grams) as features to build a model for sentiment detection and perform this task over large pieces of texts. However, the main limitation of usingthese techniques for the Twitter context is mes sages posted on Twitter, so-called tweets, are veryshort. The maximum size of a tweet is 140 char acters. In this paper, we propose a 2-step sentiment analysis classification method for Twitter, whichfirst classifies messages as subjective and ob jective, and further distinguishes the subjectivetweets as positive or negative. To reduce the la beling effort in creating these classifiers, instead of using manually annotated data to compose thetraining data, as regular supervised learning ap proaches, we leverage sources of noisy labels asour training data. These noisy labels were pro vided by a few sentiment detection websites over twitter data. To better utilize these sources, we verify the potential value of using and combining them, providing an analysis of the provided labels, examine different strategies of combining these sources in order to obtain the best outcome; and, propose a more robust feature set that captures a more abstract representation of tweets, composedby meta-information associated to words and spe cific characteristics of how tweets are written. By using it, we aim to handle better: the problem of lack of information on tweets, helping on thegeneralization process of the classification algo rithms; and the noisy and biased labels provided by those websites.The remainder of this paper is organized as fol lows. In Section 2, we provide some context about messages on Twitter and about the websites used as label sources. We introduce the features used in the sentiment detection and also provide a deep analysis of the labels generated by those sources in Section 3. We examine different strategies of 36 combining these sources and present an extensive experimental evaluation in Section 4. Finally, we discuss previous works related to ours in Section 5and conclude in Section 6, where we outline direc tions and future work. ", "conclusion": "We have presented an effective and robust sen timent detection approach for Twitter messages, which uses biased and noisy labels as input to build its models. This performance is due to the fact that: (1) our approach creates a more abstract representation of these messages, instead of usinga raw word representation of them as some previous approaches; and (2) although noisy and bi ased, the data sources provide labels of reasonablequality and, since they have different bias, com bining them also brought some benefits. The main limitation of our approach is the cases of sentences that contain antagonistic sentiments. As future work, we want to perform a more fine grained analysis of sentences in order to identifyits main focus and then based the sentiment clas sification on it. ", "summary_sents": ["In this paper, we propose an approach to automatically detect sentiments on Twitter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages. Moreover, we leverage sources of noisy labels as our training data.", "These noisy labels were provided by a few sentiment detection websites over twitter data.", "In our experiments, we show that since our features are able to capture a more abstract representation of tweets, our solution is more effective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these sources.", "We propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features."]}
{"title": "Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters", "abstract": "author: Online conversational text, typified by microblogs, and text is a challenge for natural language processing. Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There Abstract We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute). Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at: ", "introduction": "asked for your last name so he can add you on Facebook. The tagset is defined in Appendix A. Refer to Fig. 2 for word clusters corresponding to some of these words. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger\u2014building on previous work by Gimpel et al. (2011)\u2014that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for both Twitter and Internet Relay Chat (IRC) text. We also annotated a new dataset of tweets with POS tags, improved the annotations in the previous dataset from Gimpel et al., and developed annotation guidelines for manual POS tagging of tweets. We release all of these resources to the research community: ", "conclusion": "We have constructed a state-of-the-art part-ofspeech tagger for the online conversational text genres of Twitter and IRC, and have publicly released our new evaluation data, annotation guidelines, open-source tagger, and word clusters at http://www.ark.cs.cmu.edu/TweetNLP. ", "summary_sents": ["We consider the problem of part-of-speech tagging for informal, online conversational text.", "We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy.", "With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute).", "Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre.", "Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines.", "Tagging software, annotation guidelines, and large-scale word clusters are available at: http://www.ark.cs.cmu.edu/TweetNLP", "This paper describes release 0.3 of the CMU Twitter Part-of-Speech Tagger and annotated data."]}
{"title": "A Discriminative Latent Variable Model for Statistical Machine Translation", "abstract": "Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems. We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. ", "introduction": "Statistical machine translation (SMT) has seen a resurgence in popularity in recent years, with progress being driven by a move to phrase-based and syntax-inspired approaches. Progress within these approaches however has been less dramatic. We believe this is because these frequency count based' models cannot easily incorporate non-independent and overlapping features, which are extremely useful in describing the translation process. Discriminative models of translation can include such features without making assumptions of independence or explicitly modelling their interdependence. However, while discriminative models promise much, they have not been shown to deliver significant gains 'We class approaches using minimum error rate training (Och, 2003) frequency count based as these systems re-scale a handful of generative features estimated from frequency counts and do not support large sets of non-independent features. over their simpler cousins. We argue that this is due to a number of inherent problems that discriminative models for SMT must address, in particular the problems of spurious ambiguity and degenerate solutions. These occur when there are many ways to translate a source sentence to the same target sentence by applying a sequence of steps (a derivation) of either phrase translations or synchronous grammar rules, depending on the type of system. Existing discriminative models require a reference derivation to optimise against, however no parallel corpora annotated for derivations exist. Ideally, a model would account for this ambiguity by marginalising out the derivations, thus predicting the best translation rather than the best derivation. However, doing so exactly is NP-complete. For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007). In this paper we directly address the problem of spurious ambiguity in discriminative models. We use a synchronous context free grammar (SCFG) translation system (Chiang, 2007), a model which has yielded state-of-the-art results on many translation tasks. We present two main contributions. First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences. This model maximises the conditional likelihood of the data, p(e|f), where e and f are the English and foreign sentences, respectively. Our estimation method is theoretically sound, avoiding the biases of the heuristic relative frequency estimates length and the average number of derivations (on a log scale) for each reference sentence in our training corpus. (Koehn et al., 2003). Second, within this framework, we model the derivation, d, as a latent variable, p(e, d1f), which is marginalised out in training and decoding. We show empirically that this treatment results in significant improvements over a maximum-derivation model. The paper is structured as follows. In Section 2 we list the challenges that discriminative SMT must face above and beyond the current systems. We situate our work, and previous work, on discriminative systems in this context. We present our model in Section 3, including our means of training and decoding. Section 4 reports our experimental setup and results, and finally we conclude in Section 5. ", "conclusion": "", "summary_sents": ["Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.", "We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.", "We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.", "Results show that accounting for multiple derivations does indeed improve performance.", "Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.", "We show that marginalizing out the different segmentations during decoding leads to improved performance.", "We present a latent variable model that describes the relationship between translation and derivation clearly.", "For the hierarchical phrase-based approach, we present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations."]}
{"title": "Rule Writing Or Annotation: Cost-Efficient Resource Usage For Base Noun Phrase Chunking", "abstract": "This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation. Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored. Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment. ", "introduction": "One of the primary problems that NLP researchers who work in new languages or new domains encounter is a lack of available annotated data. Collection of data is neither easy nor cheap. The construction of the Penn Treebank significantly improved performance for English systems dealing in the &quot;traditional&quot; NLP domains (eg parsing, part-of-speech tagging, etc). However, for a new language, a similar investment of effort in time and money is most likely prohibitive, if not impossible. Faced with the costs associated with data acquisition, rationalists may argue that it would be more cost effective to construct systems of handcoded rule lists that capture the linguistic characteristics of the task at hand, rather than spending comparable effort annotating data and expecting the same knowledge to be acquired indirectly by a machine learning system. The question we are trying to address then is: for a given cost assumption, which approach would be the most effective. Although learning curves showing performance relative to amount of training data are common in the machine learning literature, these are inadequate for comparing systems with different sources of training data or supervision. This is especially true when a human rule-based approach and empirical learning are evaluated relative to effort invested. Such a multi-factor cost analysis is long overdue. This paper will conclude with a comprehensive cost model exposition and analysis, and an empirical study contrasting human rule-writing versus annotation-based learning approaches that are sensitive to these cost models. ", "conclusion": "This paper has illustrated that there are potentially compelling practical and performance advantages to pursuing active-learning based annotation rather than rule-writing to develop base noun phrase chunkers. The relative balance depends ultimately on one's cost model, but given the goal of minimizing total human labor cost, it appears to be consistently more efficient and effective to invest these human resources in systemdevelopment via annotation rather than rule writing. ", "summary_sents": ["This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation.", "Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored.", "Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.", "We use an ensemble based on bagging and partitioning for active learning for base NP chunking."]}
{"title": "Improving Data Driven Wordclass Tagging by System Combination", "abstract": "In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger. ", "introduction": "In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations. Traditionally, these models were categorized as either rule-based/symbolic or corpusbased/probabilistic. Recent work (e.g. Brill 1992) has demonstrated clearly that this categorization is in fact a mix-up of two distinct categorization systems: on the one hand there is the representation used for the language model (rules, Markov model, neural net, case base, etc.) and on the other hand the manner in which the model is constructed (hand crafted vs. data driven). Data driven methods appear to be the more popular. This can be explained by the fact that, in general, hand crafting an explicit model is rather difficult, especially since what is being modelled, natural language, is not (yet) wellunderstood. When a data driven method is used, a model is automatically learned from the implicit structure of an annotated training corpus. This is much easier and can quickly lead to a model which produces results with a 'reasonably' good quality. Obviously, 'reasonably good quality' is not the ultimate goal. Unfortunately, the quality that can be reached for a given task is limited, and not merely by the potential of the learning method used. Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material. Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system. However, the fact that any given system cannot go beyond this ceiling does not mean that machine learning as a whole is similarly limited. A potential loophole is that each type of learning method brings its own 'inductive bias' to the task and therefore different methods will tend to produce different errors. In this paper, we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance. In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers. It has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than all the individual systems (Ali and Pazzani 1996; Chan and Stolfo 1995; Turner and Gosh 1996). The underlying assumption is twofold. First, the combined votes will make the system more robust to the quirks of each learner's particular bias. Also, the use of information about each individual method's behaviour in principle even admits the possibility to fix collective errors. We will execute our investigation by means of an experiment. The NLP task used in the experiment is morpho-syntactic wordclass tagging. The reasons for this choice are several. First of all, tagging is a widely researched and well-understood task (cf. van Halteren (ed.) 1998). Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. ", "conclusion": "Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems. Obviously there is still room for a closer examination of the differences between the combination methods, e.g. the question whether Memory-Based combination would have performed better if we had provided more training data than just Tune, and of the remaining errors, e.g. the effects of inconsistency in the data (cf. Ratnaparkhi 1996 on such effects in the Penn Treebank corpus). Regardless of such closer investigation, we feel that our results are encouraging enough to extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and/or languages. But the investigation need not be limited to wordclass tagging, for we expect that there are many other NLP tasks where combination could lead to worthwhile improvements. ", "summary_sents": ["In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system.", "We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging.", "Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.", "After comparison, their outputs are combined using several voting strategies and second stage classifiers.", "All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.", "We suggest three voting strategies: equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair-wise voting."]}
{"title": "Effective Self-Training For Parsing", "abstract": "We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. improved model achieves an of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon. ", "introduction": "In parsing, we attempt to uncover the syntactic structure from a string of words. Much of the challenge of this lies in extracting the appropriate parsing decisions from textual examples. Given sufficient labelled data, there are several \u201csupervised\u201d techniques of training high-performance parsers (Charniak and Johnson, 2005; Collins, 2000; Henderson, 2004). Other methods are \u201csemi-supervised\u201d where they use some labelled data to annotate unlabeled data. Examples of this include self-training (Charniak, 1997) and co-training (Blum and Mitchell, 1998; Steedman et al., 2003). Finally, there are \u201cunsupervised\u201d strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002). Semi-supervised and unsupervised methods are important because good labeled data is expensive, whereas there is no shortage of unlabeled data. While some domain-language pairs have quite a bit of labelled data (e.g. news text in English), many other categories are not as fortunate. Less unsupervised methods are more likely to be portable to these new domains, since they do not rely as much on existing annotations. ", "conclusion": "Contrary to received wisdom, self-training can improve parsing. In particular we have achieved an absolute improvement of 0.8% over the baseline performance. Together with a 0.3% improvement due to superior reranking features, this is a 1.1% improvement over the previous best parser results for section 23 of the Penn Treebank (from 91.0% to 92.1%). This corresponds to a 12% error reduction assuming that a 100% performance is possible, which it is not. The preponderance of evidence suggests that it is somehow the reranking aspect of the parser that makes this possible, but given no idea of why this should be, so we reserve final judgement on this matter. Also contrary to expectations, the error analysis suggests that there has been no improvement in either the handing of unknown words, nor prepositional phrases. Rather, there is a general improvement in intermediate-length sentences (20-50 words), but no improvement at the extremes: a phenomenon we call the Goldilocks effect. The only specific syntactic phenomenon that seems to be affected is conjunctions. However, this is good news since conjunctions have long been considered the hardest of parsing problems. There are many ways in which this research should be continued. First, the error analysis needs to be improved. Our tentative guess for why sentences with unknown words failed to improve should be verified or disproven. Second, there are many other ways to use self-trained information in parsing. Indeed, the current research was undertaken as the control experiment in a program to try much more complicated methods. We still have them to try: restricting consideration to more accurately parsed sentences as training data (sentence selection), trying to learn grammatical generalizations directly rather than simply including the data for training, etc. Next there is the question of practicality. In terms of speed, once the data is loaded, the new parser is pretty much the same speed as the old \u2014 just under a second a sentence on average for treebank sentences. However, the memory requirements are largish, about half a gigabyte just to store the data. We are making our current best self-trained parser available3 as machines with a gigabyte or more of RAM are becoming commonplace. Nevertheless, it would be interesting to know if the data can be pruned to make the entire system less bulky. Finally, there is also the nature of the self-trained data themselves. The data we use are from the LA Times. Those of us in parsing have learned to expect significant decreases in parsing accuracy even when moving the short distance from LA Times to Wall Street Journal. This seemingly has not occurred. Does this mean that the reranking parser somehow overcomes at least small genre differences? On this point, we have some pilot experiments that show great promise. ", "summary_sents": ["We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data.", "We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker.", "Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing.", "Finally, we provide some analysis to better understand the phenomenon.", "We presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker.", "Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model."]}
{"title": "Tailoring Word Alignments to Syntactic Machine Translation", "abstract": "Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model\u2019s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments. ", "introduction": "Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006). However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 and Dorr, 2006). However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum\u00b4e III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules. These rules are extracted from a bitext annotated with both English (target side) parses and word alignments. Rules are extracted from target side constituents that can be projected onto contiguous spans of the source sentence via the word alignment. Constituents that project onto non-contiguous spans of the source sentence do not yield transducer rules themselves, and can only be incorporated into larger transducer rules. Thus, if the word alignment of a sentence pair does not respect the constituent structure of the target sentence, then the minimal translation units must span large tree fragments, which do not generalize well. We present and evaluate an unsupervised word alignment model similar in character and computation to the HMM model (Ney and Vogel, 1996), but which incorporates a novel, syntax-aware distortion component which conditions on target language parse trees. These trees, while automatically generated and therefore imperfect, are nonetheless (1) a useful source of structural bias and (2) the same trees which constrain future stages of processing anyway. In our model, the trees do not rule out any alignments, but rather softly influence the probability of transitioning between alignment positions. In particular, transition probabilities condition upon paths through the target parse tree, allowing the model to prefer distortions which respect the tree structure. Our model generates word alignments that better respect the parse trees upon which they are conditioned, without sacrificing alignment quality. Using the joint training technique of Liang et al. (2006) to initialize the model parameters, we achieve an AER superior to the GIZA++ implementation of IBM model 4 (Och and Ney, 2003) and a reduction of 56.3% in aligned interior nodes, a measure of agreement between alignments and parses. As a result, our alignments yield more rules, which better match those we would extract had we used manual alignments. ", "conclusion": "In light of the need to reconcile word alignments with phrase structure trees for syntactic MT, we have proposed an HMM-like model whose distortion is sensitive to such trees. Our model substantially reduces the number of interior nodes in the aligned corpus and improves rule extraction while nearly retaining the speed and alignment accuracy of the HMM model. While it remains to be seen whether these improvements impact final translation accuracy, it is reasonable to hope that, all else equal, alignments which better respect syntactic correspondences will be superior for syntactic MT. ", "summary_sents": ["Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences.", "We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model.", "Our model's predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality.", "We also discuss the impact of various posterior-based methods of reconciling bidirectional alignments.", "We refine the distortion model of an HMM aligner to reflect tree distance instead of string distance.", "We use hard union competitive thresholding.", "We use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments."]}
{"title": "Creating Speech and Language Data With Amazon&rsquo;s Mechanical Turk", "abstract": "In this paper we give an introduction to using Amazon?s Mechanical Turk crowdsourc ing platform for the purpose of collecting data for human language technologies. Wesurvey the papers published in the NAACL 2010 Workshop. 24 researchers participated in the workshop?s shared task to create data for speech and language applications with $100. ", "introduction": "This paper gives an overview of the NAACL-2010 Workshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk. A number of recent papers have evaluated the effectiveness of us ing Mechanical Turk to create annotated data for natural language processing applications. The lowcost, scalable workforce available through Mechan ical Turk (MTurk) and other crowdsourcing sites opens new possibilities for annotating speech and text, and has the potential to dramatically changehow we create data for human language technolo gies. Open questions include: What kind of researchis possible when the cost of creating annotated train ing data is dramatically reduced? What new tasks should we try to solve if we do not limit ourselves to reusing existing training and test sets? Can complex annotation be done by untrained annotators? Howcan we ensure high quality annotations from crowd sourced contributors?To begin addressing these questions, we orga nized an open-ended $100 shared task. Researchers were given $100 of credit on Amazon MechanicalTurk to spend on an annotation task of their choosing. They were required to write a short paper de scribing their experience, and to distribute the datathat they created. They were encouraged to ad dress the following questions: How did you conveythe task in terms that were simple enough for non experts to understand? Were non-experts as good as experts? What did you do to ensure quality? How quickly did the data get annotated? What is the cost per label? Researchers submitted a 1 page proposalto the workshop organizers that described their in tended experiments and expected outcomes. The organizers selected proposals based on merit, and awarded $100 credits that were generously provided by Amazon Mechanical Turk. In total, 35 credits were awarded to researchers. Shared task participants were given 10 days to run experiments between the distribution of the credit and the initial submission deadline. 30 papers were submitted to the shared task track, of which 24 were accepted. 14 papers were submitted to the generaltrack of which 10 were accepted, giving a 77% ac ceptance rate and a total of 34 papers. Shared taskparticipants were required to provide the data col lected as part of their experiments. All of the shared task data is available on the workshop website. ", "conclusion": "", "summary_sents": ["In this paper we give an introduction to using Amazon\u2019s Mechanical Turk crowdsourcing platform for the purpose of collecting data for human language technologies.", "We survey the papers published in the NAACL2010 Workshop.", "24 researchers participated in the workshop\u2019s shared task to create data for speech and language applications with $100.", "We experiment with the use of Amazon Mechanical Turk (AMT) to create and evaluate human language data (Callison-Burch and Dredze, 2010).", "We provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data."]}
{"title": "A Non-Projective Dependency Parser", "abstract": "We describe a practical parser for unrestricted dependencies. The parser creates links between words and names the links according to their syntactic functions. We first describe the older Constraint Grammar parser where many of the ideas come from. Then we proceed to describe the central ideas of our new parser. Finally, the parser is evaluated. ", "introduction": "We are concerned with surface-syntactic parsing of running text. Our main goal is to describe syntactic analyses of sentences using dependency links that show the head-modifier relations between words. In addition, these links have labels that refer to the syntactic function of the modifying word. A simplified example is in Figure 1, where the link between I and see denotes that I is the modifier of see and its syntactic function is that of subject. Similarly, a modifies bird, and it is a determiner. First, in this paper, we explain some central concepts of the Constraint Grammar framework from which many of the ideas are derived. Then, we give some linguistic background to the notations we are using, with a brief comparison to other current dependency formalisms and systems. New formalism is described briefly, and it is utilised in a small toy grammar to illustrate how the formalism works. Finally, the real parsing system, with a grammar of some 2 500 rules, is evaluated. The parser corresponds to over three man-years of work, which does not include the lexical analyser and the morphological disambiguator, both parts of the existing English Constraint Grammar parser (Karlsson et al., 1995). The parsers can be tested via WWW'. ", "conclusion": "In this paper, we have presented some main features of our new framework for dependency syntax. The most important result is that the new framework allows us to describe non-projective dependency grammars and apply them efficiently. This is a property that will be crucial when we will apply this framework to a language having free word-order. Basically, the parsing framework combines the Constraint Grammar framework (removing ambiguous readings) with a mechanism that adds dependencies between readings or tags. This means that while the parser disambiguates it also builds up a dependency forest that, in turn, is reduced by other disambiguation rules and a global pruning mechanism. This setup makes it possible to operate on several layers of information, and use and combine structural information more efficiently than in the original Constraint Grammar framework, without any further disadvantage in dealing with ambiguity. First preliminary evaluations are presented. Compared to the ENGCG syntactic analyser, the output not only contains more information but it is also more accurate and explicit. The ambiguity rate is reduced to a quarter without any compromise in correctness. We did not have access to other systems, and care must be taken when interpreting the results which are not strictly comparable. However, the comparison to other current systems suggests that our dependency parser is very promising both theoretically and practically. ", "summary_sents": ["We describe a practical parser for unrestricted dependencies.", "The parser creates links between words and names the links according to their syntactic functions.", "We first describe the older Constraint Grammar parser where many of the ideas come from.", "Then we proceed to describe the central ideas of our new parser.", "Finally, the parser is evaluated."]}
{"title": "Discriminative Training Of A Neural Network Statistical Parser", "abstract": "Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing. One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem. We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model. We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative. The latter model outperforms the previous two, achieving state-ofthe-art levels of performance (90.1% F-measure on constituents). ", "introduction": "Much recent work has investigated the application of discriminative methods to NLP tasks, with mixed results. Klein and Manning (2002) argue that these results show a pattern where discriminative probability models are inferior to generative probability models, but that improvements can be achieved by keeping a generative probability model and training according to a discriminative optimization criteria. We show how this approach can be applied to broad coverage natural language parsing. Our estimation and training methods successfully balance the conflicting requirements that the training method be both computationally tractable for large datasets and a good approximation to the theoretically optimal method. The parser which uses this approach outperforms both a generative model and a discriminative model, achieving state-of-the-art levels of performance (90.1% F-measure on constituents). To compare these different approaches, we use a neural network architecture called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001) to estimate the parameters of the probability models. SSNs have the advantage that they avoid the need to impose hand-crafted independence assumptions on the learning process. Training an SSN simultaneously trains a finite representations of the unbounded parse history and a mapping from this history representation to the parameter estimates. The history representations are automatically tuned to optimize the parameter estimates. This avoids the problem that any choice of hand-crafted independence assumptions may bias our results towards one approach or another. The independence assumptions would have to be different for the generative and discriminative probability models, and even for the parsers which use the generative probability model, the same set of independence assumptions may be more appropriate for maximizing one training criteria over another. By inducing the history representations specifically to fit the chosen model and training criteria, we avoid having to choose independence assumptions which might bias our results. Each complete parsing system we propose consists of three components, a probability model for sequences of parser decisions, a Simple Synchrony Network which estimates the parameters of the probability model, and a procedure which searches for the most probable parse given these parameter estimates. This paper outlines each of these components, but more details can be found in (Henderson, 2003b), and, for the discriminative model, in (Henderson, 2003a). We also present the training methods, and experiments on the proposed parsing models. ", "conclusion": "This article has investigated the application of discriminative methods to broad coverage natural language parsing. We distinguish between two different ways to apply discriminative methods, one where the probability model is changed to a discriminative one, and one where the probability model remains generative but the training method optimizes a discriminative criteria. We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance. Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003). This paper has also proposed a neural network training method which optimizes a discriminative criteria even when the parameters being estimated are those of a generative probability model. This training method successfully satisfies the conflicting constraints that it be computationally tractable and that it be a good approximation to the theoretically optimal method. This approach contrasts with previous approaches to scaling up discriminative methods to broad coverage natural language parsing, which have parameterizations which depart substantially from the successful previous generative models of parsing. ", "summary_sents": ["Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing.", "One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem.", "We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model.", "We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative.", "The latter model outperforms the previous two, achieving state-of-the-art levels of performance (90.1% F-measure on constituents).", "We provide a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly.", "We use neural networks to induce latent left-corner parser states.", "We find that discriminative training was too slow, and reports accuracy higher than generative models by discriminatively reranking the output of the generative model."]}
{"title": "Transductive learning for statistical machine translation", "abstract": "Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language. In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality. We propose several algorithms with this aim, and present the strengths and weaknesses of each one. We present detailed experimental evaluations on the French\u2013English EuroParl data set and on data from the NIST Chinese\u2013English largedata track. We show a significant improvement in translation quality on both tasks. ", "introduction": "In statistical machine translation (SMT), translation is modeled as a decision process. The goal is to find the translation t of source sentence s which maximizes the posterior probability: arg max p(t  |s) = arg max p(s  |t) \u00b7 p(t) (1) This decomposition of the probability yields two different statistical models which can be trained independently of each other: the translation model p(s  |t) and the target language model p(t). State-of-the-art SMT systems are trained on large collections of text which consist of bilingual corpora (to learn the parameters of p(s  |t)), and of monolingual target language corpora (for p(t)). It has been shown that adding large amounts of target language text improves translation quality considerably. However, the availability of monolingual corpora in the source language does not help improve the system\u2019s 25 performance. We will show how such corpora can be used to achieve higher translation quality. Even if large amounts of bilingual text are given, the training of the statistical models usually suffers from sparse data. The number of possible events, i.e. phrase pairs or pairs of subtrees in the two languages, is too big to reliably estimate a probability distribution over such pairs. Another problem is that for many language pairs the amount of available bilingual text is very limited. In this work, we will address this problem and propose a general framework to solve it. Our hypothesis is that adding information from source language text can also provide improvements. Unlike adding target language text, this hypothesis is a natural semi-supervised learning problem. To tackle this problem, we propose algorithms for transductive semi-supervised learning. By transductive, we mean that we repeatedly translate sentences from the development set or test set and use the generated translations to improve the performance of the SMT system. Note that the evaluation step is still done just once at the end of our learning process. In this paper, we show that such an approach can lead to better translations despite the fact that the development and test data are typically much smaller in size than typical training data for SMT systems. Transductive learning can be seen as a means to adapt the SMT system to a new type of text. Say a system trained on newswire is used to translate weblog texts. The proposed method adapts the trained models to the style and domain of the new input. ", "conclusion": "", "summary_sents": ["Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language.", "In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality.", "We propose several algorithms with this aim, and present the strengths and weaknesses of each one.", "We present detailed experimental evaluations on the French-English EuroParl data set and on data from the NIST Chinese-English large-data track.", "We show a significant improvement in translation quality on both tasks."]}
{"title": "Dual Decomposition for Parsing with Non-Projective Head Automata", "abstract": "This paper introduces algorithms for nonparsing based on decomposi- We focus on parsing algorithms for nonhead a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets. ", "introduction": "Non-projective dependency parsing is useful for many languages that exhibit non-projective syntactic structures. Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). There has been a long history in combinatorial optimization of methods that exploit structure in complex problems, using methods such as dual decomposition or Lagrangian relaxation (Lemar\u00b4echal, 2001). Thus far, however, these methods are not widely used in NLP. This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for non-projective head automata, a generalization of the head-automata models of Eisner (2000) and Alshawi (1996) to nonprojective structures. These models include nonprojective dependency parsing models with higherorder (e.g., sibling and/or grandparent) dependency relations as a special case. Although decoding of full parse structures with non-projective head automata is intractable, we leverage the observation that key components of the decoding can be efficiently computed using combinatorial algorithms. In particular, In this paper we first give the definition for nonprojective head automata, and describe the parsing algorithm. The algorithm can be viewed as an instance of Lagrangian relaxation; we describe this connection, and give convergence guarantees for the method. We describe a generalization to models that include grandparent dependencies. We then introduce a perceptron-driven training algorithm that makes use of point 1 above. We describe experiments on non-projective parsing for a number of languages, and in particular compare the dual decomposition algorithm to approaches based on general-purpose linear programming (LP) or integer linear programming (ILP) solvers (Martins et al., 2009). The accuracy of our models is higher than previous work on a broad range of datasets. The method gives exact solutions to the decoding problem, together with a certificate of optimality, on over 98% of test examples for many of the test languages, with parsing times ranging between 0.021 seconds/sentence for the most simple languages/models, to 0.295 seconds/sentence for the most complex settings. The method compares favorably to previous work using LP/ILP formulations, both in terms of efficiency, and also in terms of the percentage of exact solutions returned. While the focus of the current paper is on nonprojective dependency parsing, the approach opens up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata. ", "conclusion": "We have described dual decomposition algorithms for non-projective parsing, which leverage existing dynamic programming and MST algorithms. There are a number of possible areas for future work. As described in section 7.7, the algorithms can be easily modified to consider projective structures by replacing Y with the set of projective trees, and then using first-order dependency parsing algorithms in place of MST decoding. This method could be used to derive parsing algorithms that include higher-order features, as an alternative to specialized dynamic programming algorithms. Eisner (2000) describes extensions of head automata to include word senses; we have not discussed this issue in the current paper, but it is simple to develop dual decomposition algorithms for this case, using similar methods to those used for the grandparent models. The general approach should be applicable to other lexicalized syntactic formalisms, and potentially also to decoding in syntax-driven translation. In addition, our dual decomposition approach is well-suited to parallelization. For example, each of the head-automata could be optimized independently in a multi-core or GPU architecture. Finally, our approach could be used with other structured learning algorithms, e.g. Meshi et al. (2010). ", "summary_sents": ["This paper introduces algorithms for non-projective parsing based on dual decomposition.", "We focus on parsing algorithms for non-projective head automata, a generalization of head-automata models to non-projective structures.", "The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms.", "They provably solve an LP relaxation of the non-projective parsing problem.", "Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences.", "The accuracy of our models is higher than previous work on a broad range of datasets.", "We consider third-order features such as grand-siblings and tri-siblings."]}
{"title": "Lexical Semantic Relatedness with Random Graph Walks", "abstract": "Many systems for tasks such as question answering, multi-document summarization, and infor mation retrieval need robust numerical measures of lexical relatedness. Standard thesaurus-based measures of word pair similarity are based on only a single path between those words in the thesaurus graph. By contrast, we propose a newmodel of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the en tire graph. Our model uses a random walk over nodes and edges derived from WordNet links and corpus statistics. We treat the graph as aMarkov chain and compute a word-specific sta tionary distribution via a generalized PageRank algorithm. Semantic relatedness of a word pair is scored by a novel divergence measure, ZKL, that outperforms existing measures on certain classes of distributions. In our experiments, the resultingrelatedness measure is the WordNet-based measure most highly correlated with human similar ity judgments by rank ordering at ? = .90. ", "introduction": "Several kinds of Natural Language Processing systems need measures of semantic relatedness for arbitrary wordpairs. For example, document summarization and ques tion answering systems often use similarity scores to evaluate candidate sentence alignments, and informationretrieval systems use relatedness scores for query expan sion. Several popular algorithms calculate scores from information contained in WordNet (Fellbaum, 1998), an electronic dictionary where word senses are explicitly connected by zero or more semantic relationships. Thecentral challenge of these algorithms is to compute rea sonable relatedness scores for arbitrary word pairs given that few pairs are directly connected. Most pairs in WordNet share no direct semantic link, and for some the shortest connecting path can be surprising?even pairs that seem intuitively related, such ?furnace? and ?stove? share a lowest common ancestor in the hypernymy taxonomy (is-a links) all the way upat ?artifact? (a man-made object). Several existing algorithms compute relatedness only by traversing the hyper nymy taxonomy and find that ?furnace? and ?stove? are relatively unrelated. However, WordNet provides other types of semantic links in addition to hypernymy, such as meronymy (part/whole relationships), antonymy, andverb entailment, as well as implicit links defined by over lap in the text of definitional glosses. These links can provide valuable relatedness information. If we assume that relatedness is transitive across a wide variety of such links, then it is natural to follow paths such as furnace? crematory?gas oven?oven?kitchen appliance?stove and find a higher degree of relatedness between ?furnace? and ?stove.? This paper presents the application of random walkMarkov chain theory to measuring lexical semantic re latedness. A graph of words and concepts is constructedfrom WordNet. The random walk model posits the exis tence of a particle that roams this graph by stochastically following local semantic relational links. The particle is biased toward exploring the neighborhood around a target word, and is allowed to roam until the proportion of time it visits each node in the limit converges to a stationarydistribution. In this way we can compute distinct, word specific probability distributions over how often a particle visits all other nodes in the graph when ?starting? from a specific word. We compute the relatedness of two words as the similarity of their stationary distributions.The random walk brings with it two distinct advan tages. First, it enables the similarity measure to have a principled means of combination of multiple types of edges from WordNet. Second, by traversing all links, thewalk aggregates local similarity statistics across the en tire graph. The similarity scores produced by our method are, to our knowledge, the WordNet-based scores most highly correlated with human judgments. 581 ", "conclusion": "In this paper, we have introduced a new measure oflexical relatedness based on the divergence of the sta tionary distributions computed from random walks overgraphs extracted WordNet. We have explored the structural properties of extracted semantic graphs and characterized the distinctly different types of stationary distribu tions that result. We explored several distance measures on these distributions, including ZKL, a novel variant of KL-divergence. Our best relatedness measure is at the limit of human inter-annotator agreement and is one of the strongest measures of semantic relatedness that uses only WordNet as its underlying lexical resource.In future work, we hope to integrate other lexical resources such as Wikipedia into the walk. Incorporating more types of links from more resources will underline the importance of determining appropriate rela tive weights for all of the types of edges in the walk?s matrix. Even for WordNet, we believe that certain link types, such as antonyms, may be more or less appropriate for certain tasks and should weighted accordingly. And while our measure of lexical relatedness correlates well with human judgments, we hope to show performance gains in a real-word task from the use of our measure. Acknowledgments Thanks to Christopher D. Manning and Dan Jurafsky for their helpful comments and suggestions. We are also grateful to Siddharth Patwardhan and Ted Pedersen for assistance in comparing against their system. Thanks to Sushant Prakash, Rion Snow, and Varun Ganapathi fortheir advice on pursuing some of the ideas in this paper, and to our anonymous reviewers for their helpful cri tiques. Daniel Ramage was funded in part by an NDSEG fellowship. This work was also supported in part by the DTO AQUAINT Program, the DARPA GALE Program, and the ONR (MURI award N000140510388). 588 ", "summary_sents": ["Many systems for tasks such as question answering, multi-document summarization, and information retrieval need robust numerical measures of lexical relatedness.", "Standard thesaurus-based measures of word pair similarity are based on only a single path between those words in the thesaurus graph.", "By contrast, we propose a new model of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the entire graph.", "Our model uses a random walk over nodes and edges derived from WordNet links and corpus statistics.", "We treat the graph as a Markov chain and compute a word-specific stationary distribution via a generalized PageRank algorithm.", "Semantic relatedness of a word pair is scored by a novel divergence measure, ZKL, that outperforms existing measures on certain classes of distributions.", "In our experiments, the resulting relatedness measure is the WordNet-based measure most highly correlated with human similarity judgments by rank ordering at ? =.90.", "we use random walks over WordNet, incorporating information such as meronymy and dictionary glosses."]}
{"title": "Nymble: A High-Performance Learning Name-Finder", "abstract": "This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach. ", "introduction": "", "conclusion": "", "summary_sents": ["This paper presents a statistical, learned approach to finding names and other non-recursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model.", "We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.", "We develop Nymble, an HMM-based name tagging system operating in English and Spanish.", "Nymble uses statistical learning to acquire a Hidden Markov Model (HMM) that recognises NEs in text."]}
{"title": "Termight: Identifying And Translating Technical Terminology", "abstract": "1993). We that part of speech tagging and word alignment could have an important role in glossary construction for translation. Glossaries are extremely important for translation. How would Microsoft, or some other software vendor, want the term &quot;Character menu&quot; to be translated in their manuals? Technical terms are difficult for translators because they are generally not as familiar with the subject domain as either the author of the source text or the reader of the target text. In many cases, there may be a number of acceptable translations, but it is important for the sake of consistency to standardize on a single one. It would be unacceptable for a manual to use a variety of synonyms for a particular menu or button. Customarily, translation houses make extensive job-specific glossaries to ensure consistency and correctness of technical terminology for large jobs. A glossary is a list of terms and their translations.' We will subdivide the task of constructing a glossary into two subtasks: (1) generating a list of terms, and (2) finding the translation equivalents. The first task will be referred to as the monolingual task and the second as the bilingual task. How should a glossary be constructed? Translation schools teach their students to read as much background material as possible in both the source and target languages, an extremely time-consuming ", "introduction": "", "conclusion": "We have shown that terminology research provides a good application for robust natural language technology, in particular for part-of-speech tagging and word-alignment algorithms. Although the output of these algorithms is far from perfect, it is possible to extract from it useful information that is later corrected and augmented by a user. Our extraction algorithms emphasize completeness, and identify also infrequent candidates that may not meet some of the statistical significance criteria proposed in the literature. To make the entire process efficient, however, it is necessary to analyze the user's work process and provide interfaces that support it. In many cases, improving the way information is presented to the user may have a larger effect on productivity than improvements in the underlying natural language technology. In particular, we have found the following to be very effective: As the need for efficient knowledge acquisition tools becomes widely recognized, we hope that this experience with termight will be found useful for other text-related systems as well. ", "summary_sents": ["We propose a semi-automatic tool, termight, that helps professional translators and terminologists identify technical terms and their translations.", "The tool makes use of part-of-speech tagging and word-alignment programs to extract candidate terms and their translations.", "Although the extraction programs are far from perfect, it isn't too hard for the user to filter out the wheat from the chaff.", "The extraction algorithms emphasize completeness.", "Alternative proposals are likely to miss important but infrequent terms/translations.", "To reduce the burden on the user during the filtering phase, candidates are presented in a convenient order, along with some useful concordance evidence, in an interface that is designed to minimize keystrokes.", "Termight is currently being used by the translators at AT&T Business Translation Services (formerly AT&T Language Line Services)."]}
{"title": "Forest-Based Translation", "abstract": "Among syntax-based translation models, the which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart. However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors. We a that translates a packed forest of exponentially many parses, which encodes many more alternatives standard lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. ", "introduction": "Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is to take as input k-best parses, instead of a single tree. This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse. However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 < 50 < 26), and many subtrees are repeated across different parses (Huang, 2008). It is thus inefficient either to decode separately with each of these very similar trees. Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length. We instead propose a new approach, forest-based translation (Section 3), where the decoder translates a packed forest of exponentially many parses,1 which compactly encodes many more alternatives than k-best parses. This scheme can be seen as a compromise between the string-based and treebased methods, while combining the advantages of both: decoding is still fast, yet does not commit to a single parse. Large-scale experiments (Section 4) show an improvement of 1.7 BLEU points over the 1-best baseline, which is also 0.8 points higher than decoding with 30-best trees, and takes even less time thanks to the sharing of common subtrees. ", "conclusion": "We have presented a novel forest-based translation approach which uses a packed forest rather than the 1-best parse tree (or k-best parse trees) to direct the translation. Forest provides a compact data-structure for efficient handling of exponentially many tree structures, and is shown to be a promising direction with state-of-the-art translation results and reasonable decoding speed. This work can thus be viewed as a compromise between string-based and tree-based paradigms, with a good trade-off between speed and accuarcy. For future work, we would like to use packed forests not only in decoding, but also for translation rule extraction during training. ", "summary_sents": ["Among syntax-based translation models, the tree-based approach, which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart.", "However, current tree-based systems suffer from a major drawback: they only use the 2-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors.", "We propose a forest-based approach that translates a packed forest of exponentially many parses, which encodes many more alternatives than standard n-best lists.", "Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline.", "This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time.", "At decoding time, we parse the input sentences into trees, and convert them into translation forest by rule pattern matching.", "We propose the first direct use of packed forest."]}
{"title": "The Senseval-3 English Lexical Sample Task", "abstract": "This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise. The task drew the participation of 27 teams from around the world, with a total of 47 systems. ", "introduction": "We describe in this paper the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise. The goal of this task was to create a framework for evaluation of systems that perform targeted Word Sense Disambiguation. This task is a follow-up to similar tasks organized during the SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Preiss and Yarowsky, 2001) evaluations. The main changes in this year\u2019s evaluation consist of a new methodology for collecting annotated data (with contributions from Web users, as opposed to trained lexicographers), and a new sense inventory used for verb entries (Wordsmyth). 2 Building a Sense Tagged Corpus with Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002) 1. To overcome the current lack of sense tagged data and the limitations imposed by the creation of such data using trained lexicographers, the OMWE system enables the collection of semantically annotated corpora over the Web. Sense tagged examples are collected using a Web-based application that allows contributors to annotate words with their meanings. The tagging exercise proceeds as follows. For each target word the system extracts a set of sentences from a large textual corpus. These examples are presented to the contributors, who are asked to select the most appropriate sense for the target word in each sentence. The selection is made using checkboxes, which list all possible senses of the current target word, plus two additional choices, \u201cunclear\u201d and \u201cnone of the above.\u201d Although users are encouraged to select only one meaning per word, the selection of two or more senses is also possible. The results of the classification submitted by other users are not presented to avoid artificial biases. Similar to the annotation scheme used for the English lexical sample at SENSEVAL-2, we use a \u201ctag until two agree\u201d scheme, with an upper bound on the number of annotations collected for each item set to four. The data set used for the SENSEVAL-3 English lexical sample task consists of examples extracted from the British National Corpus (BNC). Earlier versions of OMWE also included data from the Penn Treebank corpus, the Los Angeles Times collection as provided during TREC conferences (http://trec.nist.gov), and Open Mind Common Sense (http://commonsense.media.mit.edu). The sense inventory used for nouns and adjectives is WordNet 1.7.1 (Miller, 1995), which is consistent with the annotations done for the same task during SENSEVAL-2. Verbs are instead annotated with senses from Wordsmyth (http://www.wordsmyth.net/). The main reason motivating selection of a different sense inventory is the weak verb performance of systems participating in the English lexical sample in SENSEVAL-2, which may be due to the high number of senses defined for verbs in the WordNet sense inventory. By choosing a different set of senses, we hope to gain insight into the dependence of difficulty of the sense disambiguation task on sense inventories. Table 1 presents the number of words under each part of speech, and the average number of senses for each class. For this evaluation exercise, we decided to isolate the task of semantic tagging from the task of identifying multi-word expressions; we applied a filter that removed all examples pertaining to multi-word expressions prior to the tagging phase. Consequently, the training and test data sets made available for this task do not contain collocations as possible target words, but only single word units. This is a somewhat different definition of the task as compared to previous similar evaluations; the difference may have an impact on the overall performance achieved by systems participating in the task. The inter-tagger agreement obtained so far is closely comparable to the agreement figures previously reported in the literature. Kilgarriff (2002) mentions that for the SENSEVAL-2 nouns and adjectives there was a 66.5% agreement between the first two taggings (taken in order of submission) entered for each item. About 12% of that tagging consisted of multi-word expressions and proper nouns, which are usually not ambiguous, and which are not considered during our data collection process. So far we measured a 62.8% inter-tagger agreement between the first two taggings for single word tagging, plus close-to-100% precision in tagging multi-word expressions and proper nouns (as mentioned earlier, this represents about 12% of the annotated data). This results in an overall agreement of about 67.3% which is reasonable and closely comparable with previous figures. Note that these figures are collected for the entire OMWE data set build so far, which consists of annotated data for more than 350 words. In addition to raw inter-tagger agreement, the kappa statistic, which removes from the agreement rate the amount of agreement that is expected by chance(Carletta, 1996), was also determined. We measure two figures: micro-average , where number of senses, agreement by chance, and are determined as an average for all words in the set, and macro-average , where inter-tagger agreement, agreement by chance, and are individually determined for each of the words in the set, and then combined in an overall average. With an average of five senses per word, the average value for the agreement by chance is measured at 0.20, resulting in a micro- statistic of 0.58. For macro- estimations, we assume that word senses follow the distribution observed in the OMWE annotated data, and under this assumption, the macro- is 0.35. ", "conclusion": "The English lexical sample task in SENSEVAL3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense. The objective of this task was to: (1) Determine feasibility of reliably finding the English lexical sample Word Sense Disambiguation task. Precision and recall figures are provided for both fine grained and coarse grained scoring. Corresponding team and reference to system description (in this volume) are indicated for the first system for each team. appropriate sense for words with various degrees of polysemy, using different sense inventories; and (2) Determine the usefulness of sense annotated data collected over the Web (as opposed to other traditional approaches for building semantically annotated corpora). The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation. Additionally, this task has highlighted creation of testing and training data by leveraging the knowledge of Web volunteers. The training and test data sets used in this exercise are available online from http://www.senseval.org and http://teach-computers.org. ", "summary_sents": ["This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise.", "The task drew the participation of 27 teams from around the world, with a total of 47 systems."]}
{"title": "Incremental Integer Linear Programming For Non-Projective Dependency Parsing", "abstract": "Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints. However, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable. We present an approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs. This approach is applied to Dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over stateof-the-art. ", "introduction": "Many inference algorithms require models to make strong assumptions of conditional independence between variables. For example, the Viterbi algorithm used for decoding in conditional random fields requires the model to be Markovian. Strong assumptions are also made in the case of McDonald et al.\u2019s (2005b) non-projective dependency parsing model. Here attachment decisions are made independently of one another'. However, often such assumptions can not be justified. For example in dependency parsing, if a subject has already been identified for a given verb, then the probability of attaching a second subject to the verb is zero. Similarly, if we find that one coordination argument is a noun, then the other argu'If we ignore the constraint that dependency trees must be cycle-free (see sections 2 and 3 for details). ment cannot be a verb. Thus decisions are often co-dependent. Integer Linear Programming (ILP) has recently been applied to inference in sequential conditional random fields (Roth and Yih, 2004), this has allowed the use of truly global constraints during inference. However, it is not possible to use this approach directly for a complex task like non-projective dependency parsing due to the exponential number of constraints required to prevent cycles occurring in the dependency graph. To model all these constraints explicitly would result in an ILP formulation too large to solve efficiently (Williams, 2002). A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem (Germann et al., 2001). In this paper we present a method which extends the applicability of ILP to a more complex set of problems. Instead of adding all the constraints we wish to capture to the formulation, we first solve the program with a fraction of the constraints. The solution is then examined and, if required, additional constraints are added. This procedure is repeated until all constraints are satisfied. We apply this dependency parsing approach to Dutch due to the language\u2019s non-projective nature, and take the parser of McDonald et al. (2005b) as a starting point for our model. In the following section we introduce dependency parsing and review previous work. In Section 3 we present our model and formulate it as an ILP problem with a set of linguistically motivated constraints. We include details of an incremental algorithm used to solve this formulation. Our experimental set-up is provided in Section 4 and is followed by results in Section 5 along with runtime experiments. We finally discuss future research and potential improvements to our approach. ", "conclusion": "In this paper we have presented a novel approach for inference using ILP. While previous approaches which use ILP for decoding have solved each integer linear program in one run, we incrementally add constraints and solve the resulting program until no more constraints are violated. This allows us to efficiently use ILP for dependency parsing and add constraints which provide a significant improvement over the current stateof-the-art parser (McDonald et al., 2005b) on the Dutch Alpino corpus (see bl row in Table 1). Although slower than the baseline approach, our method can still parse large sentences (more than 50 tokens) in a reasonable amount of time (less than a minute). We have shown that parsing time can be significantly reduced using a simple approximation which only marginally degrades performance. Furthermore, we believe that the method has potential for further extensions and applications. ", "summary_sents": ["Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints.", "However, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable.", "We present an approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs.", "This approach is applied to Dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over state-of-the-art.", "For dependency parsing, we study a method using integer linear programming which can incorporate global linguistic constraints.", "Our work in dependency parsing demonstrate that it is possible to use ILP to perform efficient inference for very large programs when used in an incremental manner.", "We show that even exponentially large decoding problems may be solved efficiently using ILP solvers if a Cutting-Plane Algorithm (Dantzig et al, 1954) is used.", "We tackle the MAP problem for dependency parsing by an incremental approach that starts with a relaxation of the problem, solves it, and adds additional constraints only if they are violated."]}
{"title": "KenLM: Faster and Smaller Language Model Queries", "abstract": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs. The structure uses linear probing hash tables and is designed for speed. Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline. Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations. ", "introduction": "Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries. This paper presents methods to query N-gram language models, minimizing time and space costs. Queries take the form p(wn|wn\u22121 1 ) where wn1 is an n-gram. Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn\u22121 f ) and backoff penalties b(wn\u22121 i ) are given by an already-estimated model. The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient. Many packages perform language model queries. Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders. IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption. MITLM 0.4 (Hsu and Glass, 2008) is mostly designed for accurate model estimation, but can also compute perplexity. RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. BerkeleyLM revision 152 (Pauls and Klein, 2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization. Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques, but did not release code. TPT Germann et al. (2009) describe tries with better locality properties, but did not release code. These packages are further described in Section 3. We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives. Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated. Our open-source (LGPL) implementation is also available for download as a standalone package with minimal (POSIX and g++) dependencies. ", "conclusion": "We have described two data structures for language modeling that achieve substantial reductions in time and memory cost. The PROBING model is 2.4 times as fast as the fastest alternative, SRILM, and uses less memory too. The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM. These performance gains transfer to improved system runtime performance; though we focused on Moses, our code is the best lossless option with cdec and Joshua. We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns. The code is opensource, has minimal dependencies, and offers both C++ and Java interfaces for integration. ", "summary_sents": ["We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs.", "The PROBING data structure uses linear probing hash tables and is designed for speed.", "Compared with the widely-used SRILM, our PROBING model is 2.4 times as fast while using 57% of the memory.", "The TRIE data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption.", "TRIE simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline.", "Our code is open-source, thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.", "This paper describes the several performance techniques used and presents benchmarks against alternative implementations.", "We describe a language modeling library."]}
{"title": "Automatic Error Detection In The Japanese Learners' English Spoken Data", "abstract": "This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data. In this paper, we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus, which contains information on learners\u2019 errors. ", "introduction": "One of the most important things in keeping up with our current information-driven society is the acquisition of foreign languages, especially English for international communications. In developing a computer-assisted language teaching and learning environment, we have compiled a large-scale speech corpus of Japanese learner English, which provides a great deal of useful information on the construction of a model for the developmental stages of Japanese learners\u2019 speaking abilities. In the support system for language learning, we have assumed that learners must be informed of what kind of errors they have made, and in which part of their utterances. To do this, we need to have a framework that will allow us to detect learners\u2019 errors automatically. In this paper, we introduce a method of detecting learners\u2019 errors, and we examine to what extent this could be accomplished using our learner corpus data including error tags that are labeled with the learners\u2019 errors. ", "conclusion": "In this paper, we explained how errors in learners\u2019 spoken data could be detected and in the experiment, using the corpus as it was, the recall rate was about 30% and the precision rate was about 50%. By adding corrected sentences and artificially made errors, the precision rate rose to 80% while the recall rate remained the same. ", "summary_sents": ["This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data.", "In this paper, we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus, which contains information on learners' errors.", "We use error annotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions.", "In the Japanese Learners of English corpus (Izumi et al., 2003), errors related to verbs are among the most frequent categories.", "The usage of articles has been found to be the most frequent error class in the JLE (Japanese Learner English) corpus."]}
{"title": "Parsing The WSJ Using CCG And Log-Linear Models", "abstract": "This paper describes and evaluates log-linear parsing models for Combinatory Categorial A parallel implementation of algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation. We also develop a new efficient parsing for maximises expected recall of dependencies. We compare models use all including nonstandard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with ex ", "introduction": "A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and used in parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b). In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG. However, estimating a log-linear model for a widecoverage CCG grammar is very computationally expensive. Following Miyao and Tsujii (2002), we showed how the estimation can be performed efficiently by applying the inside-outside algorithm to a packed chart. We also showed how the complete WSJ Penn Treebank can be used for training by developing a parallel version of Generalised Iterative Scaling (GIS) to perform the estimation. This paper significantly extends our earlier work in a number of ways. First, we evaluate a number of log-linear models, obtaining results which are competitive with the state-of-the-art for CCG parsing. We also compare log-linear models which use all CCG derivations, including non-standard derivations, with normal-form models. Second, we find that GIS is unsuitable for estimating a model of the size being considered, and develop a parallel version of the L-BFGS algorithm (Nocedal and Wright, 1999). And finally, we show that the parsing algorithm described in Clark and Curran (2003) is extremely slow in some cases, and suggest an efficient alternative based on Goodman (1996). The development of parsing and estimation algorithms for models which use all derivations extends existing CCG parsing techniques, and allows us to test whether there is useful information in the additional derivations. However, we find that the performance of the normal-form model is at least as good as the all-derivations model, in our experiments todate. The normal-form approach allows the use of additional constraints on rule applications, leading to a smaller model, reducing the computational resources required for estimation, and resulting in an extremely efficient parser. This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al. (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. ", "conclusion": "A major contribution of this paper has been the development of a parsing model for CCG which uses all derivations, including non-standard derivations. Non-standard derivations are an integral part of the CCG formalism, and it is an interesting question whether efficient estimation and parsing algorithms can be defined for models which use all derivations. We have answered this question, and in doing so developed a new parsing algorithm for CCG which maximises expected recall of dependencies. We would like to extend the dependency model, by including the local-rule dependencies which are used by the normal-form model, for example. However, one of the disadvantages of the dependency model is that the estimation process is already using a large proportion of our existing resources, and extending the feature set will further increase the execution time and memory requirement of the estimation algorithm. We have also shown that a normal-form model performs as well as the dependency model. There are a number of advantages to the normal-form model: it requires less space and time resources for estimation and it produces a faster parser. Our normal-form parser significantly outperforms the parser of Clark et al. (2002) and produces results at least as good as the current state-of-the-art for CCG parsing. The use of adaptive supertagging and the normal-form constraints result in a very efficient wide-coverage parser. Our system demonstrates that accurate and efficient wide-coverage CCG parsing is feasible. Future work will investigate extending the feature sets used by the log-linear models with the aim of further increasing parsing accuracy. Finally, the oracle results suggest that further experimentation with the supertagger will significantly improve parsing accuracy, efficiency and robustness. ", "summary_sents": ["This paper describes and evaluates log-linear parsing models for Combinatory Categorial Grammar (CCG).", "A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation.", "We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies.", "We compare models which use all CCG derivations, including non-standard derivations, with normal-form models.", "The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers.", "Our CCG parser is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second.", "Our parsing peformance relies on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word).", "Our parsing performance provides an indication of how super tagging accuracy corresponds to overall dependency recovery."]}
{"title": "Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked", "abstract": "Recent work in Question Answering has focused on web-based systems that answers using simple lexicosyntactic patterns. We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions. We evaluate our strategy on a challenging subset of questions, i.e. \u201cWho is ...\u201d questions, against a state of the art web-based Question Answering system. Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system. ", "introduction": "Many of the recent advances in Question Answering have followed from the insight that systems can benefit by exploiting the redundancy of information in large corpora. Brill et al. (2001) describe using the vast amount of data available on the World Wide Web to achieve impressive performance with relatively simple techniques. While the Web is a powerful resource, its usefulness in Question Answering is not without limits. The Web, while nearly infinite in content, is not a complete repository of useful information. Most newspaper texts, for example, do not remain accessible on the Web for more than a few weeks. Further, while Information Retrieval techniques are relatively successful at managing the vast quantity of text available on the Web, the exactness required of Question Answering systems makes them too slow and impractical for ordinary users. In order to combat these inadequacies, we propose a strategy in which information is extracted automatically from electronic texts offline, and stored for quick and easy access. We borrow techniques from Text Mining in order to extract semantic relations (e.g., concept-instance relations) between lexical items. We enhance these techniques by increasing the yield and precision of the relations that we extract. Our strategy is to collect a large sample of newspaper text (15GB) and use multiple part of speech patterns to extract the semantic relations. We then filter out the noise from these extracted relations using a machine-learned classifier. This process generates a high precision repository of information that can be accessed quickly and easily. We test the feasibility of this strategy on one semantic relation and a challenging subset of questions, i.e., \u201cWho is ...\u201d questions, in which either a concept is presented and an instance is requested (e.g., \u201cWho is the mayor of Boston?\u201d), or an instance is presented and a concept is requested (e.g., \u201cWho is Jennifer Capriati?\u201d). By choosing this subset of questions we are able to focus only on answers given by concept-instance relationships. While this paper examines only this type of relation, the techniques we propose are easily extensible to other question types. Evaluations are conducted using a set of \u201cWho is ...\u201d questions collected over the period of a few months from the commercial question-based search engine www.askJeeves.com. We extract approximately 2,000,000 concept-instance relations from newspaper text using syntactic patterns and machine-learned filters (e.g., \u201cpresident Bill Clinton\u201d and \u201cBill Clinton, president of the USA,\u201d). We then compare answers based on these relations to answers given by TextMap (Hermjakob et al., 2002), a state of the art web-based question answering system. Finally, we discuss the results of this evaluation and the implications and limitations of our strategy. ", "conclusion": "", "summary_sents": ["Recent work in Question Answering has focused on web-based systems that extract answers using simple lexico-syntactic patterns.", "We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions.", "We evaluate our strategy on a challenging subset of questions, i.e. \u201cWho is \u2026\u201d questions, against a state of the art web-based Question Answering system.", "Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system.", "We use part of speech patterns to extract a subset of hyponym relations involving proper nouns.", "The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise."]}
{"title": "Fully Automatic Lexicon Expansion For Domain-Oriented Sentiment Analysis", "abstract": "This paper proposes an unsupervised lexicon building method for the detecof which convey positive or negative aspects in a specific domain. The lexical entries to be acare called the minimum human-understandable syntactic structures that specify the polarity of clauses. As a clue to obtain candidate atoms, we use the tendency for same polarities to appear successively in contexts. Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values. The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon. ", "introduction": "Sentiment Analysis (SA) (Nasukawa and Yi, 2003; Yi et al., 2003) is a task to recognize writers\u2019 feelings as expressed in positive or negative comments, by analyzing unreadably large numbers of documents. Extensive syntactic patterns enable us to detect sentiment expressions and to convert them into semantic structures with high precision, as reported by Kanayama et al. (2004). From the example Japanese sentence (1) in the digital camera domain, the SA system extracts a sentiment representation as (2), which consists of a predicate and an argument with positive (+) polarity. SA in general tends to focus on subjective sentiment expressions, which explicitly describe an author\u2019s preference as in the above example (1). Objective (or factual) expressions such as in the following examples (3) and (4) may be out of scope even though they describe desirable aspects in a specific domain. However, when customers or corporate users use SA system for their commercial activities, such domain-specific expressions have a more important role, since they convey strong or weak points of the product more directly, and may influence their choice to purchase a specific product, as an example. This paper addresses the Japanese version of Domain-oriented Sentiment Analysis, which identifies polar clauses conveying goodness and badness in a specific domain, including rather objective expressions. Building domain-dependent lexicons for many domains is much harder work than preparing domainindependent lexicons and syntactic patterns, because the possible lexical entries are too numerous, and they may differ in each domain. To solve this problem, we have devised an unsupervised method to acquire domaindependent lexical knowledge where a user has only to collect unannotated domain corpora. The knowledge to be acquired is a domaindependent set of polar atoms. A polar atom is a minimum syntactic structure specifying polarity in a predicative expression. For example, to detect polar clauses in the sentences (3) and (4)i, the following polar atoms (5) and (6) should appear in the lexicon: The polar atom (5) specified the positive polarity of the verb kukkiri-suru. This atom can be generally used for this verb regardless of its arguments. In the polar atom (6), on the other hand, the nominative case of the verb tsuku (\u2018have\u2019) is limited to a specific noun zuumu (\u2018zoom lens\u2019), since the verb tsuku does not hold the polarity in itself. The automatic decision for the scopes of the atoms is one of the major issues. For lexical learning from unannotated corpora, our method uses context coherency in terms of polarity, an assumption that polar clauses with the same polarity appear successively unless the context is changed with adversative expressions. Exploiting this tendency, we can collect candidate polar atoms with their tentative polarities as those adjacent to the polar clauses which have been identified by their domain-independent polar atoms in the initial lexicon. We use both intrasentential and inter-sentential contexts to obtain more candidate polar atoms. Our assumption is intuitively reasonable, but there are many non-polar (neutral) clauses adjacent to polar clauses. Errors in sentence delimitation or syntactic parsing also result in false candidate atoms. Thus, to adopt a candidate polar atom for the new lexicon, some threshold values for the frequencies or ratios are required, but they depend on the type of the corpus, the size of the initial lexicon, etc. Our algorithm is fully automatic in the sense that the criteria for the adoption of polar atoms are set automatically by statistical estimation based on the distributions of coherency: coherent precision and coherent density. No manual tuning process is required, so the algorithm only needs unannotated domain corpora and the initial lexicon. Thus our learning method can be used not only by the developers of the system, but also by endusers. This feature is very helpful for users to 'The English translations are included only for convenience. analyze documents in new domains. In the next section, we review related work, and Section 3 describes our runtime SA system. In Section 4, our assumption for unsupervised learning, context coherency and its key metrics, coherent precision and coherent density are discussed. Section 5 describes our unsupervised learning method. Experimental results are shown in Section 6, and we conclude in Section 7. ", "conclusion": "We proposed an unsupervised method to acquire polar atoms for domain-oriented SA, and demonstrated its high performance. The lexicon can be expanded automatically by using unannotated corpora, and tuning of the threshold values is not required. Therefore even end-users can use this approach to improve the sentiment analysis. These features allow them to do on-demand analysis of more narrow domains, such as the domain of digital &quot;Perhaps because cameras tend to consume battery power and some users don\u2019t need them. cameras of a specific manufacturer, or the domain of mobile phones from the female users\u2019 point of view. ", "summary_sents": ["This paper proposes an unsupervised lexicon building method for the detection of polar clauses, which convey positive or negative aspects in a specific domain.", "The lexical entries to be acquired are called polar atoms, the minimum human-understandable syntactic structures that specify the polarity of clauses.", "As a clue to obtain candidate polar atoms, we use context coherency, the tendency for same polarities to appear successively in contexts.", "Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values.", "The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon.", "We validate that polar text units with the same polarity tend to appear together to make contexts coherent.", "We propose an algorithm to automatically expand an initial opinion lexicon based on context coherency, the tendency for same polarities to appear successively in contexts.", "We use conjunction rules to solve this problem from large domain corpora.", "We adopt domain knowledge by extracting sentiment words from the domain-specific corpus."]}
{"title": "Learning Subjective Language", "abstract": "Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article. ", "introduction": "", "conclusion": "", "summary_sents": ["Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations.", "There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.", "The goal of this work is learning subjective language from corpora.", "Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.", "The features are also examined working together in concert.", "The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.", "In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.", "Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.", "We show that low-frequency words and some collocations are a good indicators of subjectivity."]}
{"title": "Automatic Tagging Of Arabic Text: From Raw Text To Base Phrase Chunks", "abstract": "", "introduction": "Arabic is garnering attention in the NLP community due to its socio-political importance and its linguistic differences from Indo-European languages. These linguistic characteristics, especially dialect differences and complex morphology present interesting challenges for NLP researchers. But like most non-European languages, Arabic is lacking in annotated resources and tools. Fully automated fundamental NLP tools such as Tokenizers, Part Of Speech (POS) Taggers and Base Phrase (BP) Chunkers are still not available for Arabic. Meanwhile, these tools are readily available and have achieved remarkable accuracy and sophistication for the processing of many European languages. With the release of the Arabic Penn TreeBank 1 (v2.0),1 the story is about to change. In this paper, we propose solutions to the problems of Tokenization, POS Tagging and BP Chunking of Arabic text. By Tokenization we mean the process of segmenting clitics from stems, since in Arabic, prepositions, conjunctions, and some pronouns are cliticized (orthographically and phonological fused) onto stems. Separating conjunctions from the following noun, for example, is a key first step in parsing. By POS Tagging, we mean the standard problem of annotating these segmented words with parts of speech drawn from the \u2018collapsed\u2019 Arabic Penn TreeBank POS tagset. Base Phrase (BP) Chunking is the process of creating non-recursive base phrases such as noun phrases, adjectival phrases, verb phrases, preposition phrases, etc. For each of these tasks, we adopt a supervised machine learning perspective using Support Vector Machines (SVMs) trained on the Arabic TreeBank, leveraging off of already existing algorithms for English. The results are comparable to state-of-the-art results on English text when trained on similar sized data. ", "conclusion": "We have presented a machine-learning approach using SVMs to solve the problem of automatically annotating Arabic text with tags at different levels; namely, tokenization at morphological level, POS tagging at lexical level, and BP chunking at syntactic level. The technique is language independent and highly accurate with an score of 99.12 on the tokenization task, 95.49% accuracy on the POS tagging task and score of 92.08 on the BP Chunking task. To the best of our knowledge, these are the first results reported for these tasks in Arabic natural language processing. We are currently trying to improve the performance of the systems by using additional features, a wider context and more data created semi-automatically using an unannotated large Arabic corpus. In addition, we are trying to extend the approach to semantic chunking by handlabeling a part of Arabic TreeBank with arguments or semantic roles for training. ", "summary_sents": ["To date, there are no fully automated systems addressing the community's need for fundamental language processing tools for Arabic text.", "In this paper, we present a Support Vector Machine (SVM) based approach to automatically tokenize (segmenting off clitics), part-of- speech (POS) tag and annotate base phrases (BPs) in Arabic text.", "We adapt highly accurate tools that have been developed for English text and apply them to Arabic text.", "Khoja (2001) first introduced a tagger for Arabic, which has 131 tags, but this work has collapsed the tag set to simplify tagging.", "We describe a part-of-speech tagger based on support vector machines that is trained on tokenized data (clitics are separate tokens), reporting a tagging accuracy of 95.5%."]}
{"title": "Thumbs Up? Sentiment Classification Using Machine Learning Techniques", "abstract": "We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging. ", "introduction": "Today, very large amounts of information are available in on-line documents. As part of the effort to better organize this information for users, researchers have been actively investigating the problem of automatic text categorization. The bulk of such work has focused on topical categorization, attempting to sort documents according to their subject matter (e.g., sports vs. politics). However, recent years have seen rapid growth in on-line discussion groups and review sites (e.g., the New York Times\u2019 Books web page) where a crucial characteristic of the posted articles is their sentiment, or overall opinion towards the subject matter \u2014 for example, whether a product review is positive or negative. Labeling these articles with their sentiment would provide succinct summaries to readers; indeed, these labels are part of the appeal and value-add of such sites as www.rottentomatoes.com, which both labels movie reviews that do not contain explicit rating indicators and normalizes the different rating schemes that individual reviewers use. Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye\u2019s Lexant system') and recommender systems (e.g., Terveen et al. (1997), Tatemura (2000)), where user input and feedback could be quickly summarized; indeed, in general, free-form survey responses given in natural language format could be processed using sentiment categorization. Moreover, there are also potential applications to message filtering; for example, one might be able to use sentiment information to recognize and discard \u201cflames\u201d(Spertus, 1997). In this paper, we examine the effectiveness of applying machine learning techniques to the sentiment classification problem. A challenging aspect of this problem that seems to distinguish it from traditional topic-based classification is that while topics are often identifiable by keywords alone, sentiment can be expressed in a more subtle manner. For example, the sentence \u201cHow could anyone sit through this movie?\u201d contains no single word that is obviously negative. (See Section 7 for more examples). Thus, sentiment seems to require more understanding than the usual topic-based classification. So, apart from presenting our results obtained via machine learning techniques, we also analyze the problem to gain a better understanding of how difficult it is. ", "conclusion": "", "summary_sents": ["We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative.", "Using movie reviews as data, we find that standard machine learning techniques deflnitively outperform human-produced baselines.", "However, the three machine learning methods we employed (Naive Bayes, maximum entropy classiflcation, and support vector machines) do not perform as well on sentiment classiflcation as on traditional topic-based categorization.", "We conclude by examining factors that make the sentiment classiflcation problem more challenging.", "We collect reviews form a movie database and rate them as positive, negative or neutral based on the training given by the reviewer.", "We suggest that term-based models perform better than the frequency-based alternatives."]}
{"title": "cdec: A Decoder Alignment and Learning Framework for Finite-State and Context-Free Translation Models", "abstract": "present an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can not only the 1or translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. ", "introduction": "The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. In cdec, model-specific code is only required to construct a translation forest (\u00a73). General rescoring (with language models or other models), pruning, inference, and alignment algorithms then apply to the unified data structure (\u00a74). Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc. ); new models can be more easily prototyped; and controlled comparison of models is made easier. Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009). Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (\u00a75), any model type can be trained using with any of the supported training criteria. The software package includes general function optimization utilities that can be used for discriminative training (\u00a76). These features are implemented without compromising on performance. We show experimentally that cdec uses less memory and time than comparable decoders on a controlled translation task (\u00a77). ", "conclusion": "", "summary_sents": ["We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars.", "Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms.", "From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques.", "Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.", "We present cdec, a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system.", "Our cdec decoder learns word segmentation lattices from raw text in an unsupervised manner."]}
{"title": "Introduction To The CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition", "abstract": "", "introduction": "Named entities are phrases that contain the names of persons, organizations and locations. Example: [ORG U.N. ] official [PER Ekeus ] heads for [LOC Baghdad ] . This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location. Named entity recognition is an important task of information extraction systems. There has been a lot of work on named entity recognition, especially for English (see Borthwick (1999) for an overview). The Message Understanding Conferences (MUC) have offered developers the opportunity to evaluate systems for English on the same data in a competition. They have also produced a scheme for entity annotation (Chinchor et al., 1999). More recently, there have been other system development competitions which dealt with different languages (IREX and CoNLL-2002). The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002). The participants of the 2003 shared task have been offered training and test data for two other European languages: English and German. They have used the data for developing a named-entity recognition system that includes a machine learning component. The shared task organizers were especially interested in approaches that made use of resources other than the supplied training data, for example gazetteers and unannotated data. ", "conclusion": "", "summary_sents": ["We describe the CoNLL-2003 shared task: language-independent named entity recognition.", "We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance."]}
{"title": "An Algorithm For Finding Noun Phrase Correspondences In Bilingual Corpora", "abstract": "The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus. The taggers provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages. Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers. The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated. Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings. ", "introduction": "", "conclusion": "", "summary_sents": ["The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus.", "The taggets provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages.", "Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers.", "The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated.", "Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings.", "We attempt to find noun phrase correspondence in parallel corpora using part-of-speech tagging and noun phrase recognition methods."]}
{"title": "Trainable Methods For Surface Natural Language Generation", "abstract": "We present three systems for surface natural language generation that are trainable from annotated corpora. The first two systems, called NLG1 and NLG2, require a corpus marked only with domainspecific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information. All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation. NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase. The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase. We present experiments in which we generate phrases to describe flights in the air travel domain. ", "introduction": "This paper presents three trainable systems for surface natural language generation (NLG). Surface NLG, for our purposes, consists of generating a grammatical natural language phrase that expresses the meaning of an input semantic representation. The systems take a &quot;corpus-based&quot; or &quot;machinelearning&quot; approach to surface NLG, and learn to generate phrases from semantic input by statistically analyzing examples of phrases and their corresponding semantic representations. The determination of the content in the semantic representation, or &quot;deep&quot; generation, is not discussed here. Instead, the systems assume that the input semantic representation is fixed and only deal with how to express it in natural language. This paper discusses previous approaches to surface NLG, and introduces three trainable systems for surface NLG, called NLG1, NLG2, and NLG3. Quantitative evaluation of experiments in the air travel domain will also be discussed. ", "conclusion": "This paper presents the first systems (known to the author) that use a statistical learning approach to produce natural language text directly from a semantic representation. Information to solve the attribute ordering and lexical choice problems\u2014 which would normally be specified in a large handwritten grammar\u2014 is automatically collected from data with a few feature patterns, and is combined via the maximum entropy framework. NLG2 shows that using just local n-gram information can outperform the baseline, and NLG3 shows that using syntactic information can further improve generation accuracy. We conjecture that NLG2 and NLG3 should work in other domains which have a complexity similar to air travel, as well as available annotated data. ", "summary_sents": ["We present three systems for surface natural language generation that are trainable from annotated corpora.", "The first two systems, called NLG1 and NLG2, require a corpus marked only with domain-specific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information.", "All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation.", "NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.", "The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase.", "We present experiments in which we generate phrases to describe flights in the air travel domain.", "We use maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features.", "We use a large collection of generation templates for surface realization.", "We present maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain."]}
{"title": "A Semantic-Head-Driven Generation Algorithm For Unification-Based Formalisms", "abstract": "We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable. In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. ", "introduction": "The problem of generating a well-formed naturallanguage expression from an encoding of its meaning possesses certain properties which distinguish it from the converse problem of recovering a meaning encoding from a given natural-language expression. In previous work (Shieber, 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural-language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983) but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural-language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose well-foundedness relies on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property, discussed further in Section 3.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation\u2014as it might be called\u2014has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance of a uniform architecture for parsing and generation\u2014just as the extended Earley parser (Shieber, 1985b) and the bottom-up generator were instances of the generalized Earley deduction architecture\u2014our efforts to date have been aimed foremost toward the development of the algorithm for generation alone. We will have little to say about its relation to parsing, leaving such questions for later research.' 2 Applicability of the Algorithm As does the Earley-based generator, the new algorithm assumes that the grammar is a unificationbased or logic grammar with a phrase-structure backbone and complex nonterminals. Furthermore, and again consistent with previous work, we assume that the nonterminals associate to the phrases they describe logical expressions encoding their possible meanings. We will describe the algorithm in terms of an implementation of it for definite-clause grammars (DCG), although we beI Martin Kay (personal communication) has developed a parsing algorithm that seems to be the parsing correlate to the generation algorithm presented here. Its existence might point the way towards a uniform architecture. lieve the underlying method to be more broadly applicable. A variant of our method is used in Van Noord's BUG (Bottom-Up Generator) system, part of MiMo2, an experimental machine translation system for translating international news items of Teletext, which uses a Prolog version of PATR-II similar to that of Hirsh (1987). According to Martin Kay (personal communication), the STREP machine translation project at the Center for the Study of Language and Information uses a version of our algorithm to generate with respect to grammars based on head-driven phrase-structure grammar (HPSG). Finally, Calder et al. (1989) report on a generation algorithm for unification categorial grammar that appears to be a special case of ours. ", "conclusion": "", "summary_sents": ["We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable.", "In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion.", "The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion.", "We point out a termination problem in the left-recursive rules."]}
{"title": "An Unsupervised Method For Detecting Grammatical Errors", "abstract": "We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora. The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL). The errorrecognition system, ALEK, performs with about 80% precision and 20% recall. ", "introduction": "A good indicator of whether a person knows the meaning of a word is the ability to use it appropriately in a sentence (Miller and Gildea, 1987). Much information about usage can be obtained from quite a limited context: Choueka and Lusignan (1985) found that people can typically recognize the intended sense of a polysemous word by looking at a narrow window of one or two words around it. Statistically-based computer programs have been able to do the same with a high level of accuracy (Kilgarriff and Palmer, 2000). The goal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word. We have developed a statistical system, ALEK (Assessing Lexical Knowledge), that uses statistical analysis for this purpose. A major objective of this research is to avoid the laborious and costly process of collecting errors (or negative evidence) for each word that we wish to evaluate. Instead, we train ALEK on a general corpus of English and on edited text containing example uses of the target word. The system identifies inappropriate usage based on differences between the word's local context cues in an essay and the models of context it has derived from the corpora of well-formed sentences. A requirement for ALEK has been that all steps in the process be automated, beyond choosing the words to be tested and assessing the results. Once a target word is chosen, preprocessing, building a model of the word's appropriate usage, and identifying usage errors in essays is performed without manual intervention. ALEK has been developed using the Test of English as a Foreign Language (TOEFL) administered by the Educational Testing Service. TOEFL is taken by foreign students who are applying to US undergraduate and graduate-level programs. ", "conclusion": "The unsupervised techniques that we have presented for inferring negative evidence are effective in recognizing grammatical errors in written text. Preliminary results indicate that ALEK's error detection is predictive of TOEFL scores. If ALEK accurately detects usage errors, then it should report more errors in essays with lower scores than in those with higher scores. We have already seen in Table 1 that there is a negative correlation between essay score and two of ALEK's component measures, the general corpus n-grams. However, the data in Table 1 were not based on specific vocabulary items and do not reflect overall system performance, which includes the other measures as well. Table 6 shows the proportion of test word occurrences that were classified by ALEK as containing errors within two positions of the target at each of 6 TOEFL score points. As predicted, the correlation is negative (F., = -1.00, n = 6,p <.001, two-tailed). These data support the validity of the system as a detector of inappropriate usage, even when only a limited number of words are targeted and only the immediate context of each target is examined. ALEK and by a human judge For comparison, Table 6 also gives the estimated proportions of inappropriate usage by score point based on the human judge's classification. Here, too, there is a negative correlation: rs = \u2013.90, n = 5, p < .05, two-tailed. Although the system recognizes a wide range of error types, as Table 6 shows, it detects only about one-fifth as many errors as a human judge does. To improve recall, research needs to focus on the areas identified in section 3.2 and, to improve precision, efforts should be directed at reducing the false positives described in 3.3. ALEK is being developed as a diagnostic tool for students who are learning English as a foreign language. However, its techniques could be incorporated into a grammar checker for native speakers. ", "summary_sents": ["We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.", "The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL).", "The error-recognition system, ALEK, performs with about 80% precision and 20% recall.", "We attempt to identify errors on the basis of context -- more specifically a 2 word window around the word of interest, from which we consider function words and POS tags.", "We use a mutual information measure in addition to raw frequency of n grams.", "The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors.", "We utilize mutual information and chi-square statistics to identify typical contexts for a small set of targeted words from a large well-formed corpus."]}
{"title": "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections", "abstract": "We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm. ", "introduction": "Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems. Supervised part-of-speech (POS) taggers, for example, approach the level of inter-annotator agreement (Shen et al., 2007, 97.3% accuracy for English). However, supervised methods rely on labeled training data, which is time-consuming and expensive to generate. Unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for training models. Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best. To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language. This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al., 2000; Xi and Hwa, 2005; Ganchev et al., 2009). Naseem et al. (2009) and Snyder et al. (2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available. Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. First, we use a novel graph-based framework for projecting syntactic information across language boundaries. To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\u00a73), and then use graph label propagation to project syntactic information from English to the foreign language (\u00a74). Second, we treat the projected labels as features in an unsupervised model (\u00a75), rather than using them directly for supervised training. To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011). Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction. Because there might be some controversy about the exact definitions of such universals, this set of coarse-grained POS categories is defined operationally, by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages. These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels. We evaluate our approach on eight European languages (\u00a76), and show that both our contributions provide consistent and statistically significant improvements. Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%). ", "conclusion": "We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages. Because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs. Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data, but have translations into a resource-rich language. Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models. ", "summary_sents": ["We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.", "Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.", "We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010).", "Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.", "We build a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text."]}
{"title": "Unsupervised Multilingual Learning for Morphological Segmentation", "abstract": "For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme pator We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family. ", "introduction": "For centuries, the deep connection between human languages has fascinated linguists, anthropologists and historians (Eco, 1995). The study of this connection has made possible major discoveries about human communication: it has revealed the evolution of languages, facilitated the reconstruction of proto-languages, and led to understanding language universals. The connection between languages should be a powerful source of information for automatic linguistic analysis as well. In this paper we investigate two questions: (i) Can we exploit cross-lingual correspondences to improve unsupervised language learning? (ii) Will this joint analysis provide more or less benefit when the languages belong to the same family? We study these two questions in the context of unsupervised morphological segmentation, the automatic division of a word into morphemes (the basic units of meaning). For example, the English word misunderstanding would be segmented into mis understand - ing. This task is an informative testbed for our exploration, as strong correspondences at the morphological level across various languages have been well-documented (Campbell, 2004). The model presented in this paper automatically induces a segmentation and morpheme alignment from a multilingual corpus of short parallel phrases.1 For example, given parallel phrases meaning in my land in English, Arabic, Hebrew, and Aramaic, we wish to segment and align morphemes as follows: ", "conclusion": "We started out by posing two questions: (i) Can we exploit cross-lingual patterns to improve unsupervised analysis? (ii) Will this joint analysis provide more or less benefit when the languages belong to the same family? The model and results presented in this paper answer the first question in the affirmative, at least for the task of morphological segmentation. We also provided some evidence that considering closely related languages may be more beneficial than distant pairs if the model is able to explicitly represent shared language structure (the characterto-character phonetic correspondences in our case). In the future, we hope to apply similar multilingual models to other core unsupervised analysis tasks, including part-of-speech tagging and grammar induction, and to further investigate the role that language relatedness plays in such models. 7 ", "summary_sents": ["For centuries, the deep connection between languages has brought about major discoveries about human communication.", "In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning.", "In particular, we study the task of morphological segmentation of multiple languages.", "We present a non-parametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme patterns, or abstract morphemes.", "We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English.", "Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models.", "Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family.", "We use bilingual information but the segmentation is learned independently from translation modeling."]}
{"title": "Identifying Agreement And Disagreement In Conversational Speech: Use Of Bayesian Networks To Model Pragmatic Dependencies", "abstract": "We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work. ", "introduction": "One of the main features of meetings is the occurrence of agreement and disagreement among participants. Often meetings include long stretches of controversial discussion before some consensus decision is reached. Our ultimate goal is automated summarization of multi-participant meetings and we hypothesize that the ability to automatically identify agreement and disagreement between participants will help us in the summarization task. For example, a summary might resemble minutes of meetings with major decisions reached (consensus) along with highlighted points of the pros and cons for each decision. In this paper, we present a method to automatically classify utterances as agreement, disagreement, or neither. Previous work in automatic identification of agreement/disagreement (Hillard et al., 2003) demonstrates that this is a feasible task when various textual, durational, and acoustic features are available. We build on their approach and show that we can get an improvement in accuracy when contextual information is taken into account. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational and structural features that look both forward and backward in the discourse. This allows us to acquire, and subsequently process, knowledge about who speaks to whom. We hypothesize that pragmatic features that center around previous agreement between speakers in the dialog will influence the determination of agreement/disagreement. For example, if a speaker disagrees with another person once in the conversation, is he more likely to disagree with him again? We model context using Bayesian networks that allows capturing of these pragmatic dependencies. Our accuracy for classifying agreements and disagreements is 86.9%, which is a 4.9% improvement over (Hillard et al., 2003). In the following sections, we begin by describing the annotated corpus that we used for our experiments. We then turn to our work on identifying adjacency pairs. In the section on identification of agreement/disagreement, we describe the contextual features that we model and the implementation of the classifier. We close with a discussion of future work. ", "conclusion": "We have shown how identification of adjacency pairs can help in designing features representing pragmatic dependencies between agreement and disagreement labels. These features are shown to be informative and to help the classification task, yielding a substantial improvement (1.3% to reach a 86.9% accuracy in three-way classification). We also believe that the present work may be useful in other computational pragmatic research focusing on multi-party dialogs, such as dialog act (DA) classification. Most previous work in that area is limited to interaction between two speakers (e.g. Switchboard, (Stolcke et al., 2000)). When more than two speakers are involved, the question of who is the addressee of an utterance is crucial, since it generally determines what DAs are relevant after the addressee\u2019s last utterance. So, knowledge about adjacency pairs is likely to help DA classification. In future work, we plan to extend our inference process to treat speaker ranking (i.e. AP identification) and agreement/disagreement classification as a single, joint inference problem. Contextual information about agreements and disagreements can also provide useful cues regarding who is the addressee of a given utterance. We also plan to incorporate acoustic features to increase the robustness of our procedure in the case where only speech recognition output is available. ", "summary_sents": ["We describe a statistical approach for modeling agreements and disagreements in conversational interaction.", "Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse.", "We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance.", "Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.", "We define an adjacent pair to consist of two parts that are ordered, adjacent, and produced by different speakers.", "We also achieved an 8% increase in speaker identification.", "We consider speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network.", "We suggest that further gains can be achieved by augmenting the feature set."]}
{"title": "TINA: A Natural Language System For Spoken Language Applications", "abstract": "new natural language system, been developed for applications involving spoken tasks. key ideas from context free grammars, Augmented Transition (ATN's), and the unification concept. a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance. An initial set of context-free rewrite rules provided by hand is first converted to a network structure. Probability assignments on all arcs in the network are obtained automatically from a set of example sentences. The parser uses a stack decoding search strategy, with a top-down control flow, and includes a feature-passing mechanism to deal long-distance movement, agreement, and semantic constraints. an automatic sentence generation capability that has been effective for identifying overgeneralization problems as well as in producing a word-pair language model for a recognizer. The parser is currently with MIT's for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process. ", "introduction": "", "conclusion": "", "summary_sents": ["A new natural language system, TINA, has been developed for applications involving spoken language tasks.", "TINA integrates key ideas from context free grammars, Augmented Transition Networks (ATN's), and the unification concept.", "TINA provides a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance.", "An initial set of context-free rewrite rules provided by hand is first converted to a network structure.", "Probability assignments on all arcs in the network are obtained automatically from a set of example sentences.", "The parser uses a stack decoding search strategy, with a top-down control flow, and includes a feature-passing mechanism to deal with long-distance movement, agreement, and semantic constraints.", "TINA provides an automatic sentence generation capability that has been effective for identifying overgeneralization problems as well as in producing a word-pair language model for a recognizer.", "The parser is currently integrated with MIT's SUMMIT recognizer for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process.", "We propose the language understanding system, TINA, that integrates key ideas context free grammar, augmented transition network and unification concepts."]}
{"title": "Word-Sense Disambiguation Using Decomposable Models", "abstract": "is composed of interdependent variables. The test used to evaluate a model gives preference to those that have the fewest number of interdependencies, thereby selecting models expressing only the most systematic variable interactions. To summarize the method, one first identifies informative contextual features (where &quot;informative&quot; is a well-defined notion, discussed in Section 2). Then, out of all possible decomposable models characterizing interdependency relationships among the selected variables, those that are found to produce good approximations to the data are identified (using the test mentioned above) and one of those models is used to perform disambiguation. Thus, we are able to use multiple contextual features without the need for untested assumptions regarding the form of the model. Further, approximating the joint distribution of all variables with a model identifying only the most important systematic interactions among variables limits the number of parameters to be estimated, supports computational efficiency, and provides an understanding of the data. The biggest limitation associated with this method is the need for large amounts of sense-tagged data. Because asymptotic distributions of the test statistics are used, the validity of the results obtained using this approach are compromised when it is applied to sparse data (this point is discussed further in Section 2). To test the method of model selection presented in this paper, a case study of the disambiguation of the performed. selected because it has been shown in previous studies to be a difficult word to disambiguate. We selected as the set of tags all non-idiomatic noun senses of defined in the electronic version of Longman's Dictionary of Contemporary English (LDOCE) ([23]). Using the models produced in this study, we are able to assign an sense tag to every usage of a heldout test set with 78% accuracy. Although it is difficult to compare our results to those reported for previous disambiguation experiments, as will be discussed later, we feel these results are encouraging. The remainder of the paper is organized as follows. Section 2 provides a more complete definition of the Abstract Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features. In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model. Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data. ", "introduction": "This paper presents a method for constructing probabilistic classifiers for word-sense disambiguation that offers advantages over previous approaches. Most previous efforts have not attempted to systematically identify the interdependencies among contextual features (such as collocations) that can be used to classify the meaning of an ambiguous word. Many researchers have performed disambiguation on the basis of only a single feature, while others who do consider multiple contextual features assume that all contextual features are either conditionally independent given the sense of the word or fully independent. Of course, all contextual features could be treated as interdependent, but, if there are several features, such a model could have too many parameters to estimate in practice. We present a method for formulating probabilistic models that describe the relationships among all variables in terms of only the most important interdependencies, that is, models of a certain class that are good approximations to the joint distribution of contextual features and word meanings. This class is the set of decomposable models: models that can be expressed as a product of marginal distributions, where each marginal methodology used for formulating decomposable models and Section 3 describes the details of the case study performed to test the approach. The results of the disambiguation case study are discussed and contrasted with similar efforts in Sections 4 and 5. Section 6 is the conclusion. In this Section, we address the problem of finding the models that generate good approximations to a given discrete probability distribution, as selected from among the class of decomposable models. Decomposable models are a subclass of log-linear models and, as such, can be used to characterize and study the structure of data ([21), that is, the interactions among variables as evidenced by the frequency with which the values of the variables co-occur. Given a data sample of objects, where each object is described by d discrete variables, let x=(xi , x2, ... , xg) be a q-dimensional vector of counts, where each xi is the frequency with which one of the possible combinations of the values of the d variables occurs in the data sample (and the frequencies of all such possible combinations are included in x). The log-linear model expresses the logarithm of E[x] (the mean of x) as a linear sum of the contributions of the &quot;effects&quot; of the variables and the interactions among the variables. Assume that a random sample consisting of N independent and identical trials (i.e., all trials are described by the same probability density function) is drawn from a discrete d-variate distribution. In such a situation, the outcome of each trial must be an event corresponding to a particular combination of the values of the d variables. Let pi be the probability that the ith event (i.e., the jth possible combination of the values of all variables) occurs on any trial and let xi be the number of times that the i event occurs in the random sample. Then (xl, x2,.. , xg) has a multinomial distribution with parameters N and pi, ,p9. For a given sample size, N, the likelihood of selecting any particular random sample is defined once the population parameters, that is, the pi's or, equivalently, the E[xi]'s (where E[xi] is the mean frequency of event i), are known. Log-linear models express the value of the logarithm of each E[xi] or pi as a linear sum of a smaller (i.e., less than q) number of new population parameters that characterize the effects of individual variables and their interactions. The theory of log-linear models specifies the sufficient statistics (functions of x) for estimating the effects of each variable and of each interaction among variables on E[x]. The sufficient statistics are the sample counts from the highest-order marginals composed of only interdependent variables. These statistics are the maximum likelihood estimates of the mean values of the corresponding marginals distributions. Consider, for example, a random sample taken from a population in which four contextual features are used to characterize each occurrence of an ambiguous word. The sufficient statistics for the model describing contextual features one and two as independent but all other variables as interdependent are, for all i, j, k, in, n (in this and all subsequent equations, f is an abbreviation for feature): Within the class of decomposable models, the maximum likelihood estimate for E[x] reduces to the product of the sufficient statistics divided by the sample counts defined in the marginals composed of the common elements in the sufficient statistics. As such, decomposable models are models that can be expressed as a product of marginals,1 where each marginal consists of only interdependent variables. Returning to our previous example, the maximum likelihood estimate for E[x] is, for all i, j, k, m, n: Expressing the population parameters as probabilities instead of expected counts, the equation above can be rewritten as follows, where the sample marginal relative frequencies are the maximum likelihood estimates of the population marginal probabilities. For all The degree to which the data is approximated by a model is called the fit of the model. In this work, the likelihood ratio statistic, G2, is used as the measure of the goodness-of-fit of a model. It is distributed asymptotically as X2 with degrees of freedom corresponding to the number of interactions (and/or variables) omitted from (unconstrained in) the model. Accessing the fit 'The marginal distributions can be represented in terms of counts or relative frequencies, depending on whether the parameters are expressed as expected frequencies or probabilities, respectively. of a model in terms of the significance of its G2 statistic gives preference to models with the fewest number of interdependencies, thereby assuring the selection of a model specifying only the most systematic variable interactions. Within the framework described above, the process of model selection becomes one of hypothesis testing, where each pattern of dependencies among variables expressible in terms of a decomposable model is postulated as a hypothetical model and its fit to the data is evaluated. The &quot;best fitting&quot; models are identified, in the sense that the significance of their reference x2 values are large, and, from among this set, a conceptually appealing model is chosen. The exhaustive search of decomposable models can be conducted as described in [12]. What we have just described is a method for approximating the joint distribution of all variables with a model containing only the most important systematic interactions among variables. This approach to model formulation limits the number of parameters to be estimated, supports computational efficiency, and provides an understanding of the data. The single biggest limitation remaining in this day of large memory, high speed computers results from reliance on asymptotic theory to describe the distribution of the maximum likelihood estimates and the likelihood ratio statistic. The effect of this reliance is felt most acutely when working with large sparse multinomials, which is exactly when this approach to model construction is most needed. When the data is sparse, the usual asymptotic properties of the distribution of the likelihood ratio statistic and the maximum likelihood estimates may not hold. In such cases, the fit of the model will appear to be too good, indicating that the model is in fact over constrained for the data available. In this work, we have limited ourselves to considering only those models with sufficient statistics that are not sparse, where the significance of the reference x2 is not unreasonable; most such models have sufficient statistics that are lower-order marginal distributions. In the future, we will investigate other goodness-of-fit tests ([18], [1], [22]) that are perhaps more appropriate for sparse data. Unlike several previous approaches to word sense disambiguation ([29], [5], [7], [10]), nothing in this approach limits the selection of sense tags to a particular number or type of meaning distinctions. In this study, our goal was to address a non-trivial case of ambiguity, but one that would allow some comparison of results with previous work. As a result of these considerations, the word interest was chosen as a test case, and the six non-idiomatic noun senses of interest defined in LDOCE were selected as the tag set. The only restriction limiting the choice of corpus is the need for large amounts of on-line data. Due to availability, the Penn Treebank Wall Street Journal corpus was selected. In total, 2,476 usages2 of interest as a noun3 were automatically extracted from the corpus and manually assigned sense tags corresponding to the LDOCE definitions. During tagging, 107 usages were removed from the data set due to the authors' inability to classify them in terms of the set of LDOCE senses. Of the rejected usages, 43 are metonymic, and the rest are hybrid meanings specific to the domain, such as public interest group. Because our sense distinctions are not merely between two or three clearly defined core senses of a word, the task of hand-tagging the tokens of interest required subtle judgments, a point that has also been observed by other researchers disambiguating with respect to the full set of LDOCE senses ([6], [28]). Although this undoubtedly degraded the accuracy of the manually assigned sense tags (and thus the accuracy of the study as well), this problem seems unavoidable when making semantic distinctions beyond clearly defined core senses of a word ([17], [11], [14], [15]). Of the 2,369 sentences containing the sense-tagged usages of interest, 600 were randomly selected and set aside to serve as the test set. The distribution of sense tags in the data set is presented in Table 1. We now turn to the selection of individually informative contextual features. In our approach to disambiguation, a contextual feature is judged to be informative (i.e., correlated with the sense tag of the ambiguous word) if the model for independence between that feature and the sense tag is judged to have an extremely poor fit using the test described in Section 2. The worse the fit, the more informative the feature is judged to be (similar to the approach suggested in [9]). Only features whose values can be automatically determined were considered, and preference was given to features that intuitively are not specific to interest (but see the discussion of collocational features below). An additional criterion was that the features not have too many possible values, in order to curtail sparsity in the resulting data matrix. We considered three different types of contextual features: morphological, collocation-specific, and classbased, with part-of-speech (POS) categories serving as the word classes. Within these classes, we choose a number of specific features, each of which was judged to be informative as described above. We used one morphological feature: a dichotomous variable indicating the presence or absence of the plural form. The values of the class-based variables are a set of twenty-five POS tags formed, with one exception, from the first letter of the tags used in the Penn Treebank corpus. Two different sets of class-based variables were selected. The first set contained only the POS tags of the word immediately preceding and the word immediately succeeding the ambiguous word, while the second set was extended to include the FOS tags of the two immediately preceding and two succeeding words. A limited number of collocation-specific variables were selected, where the term collocation is used loosely to refer to a specific spelling form occurring in the same sentence as the ambiguous word. All of our collocational variables are dichotomous, indicating the presence or absence of the associated spelling form. While collocation-specific variables are, by definition, specific to the word being disambiguated, the procedure used to select them is general. The search for collocationspecific variables was limited to the 400 most frequent spelling forms in a data sample composed of sentences containing interest. Out of these 400, the five spelling forms found to be the most informative using the test described above were selected as the collocational variables. It is not enough to know that each of the features described above is highly correlated with the meaning of the ambiguous word. In order to use the features in concert to perform disambiguation, a model describing the interactions among them is needed. Since we had no reason to prefer, a priori, one form of model over another, all models describing possible interactions among the features were generated, and a model with good fit was selected. Models were generated and tested as described in Section 2. ", "conclusion": "", "summary_sents": ["Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features.", "In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun interest.", "We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model.", "Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data.", "We manually assign 2,476 usages of interest with sense tags from the Longman Dictionary of Contemporary English."]}
{"title": "Automatic Grammar Induction And Parsing Free Text: A Transformation-Based Approach", "abstract": "In this paper we describe a new technique for parsing free text: a transformational grammar' is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction. ", "introduction": "", "conclusion": "", "summary_sents": ["In this paper we describe a new technique for parsing free text: a transformational grammar I is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled.", "The algorithm works by beginning in a very naive state of knowledge about phrase structure.", "By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error.", "After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.", "Our transformation-based tagging requires a handtagged text for training."]}
{"title": "Linguistic Regularities in Continuous Space Word Representations", "abstract": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, \u201cKing - Man + Woman\u201d results in a vector very close to \u201cQueen.\u201d We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems. ", "introduction": "A defining feature of neural network language models is their representation of words as high dimensional real valued vectors. In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network. As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors. Thus, when the model parameters are adjusted in response to a particular word or word-sequence, the improvements will carry over to occurrences of similar words and sequences. By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which may be used for other, potentially unrelated, tasks. This has been used to good effect, for example in (Collobert and Weston, 2008; Turian et al., 2010) where induced word representations are used with sophisticated classifiers to improve performance in many NLP tasks. In this work, we find that the learned word representations in fact capture meaningful syntactic and semantic regularities in a very simple way. Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship. For example, if we denote the vector for word i as xi, and focus on the singular/plural relation, we observe that x apple\u2212xapples \u2248 xcar\u2212xcars, xfamily\u2212xfamilies \u2248 xcar\u2212xcars, and so on. Perhaps more surprisingly, we find that this is also the case for a variety of semantic relations, as measured by the SemEval 2012 task of measuring relation similarity. Atlanta, Georgia, 9\u201314 June 2013. c\ufffd2013 Association for Computational Linguistics The remainder of this paper is organized as follows. In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7. ", "conclusion": "We have presented a generally applicable vector offset method for identifying linguistic regularities in continuous space word representations. We have shown that the word representations learned by a RNNLM do an especially good job in capturing these regularities. We present a new dataset for measuring syntactic performance, and achieve almost 40% correct. We also evaluate semantic generalization on the SemEval 2012 task, and outperform the previous state-of-the-art. Surprisingly, both results are the byproducts of an unsupervised maximum likelihood training criterion that simply operates on a large amount of text data. ", "summary_sents": ["Continuous space language models have recently demonstrated outstanding results across a variety of tasks.", "In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights.", "We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset.", "This allows vector-oriented reasoning based on the offsets between words.", "For example, the male/female relationship is automatically learned, and with the induced vector representations, \"King - Man + Woman\" results in a vector very close to \"Queen\".", "We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.", "We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions.", "Remarkably, this method outperforms the best previous systems.", "We reach top accuracy on the syntactic subset (an syn) with a CBOW predict model."]}
{"title": "Translating Collocations For Bilingual Lexicons: A Statistical Approach", "abstract": "Collocations are notoriously difficult for non-native speakers to translate, primarily because they are opaque and cannot be translated on a word-by-word basis. We describe a program named given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations. Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains. The algorithm we use is based on statistical methods and produces p-word translations of collocations in which not be the same. For example, . . . decision, employment equity, market . . . decision, equite matiere d'emploi, Testing three years' worth of the Hansards corpus yielded the French translations of 300 collocations for each year, evaluated at 73% accuracy on average. In this paper, we describe the statistical measures used, the algorithm, the implementation of our results and evaluation. ", "introduction": "", "conclusion": "", "summary_sents": ["Collocations are notoriously difficult for non-native speakers to translate, primarily because they are opaque and cannot be translated on a word-by-word basis.", "We describe a program named Champollion which, given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations.", "Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains.", "The algorithm we use is based on statistical methods and produces p-word translations of n-word collocations in which n and p need not be the same.", "For example, Champollion translates make ... decision, employment equity, and stock market into prendre ... decision, equite en matiere d'emploi, and bourse respectively.", "Testing Champollion on three years' worth of the Hansards corpus yielded the French translations of 300 collocations for each year, evaluated at 73% accuracy on average.", "In this paper, we describe the statistical measures used, the algorithm, and the implementation of Champollion, presenting our results and evaluation.", "The relationship between pointwise Mutual Information and the Dice coefficient is discussed in this work.", "We propose a corpus-based method to extract bilingual lexicons.", "We propose a statistical association measure of the Dice coefficient to deal with the problem of collocation translation."]}
{"title": "Statistical Machine Translation for Query Expansion in Answer Retrieval", "abstract": "We present an approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers. SMT-based query expansion is done by i) using a full-sentence paraphraser to introduce synonyms in context of the entire query, and ii) by translating query terms into answer terms using a full-sentence SMT model trained on question-answer pairs. We evaluate these global, context-aware query expansion techon from 10 million question-answer pairs extracted from FAQ pages. Experimental results show that SMTbased expansion improves retrieval performance over local expansion and over retrieval without expansion. ", "introduction": "One of the fundamental problems in Question Answering (QA) has been recognized to be the \u201clexical chasm\u201d (Berger et al., 2000) between question strings and answer strings. This problem is manifested in a mismatch between question and answer vocabularies, and is aggravated by the inherent ambiguity of natural language. Several approaches have been presented that apply natural language processing technology to close this gap. For example, syntactic information has been deployed to reformulate questions (Hermjakob et al., 2002) or to replace questions by syntactically similar ones (Lin and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of \u201cterm mismatch\u201d between queries and documents. A standard IR solution, query expansion, attempts to increase the chances of matching words in relevant documents by adding terms with similar statistical properties to those in the original query (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). In this paper we will concentrate on the task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FAQ pages. Equivalently, this is a QA problem that concentrates on finding answers given FAQ documents that are known to contain the answers. Our approach to close the lexical gap in this setting attempts to marry QA and IR technology by deploying SMT methods for query expansion in answer retrieval. We present two approaches to SMT-based query expansion, both of which are implemented in the framework of phrase-based SMT (Och and Ney, 2004; Koehn et al., 2003). Our first query expansion model trains an endto-end phrase-based SMT model on 10 million question-answer pairs extracted from FAQ pages. The goal of this system is to learn lexical correlations between words and phrases in questions and answers, for example by allowing for multiple unaligned words in automatic word alignment, and disregarding issues such as word order. The ability to translate phrases instead of words and the use of a large language model serve as rich context to make precise decisions in the case of ambiguous translations. Query expansion is performed by adding content words that have not been seen in the original query from the n-best translations of the query. Our second query expansion model is based on the use of SMT technology for full-sentence paraphrasing. A phrase table of paraphrases is extracted from bilingual phrase tables (Bannard and CallisonBurch, 2005), and paraphrasing quality is improved by additional discriminative training on manually created paraphrases. This approach utilizes large bilingual phrase tables as information source to extract a table of para-phrases. Synonyms for query expansion are read off from the n-best paraphrases of full queries instead of from paraphrases of separate words or phrases. This allows the model to take advantage of the rich context of a large n-gram language model when adding terms from the n-best paraphrases to the original query. In our experimental evaluation we deploy a database of question-answer pairs extracted from FAQ pages for both training a question-answer translation model, and for a comparative evaluation of different systems on the task of answer retrieval. Retrieval is based on the tfidf framework of Jijkoun and de Rijke (2005), and query expansion is done straightforwardly by adding expansion terms to the query for a second retrieval cycle. We compare our global, context-aware query expansion techniques with Jijkoun and de Rijke\u2019s (2005) tfidf model for answer retrieval and a local query expansion technique (Xu and Croft, 1996). Experimental results show a significant improvement of SMTbased query expansion over both baselines. ", "conclusion": "We presented two techniques for query expansion in answer retrieval that are based on SMT technology. Our method for question-answer translation uses a large corpus of question-answer pairs extracted from FAQ pages to learn a translation model from questions to answers. SMT-based paraphrasing utilizes large amounts of bilingual data as a new information source to extract phrase-level synonyms. Both SMT-based techniques take the entire query context into account when adding new terms to the original query. In an experimental comparison with a baseline tfidf approach and a local query expansion technique on the task of answer retrieval from FAQ pages, we showed a significant improvement of both SMT-based query expansion over both baselines. Despite the small-scale nature of our current experimental results, we hope to apply the presented techniques to general web retrieval in future work. Another task for future work is to scale up the extraction of question-answer pair data in order to provide an improved resource for question-answer translation. ", "summary_sents": ["We present an approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers.", "SMT-based query expansion is done by i) using a full-sentence paraphraser to introduce synonyms in context of the entire query, and ii) by translating query terms into answer terms using a full-sentence SMT model trained on question-answer pairs.", "We evaluate these global, context-aware query expansion techniques on tfidf retrieval from 10 million question-answer pairs extracted from FAQ pages.", "Experimental results show that SMT-based expansion improves retrieval performance over local expansion and over retrieval without expansion.", "We demonstrate the advantages of translation-based approach to answer retrieval by utilizing a more complex translation model also trained from a large amount of data extracted from FAQs on the Web."]}
{"title": "Learning A Translation Lexicon From Monolingual Corpora", "abstract": "This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated monolingual corpora. We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency. Experimental results for the construction of a German-English noun lexicon are reported. Noun translation accuracy of 39% scored against a parallel test corpus could be achieved. ", "introduction": "Recently, there has been a surge in research in machine translation that is based on empirical methods. The seminal work by Brown et al. [1990] at IBM on the Candide system laid the foundation for much of the current work in Statistical Machine Translation (SMT). Some of this work has been re-implemented and is freely available for research purposes [AlOnaizan et al., 1999]. Roughly speaking, SMT divides the task of translation into two steps: a word-level translation model and a model for word reordering during the translation process. The statistical models are trained on parallel corpora: large amounts of text in one language along with their translation in another. Various parallel texts have recently become available, mostly from government sources such as parliament proceedings (the Canadian Hansard, the minutes of the European parliament1) or law texts (from Hong Kong). Still, for most language pairs, parallel texts are hard to come by. This is clearly the case for low-density languages such as Tamil, Swahili, or Tetun. Furthermore, texts derived from parliament speeches may not be appropriate for a particular targeted domain. Specific parallel texts can be constructed by hand for the purpose of training an SMT system, but this is a very costly endeavor. On the other hand, the digital revolution and the wide-spread use of the World Wide Web have proliferated vast amounts of monolingual corpora. Publishing text in one language is a much more natural human activity than producing parallel texts. To illustrate this point: The world wide web alone contains currently over two billion pages, a number that is still growing exponentially. According to Google,2 the word directory occurs 61 million times, empathy 383,000 times, and reflex 787,000 times. In the Hansard, each of these words occurs only once. The objective of this research to build a translation lexicon solely from monolingual corpora. Specifically, we want to automatically generate a one-to-one mapping of German and English nouns. We are testing our mappings against a bilingual lexicon of 9,206 German and 10,645 English nouns. The two monolingual corpora should be in a fairly comparable domain. For our experiments we use the 1990-1992 Wall Street Journal corpus on the English side and the 1995-1996 German news wire (DPA) corpus on the German side. Both corpora are news sources in the general sense. However, they span different time periods and have a different orientation: the World Street Journal covers mostly business news, the German news wire mostly German politics. For experiments on training probabilistic translation lexicons from parallel corpora and similar tasks on the same test corpus, refer to our earlier work [Koehn and Knight, 2000, 2001]. ", "conclusion": "We have attempted to learn a one-to-one translation lexicon purely from unrelated monolingual corpora. Using identically spelled words proved to be a good starting point. Beyond this, we examined four different clues. Two of them, matching similar spelled words and words with the same context, helped us to learn a significant number of additional correct lexical entries. Our experiments have been restricted to nouns. Verbs, adjectives, adverbs and other part of speech may be tackled in a similar way. They might also provide useful context information that is beneficial to building a noun lexicon. These methods may be also useful given a different starting point: For efforts in building machine translation systems, some small parallel text should be available. From these, some high-quality lexical entries can be learned, but there will always be many words that are missing. These may be learned using the described methods. ", "summary_sents": ["This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated mono-lingual corpora.", "We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.", "Experimental results for the construction of a German-English noun lexicon are reported.", "Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.", "We automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts."]}
{"title": "Automatic Acquisition Of A Large Sub Categorization Dictionary From Corpora", "abstract": "This paper presents a new method for producing a dictionary of subcategorization frames from unlabelled text corpora. It is shown that statistical filtering of the results of a finite state parser running on the output of a stochastic tagger produces high quality results, despite the error rates of the tagger and the parser. Further, it is argued that this method can be used to learn all subcategorization frames, whereas previous methods are not extensible to a general solution to the problem. ", "introduction": "", "conclusion": "", "summary_sents": ["This paper presents a new method for producing a dictionary of subcategorization frames from unlabelled text corpora.", "It is shown that statistical filtering of the results of a finite state parser running on the output of a stochastic tagger produces high quality results, despite the error rates of the tagger and the parser.", "Further, it is argued that this method can be used to learn all subcategorization frames, whereas previous methods are not extensible to a general solution to the problem.", "We used the 4 million word corpus of the New York Times and selected only clauses with auxiliary verbs followed by automatically analyzing them with a finite-state parser."]}
{"title": "A Phrase-Based Joint Probability Model For Statistical Machine Translation", "abstract": "We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4. 1 Motivation Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models. In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to \u201cgenerate\u201d a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977). The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences. A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few. Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are into target individual words may contain a non-existent element, called NULL. We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons. Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons. Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons. Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results. Consider, for example, the parallel corpus of three sentence pairs shown in Figure 1. Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c. Sentence pair (S2, T2) offers strong evidence that \u201cb c\u201d in language S means the same thing as \u201cx\u201d in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that \u201ca\u201d in language S means the same thing as \u201cy\u201d in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word \u2014 as it is the case in the IBM models (Brown et al., 1993) \u2014 it is impossible to learn that the phrase \u201cb c\u201d in language S means the same thing as word \u201cx\u201d in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the probabilities shown in Figure Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999). IBM\u22124 T\u2212Table IBM\u22124 Intuitive Joint Joint T\u2212Table p(y  |a) = 1 p(x  |c) = 1 p(z  |b) = 0.98 p(x  |b) = 0.02 S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c p(x y, a b c) = 0.32 p(x, b c) = 0.34 p(y, a) = 0.01 p(z, b) = 0.33 Corresponding Conditional Table T2: x T2: x T2: x p(x y  |a b c ) = 1 p(x  |b c) = 1 p(y  |a) = 1 p(z  |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities. (Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).) In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well. In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously. In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications. The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexicons but also for the automatic translation of unseen sentences. In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3). We briefly describe a decoding algorithm that works in conjunction with our model (Section 4) and evaluate the performance of a translation system that uses the joint-probability model (Section 5). We end with a discussion of the strengths and weaknesses of our model as compared to other models proposed in the literature. 2 A Phrase-Based Joint Probability Model 2.1 Model 1 In developing our joint probability model, we started out with a very simple generative story. We assume that each sentence pair in our corpus is generated by the following stochastic process: 1. Generate a bag of concepts. 2. For each concept , generate a pair of phrases , according to the distribution contain at least one word. 3. Order the phrases generated in each language so as to create two linear sequences of phrases; these sequences correspond to the sentence pairs in a bilingual corpus. For simplicity, we initially assume that the bag of concepts and the ordering of the generated phrases are modeled by uniform distributions. We do not assume that is a hidden variable that generates pair , but rather that . Under these assumptions, it follows that the probability of generating a sentence pair (E, F) using concepts given by the product of all phrase-tophrase translation probabilities, yield bags of phrases that can be ordered linearly so as to obtain the sentences E and F. For example, the sentence pair \u201ca b c\u201d \u2014 \u201cx y\u201d can be generated using two concepts, (\u201ca b\u201d : \u201cy\u201d) and (\u201cc\u201d : \u201cx\u201d); or one concept, (\u201ca b c\u201d : \u201cx y\u201d), because in both cases the phrases in each language can be arranged in a sequence that would yield the original sentence pair. However, the same sentence pair cannot be generated using the concepts (\u201ca b\u201d : \u201cy\u201d) and (\u201cc\u201d : \u201cy\u201d) because the sequence \u201cx y\u201d cannot be recreated from the two phrases \u201cy\u201d and \u201cy\u201d. Similarly, the pair cannot be generated using concepts (\u201ca c\u201d : \u201cx\u201d) and (\u201cb\u201d : \u201cy\u201d) because the sequence \u201ca b c\u201d cannot be created by catenating the phrases \u201ca c\u201d and \u201cb\u201d. We say that a set of concepts can be linearized into a sentence pair (E, F) if E and F can be obtained by permuting the phrasesandthat characterize concepts . We denote this property using the predicate . Under this model, the probability of a given sentence pair (E, F) can then be obtained by summing up over all possible ways of generating bags of concepts that can linearized to (E, F). (1) 2.2 Model 2 Although Model 1 is fairly unsophisticated, we have found that it produces in practice fairly good alignments. However, this model is clearly unsuited for translating unseen sentences as it imposes no constraints on the ordering of the phrases associated with a given concept. In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions. The generative story of Model 2 is this: 1. Generate a bag of concepts. 2. Initialize E and F to empty sequences. 3. Randomly take a concept and generate a pair of phrases , according to the distribution , whereandeach contain at least one word. Remove then from. 4. Append phraseat the end of F. Letbe the start position ofin F. 5. Insert phraseat positionin E provided that no other phrase occupies any of the positions betweenand , wheregives length of the phrase. We hence create the alignment between the two phrasesand with probability is a position-based distortion distribution. 6. Repeat steps 3 to 5 untilis empty. In Model 2, the probability to generate a sentence pair (E, F) is given by formula (2), where the position of wordof phrasein sen- F and denotes the position in tence E of the center of mass of phrase. (2) Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3. We have tried many types of distortion models. We eventually settled for the model discussed here because it produces better translations during decoding. Since the number of factors involved in computing the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predisposed to produce translations that are shorter than the Source sentences given as input. 3 Training Training the models described in Section 2 is computationally challenging. Since there is an exponential number of alignments that can generate a sentence pair (E, F), it is clear that we cannot apply the 1. Determine high-frequency ngrams in the bilingual corpus. 2. Initialize the t-distribution table. 3. Apply EM training on the Viterbi alignments, while using smoothing. 4. Generate conditional model probabilities. Figure 2: Training algorithm for the phrase-based joint probability model. EM training algorithm exhaustively. To estimate the parameters of our model, we apply the algorithm in Figure 2, whose steps are motivated and described below. 3.1 Determine high-frequency n-grams in E and F If one assumes from the outset that any phrases can be generated from a cept , one would need a supercomputer in order to store in the memory a table that models the distribution. Since we don\u2019t have access to computers with unlimited memory, we initially learn t distribution entries only for the phrases that occur often in the corpus and for unigrams. Then, through smoothing, we learn t distribution entries for the phrases that occur rarely as well. In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus. 3.2 Initialize the t-distribution table Before the EM training procedure starts, one has no idea what word/phrase pairs are likely to share the same meaning. In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability. Under these conditions, the evidence that a sentence pair (E, F) contributes to fact that are generated by the same cept is given by the number of alignments that can be built between (E, F) that have a concept that is linked to phrasein sentence E and phrase sentence F divided by the total number of alignments that can be built between the two sentences. Both these numbers can be easily approximated. Given a sentence E ofwords, there are ways in which thewords can be partitioned into sets/concepts, where is the ling number of second kind. There are also ways in which the words a sentence F can be partitioned into nonempty sets. Given that any words in E can be mapped to any words in F, it follows that there are alignments that can be built between two sentences (E, F) of lengthsand , respectively. When a concept generates two phrases of lengthand, respectively, there are only and words left to link. Hence, the absence of any other information, the probability that phrasesandare generated by the same concept is given by formula (4). Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive. However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice. In the second step of the algorithm, we apply equation (4) to collect fractional counts for all unigram and high-frequency n-gram pairs in the cartesian product defined over the phrases in each sentence pair (E, F) in a corpus. We sum over all these t-counts and we normalize to obtain an initial joint distribution. This step amounts to running the EM algorithm for one step over all possible alignments in the corpus. 3.3 EM training on Viterbi alignments Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time. Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities. We then hillclimb towards the Viterbi alignment of highest probability by breaking and merging concepts, swapping words between concepts, and moving words across concepts. We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments. We apply this Viterbi-based EM training procedure for a few iterations. The first iterations estimate the alignment probabilities using Model 1. The rest of the iterations estimate the alignment probabilities using Model 2. During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus. 3.4 Derivation of conditional probability model At the end of the training procedure, we take marginals on the joint probability distributionsand . This yields conditional probability distributions and which we use for decoding. 3.5 Discussion When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e. At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build alignments between the smallest phrases possible. However, note that the choice made by our model is quite reasonable. After all, in the absence of additional information, the model can either assume that \u201ca\u201d and \u201cy\u201d mean the same thing or that phrases \u201ca b c\u201d and \u201cx y\u201d mean the same thing. The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one. Also note that although the joint distribution puts the second hypothesis at an advantage, the conditional distribution does not. The conditional distribution in Figure 1.e is consistent with our intuitions that tell us that it is reasonable both to translate \u201ca b c\u201d into \u201cx y\u201d, as well as \u201ca\u201d into \u201cy\u201d. The conditional distribution mirrors perfectly our intuitions. 4 Decoding For decoding, we have implemented a greedy procedure similar to that proposed by Germann et al. (2001). Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat the probability . We then tively hillclimb by modifying E and the alignment between E and F so as to maximize the formula . We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time. These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts. The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). The language model is estimated at the word (not phrase) level. Figure 3 shows the steps taken by our decoder in order to find the translation of sentence \u201cje vais me arr\u02c6eter l`a .\u201d Each intermediate translation in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability. 5 Evaluation To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus. The sentences in the corpus were at most 20 words long. The English side had a total of 1,073,480 words (21,484 unique tokens). The French side had a total of 1,177,143 words (28,132 unique tokens). We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20. For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et (2001) and the decoder that uses the joint prob- Model Percent perfect translations IBM Bleu score Sentence length Sentence length 6 8 10 15 20 Avg. 6 8 10 15 20 Avg. IBM 36 26 35 11 2 22 0.2076 0.2040 0.2414 0.2248 0.2011 0.2158 Phrase-based 43 37 33 19 6 28 0.2574 0.2181 0.2435 0.2407 0.2028 0.2325 Table 1: Comparison of IBM and Phrase-Based, Joint Probability Models on a translation task. je vais me arreter la . je vais me arreter la . 9.46e\u221208 i am going to stop there . Figure 3: Example of phrase-based greedy decoding. ability model. We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002). The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics. 6 Discussion 6.1 Limitations The main shortcoming of the phrase-based model in this paper concerns the size of the t-table and the cost of the training procedure we currently apply. To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side. Also, the swap, break, and merge operations used during the Viterbi training are computationally expensive. We are currently investigating the applicability of dynamic programming techniques to increase the speed of the training procedure. Clearly, there are language pairs for which it would be helpful to allow concepts to be realized as non-contiguous phrases. The English word \u201cnot\u201d, for example, is often translated into two French words, \u201cne\u201d and \u201cpas\u201d. But \u201cne\u201d and \u201cpas\u201d almost never occur in adjacent positions in French texts. At the outset of this work, we attempted to develop a translation model that enables concepts to be mapped into non-contiguous phrases. But we were not able to scale and train it on large amounts of data. The model described in this paper cannot learn that the English word \u201cnot\u201d corresponds to the French words \u201cne\u201d and \u201cpas\u201d. However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (\u201cne est pas\u201d, \u201cis not\u201d); (\u201cest inadmissible\u201d, \u201cis not good enough\u201d); and (\u201cne est pas ici\u201d, \u201cis not here\u201d). 6.2 Comparison with other work A number of researchers have already gone beyond word-level translations in various MT settings. For example, Melamed (2001) uses wordlevel alignments in order to learn translations of noncompositional compounds. Och and Ney (1999) learn phrase-to-phrase mappings involving word classes, which they call \u201ctemplates\u201d, and exploit them in a statistical machine translation system. And Marcu (2001) extracts phrase translations from automatically aligned corpora and uses them in conjunction with a word-for-word statistical translation system. However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words. As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the je vais me arreter la . 7.50e\u221211 FuseAndChangeTrans(&quot;la .&quot;, &quot;there .&quot;) i want me to that . je vais me arreter la . 2.97e\u221210 ChangeWordTrans(&quot;arreter&quot;,&quot;stop&quot;) 7.75e\u221210 1.09e\u221209 i want me to there . je vais me arreter la . i want me stop there . je vais me arreter la . let me to stop there . FuseAndChange(&quot;je vais&quot;,&quot;let me&quot;) FuseAndChange(&quot;je vais me&quot;, &quot;i am going to&quot;) 1.28e\u221214 changeWordTrans(&quot;vais&quot;, &quot;want&quot;) i . me to that . data. In our approach, phrases are not treated differently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased. Working with phrase translations that are learned independent of a translation model can also affect the decoder performance. For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993). The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding. The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4. However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability. The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role. ", "introduction": "", "conclusion": "", "summary_sents": ["We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.", "Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.", "We propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristics for phrase extraction."]}
{"title": "Named Entity Recognition With Character-Level Models", "abstract": "We discuss two named-entity recognition models which use characters and character -grams either exclusively or as an important part of their data representation. The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features. Our best model achieves an overall F of 86.07% on the English test data (92.31% on the development data). This number represents a 25% error reduction over the same model without word-internal (substring) features. ", "introduction": "For most sequence-modeling tasks with word-level evaluation, including named-entity recognition and part-ofspeech tagging, it has seemed natural to use entire words as the basic input features. For example, the classic HMM view of these two tasks is one in which the observations are words and the hidden states encode class labels. However, because of data sparsity, sophisticated unknown word models are generally required for good performance. A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al., 1997, Bikel et al., 1997). One then treats the unknown word as a collection of such features. Having such unknown-word models as an add-on is perhaps a misplaced focus: in these tasks, providing correct behavior on unknown words is typically the key challenge. Here, we examine the utility of taking character sequences as a primary representation. We present two models in which the basic units are characters and character -grams, instead of words and word phrases. Earlier papers have taken a character-level approach to named entity recognition (NER), notably Cucerzan and Yarowsky (1999), which used prefix and suffix tries, though to our knowledge incorporating all character grams is new. In section 2, we discuss a character-level HMM, while in section 3 we discuss a sequence-free maximum-entropy (maxent) classifier which uses -gram substring features. Finally, in section 4 we add additional features to the maxent model, and chain these models into a conditional markov model (CMM), as used for tagging (Ratnaparkhi, 1996) or earlier NER work (Borthwick, 1999). ", "conclusion": "The primary argument of this paper is that character substrings are a valuable, and, we believe, underexploited source of model features. In an HMM with an admittedly very local sequence model, switching from a word model to a character model gave an error reduction of about 30%. In the final, much richer chained maxent setting, the reduction from the best model minus -gram features to the reported best model was about 25% \u2013 smaller, but still substantial. This paper also again demonstrates how the ease of incorporating features into a discriminative maxent model allows for productive feature engineering. ", "summary_sents": ["We discuss two named-entity recognition models which use characters and character n-grams either exclusively or as an important part of their data representation.", "The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.", "Our best model achieves an overall F1 of 86.07% on the English test data (92.31% on the development data).", "This number represents a 25% error reduction over the same model without word-internal (substring) features.", "We find that the introduction of character n-gram features improved the overall F1 score by over 20%."]}
{"title": "A Memory-Based Approach to Learning Shallow Natural Language Patterns", "abstract": "Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction. ", "introduction": "Identifying local patterns of syntactic sequences and relationships is a fundamental task in natural language processing (NLP). Such patterns may correspond to syntactic phrases, like noun phrases, or to pairs of words that participate in a syntactic relationship, like the heads of a verb-object relation. Such patterns have been found useful in various application areas, including information extraction, text summarization, and bilingual alignment. Syntactic patterns are useful also for many basic computational linguistic tasks, such as statistical word similarity and various disambiguation problems. One approach for detecting syntactic patterns is to obtain a full parse of a sentence and then extract the required patterns. However, obtaining a complete parse tree for a sentence is difficult in many cases, and may not be necessary at all for identifying most instances of local syntactic patterns. An alternative approach is to avoid the complexity of full parsing and instead to rely only on local information. A variety of methods have been developed within this framework, known as shallow parsing, chunking, local parsing etc. (e.g., (Abney, 1991; Greffenstette, 1993)). These works have shown that it is possible to identify most instances of local syntactic patterns by rules that examine only the pattern itself and its nearby context. Often, the rules are applied to sentences that were tagged by partof-speech (POS) and are phrased by some form of regular expressions or finite state automata. Manual writing of local syntactic rules has become a common practice for many applications. However, writing rules is often tedious and time consuming. Furthermore, extending the rules to different languages or sub-language domains can require substantial resources and expertise that are often not available. As in many areas of NLP, a learning approach is appealing. Surprisingly, though, rather little work has been devoted to learning local syntactic patterns, mostly noun phrases (Ramshaw and Marcus, 1995; Vilain and Day, 1996). This paper presents a novel general learning approach for recognizing local sequential patterns, that may be perceived as falling within the memorybased learning paradigm. The method utilizes a part-of-speech tagged training corpus in which all instances of the target pattern are marked (bracketed). The training data are stored as-is in suffix-tree data structures, which enable linear time searching for subsequences in the corpus. The memory-based nature of the presented algorithm stems from its deduction strategy: a new instance of the target pattern is recognized by examining the raw training corpus, searching for positive and negative evidence with respect to the given test sequence. No model is created for the training corpus, and the raw examples are not converted to any other representation. Consider the following examplel . Suppose we want to decide whether the candidate sequence is a noun phrase (NP) by comparing it to the training corpus. A good match would be if the entire sequence appears as-is several times in the corpus. However, due to data sparseness, an exact match cannot always be expected. A somewhat weaker match may be obtained if we consider sub-parts of the candidate sequence (called tiles). For example, suppose the corpus contains noun phrase instances with the following structures: The first structure provides positive evidence that the sequence &quot;DT ADJ ADJ NM&quot; is a possible NP prefix while the second structure provides evidence for &quot;ADJ NN NNP&quot; being an NP suffix. Together, these two training instances provide positive evidence that covers the entire candidate. Considering evidence for sub-parts of the pattern enables us to generalize over the exact structures that are present in the corpus. Similarly, we also consider the negative evidence for such sub-parts by noting where they occur in the corpus without being a corresponding part of a target instance. The proposed method, as described in detail in the next section, formalizes this type of reasoning. It searches specialized data structures for both positive and negative evidence for sub-parts of the candidate structure, and considers additional factors such as context and evidence overlap. Section 3 presents experimental results for three target syntactic patterns in English, and Section 4 describes related work. ", "conclusion": "We have presented a novel general schema and a particular instantiation of it for learning sequential patterns. Applying the method to three syntactic patterns in English yielded positive results, suggesting its applicability for recognizing local linguistic patterns. In future work we plan to investigate a datadriven approach for optimal selection and weighting of statistical features of candidate scores, as well as to apply the method to syntactic patterns of Hebrew and to domain-specific patterns for information extraction. ", "summary_sents": ["Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing.", "The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata.", "This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus.", "The training data are stored as-is, in efficient suffix-tree data structures.", "Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus.", "This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training.", "The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English.", "Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction.", "We segment the POS sequence of a multi-word into small POS titles, count tile frequency in the new word and non-new-word on the training set respectively and detect new words using these counts."]}
{"title": "Online Large-Margin Training of Syntactic and Structural Translation Features", "abstract": "Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize. Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT. We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost. We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrasebased model: first, we simultaneously train a large number of Marton and Resnik\u2019s soft syntactic constraints, and, second, we introduce a novel structural distortion model. In both cases we obtain significant improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, improve performance by 2.6 a subset of the NIST 2006 Arabic-English evaluation data. ", "introduction": "Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems. However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features. One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information. Each of their new features rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT\u2019s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a large-scale translation task. Taking this further, we test MIRA on two classes of features that make use of syntactic information and hierarchical structure. First, we generalize Marton and Resnik\u2019s (2008) soft syntactic constraints by training all of them simultaneously; and, second, we introduce a novel structural distortion model. We obtain significant improvements in both cases, and further large improvements when the two feature sets are combined. The paper proceeds as follows. We describe our training algorithm in section 2; our generalization of Marton and Resnik\u2019s soft syntactic constraints in section 3; our novel structural distortion features in section 4; and experimental results in section 5. ", "conclusion": "In this paper, we have brought together two existing lines of work: the training method of Watanabe et al. (2007), and the models of Chiang (2005) and Marton and Resnik (2008). Watanabe et al.\u2019s work showed that large-margin training with MIRA can be made feasible for state-of-the-art MT systems by using a manageable tuning set; we have demonstrated that parallel processing and exploiting more of the parse forest improves MIRA\u2019s performance and that, even using the same set of features, MIRA\u2019s performance compares favorably to MERT in terms of both translation quality and computational cost. Marton and Resnik (2008) showed that it is possible to improve translation in a data-driven framework by incorporating source-side syntactic analysis in the form of soft syntactic constraints. This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT. In the area of source-side syntax, recent research has continued to improve tree-to-string translation models, soften the constraints of the input tree in various ways (Mi et al., 2008; Zhang et al., 2008), and extend phrase-based translation with sourceside soft syntactic constraints (Cherry, 2008). All this work shows strong promise, but Marton and Resnik\u2019s soft syntactic constraint approach is particularly appealing because it can be used unobtrusively with any hierarchically-structured translation model. Here, we have shown that using MIRA to weight all the constraints at once removes the crucial drawback of the approach, the problem of feature selection. Finally, we have introduced novel structural distortion features to fill a notable gap in the hierarchical phrase-based approach. By capturing how reordering depends on constituent length, these features improve translation quality significantly. In sum, we have shown that removing the bottleneck of MERT opens the door to many possibilities for better translation. ", "summary_sents": ["Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize.", "Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.", "We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost.", "We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrase-based model: first, we simultaneously train a large number of Marton and Resnik's soft syntactic constraints, and, second, we introduce a novel structural distortion model.", "In both cases we obtain significant improvements in translation performance.", "Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 BLUE on a subset of the NIST 2006 Arabic-English evaluation data.", "We introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length.", "We show that MERT is competitive with small numbers of features compared to high-dimensional optimizers such as MIRA.", "Our feature explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals."]}
{"title": "A Statistical Approach To Anaphora Resolution", "abstract": "This paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm. We incorporate multiple anaphora resolution factors into a statistical framework \u2014 specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition. We combine into a single probability that enables to identify the referent. Our first experiment shows the relative contribution of each source of information and demonstrates a success rate 82.9% for all sources combined. experiment investigates a method for unsupervised learning of gender/number/animaticity information. We present some experiments illustrating the accuracy of the method and note that with this information added, our pronoun resolution method achieves 84.2% accuracy. ", "introduction": "We present a statistical method for determining pronoun anaphora. This program differs from earlier work in its almost complete lack of hand-crafting, relying instead on a very small corpus of Penn Wall Street Journal Tree-bank text (Marcus et al., 1993) that has been marked with co-reference information. The first sections of this paper describe this program: the probabilistic model behind it, its implementation, and its performance. The second half of the paper describes a method for using (portions of) the aforementioned program to learn automatically the typical gender of English words, information that is itself used in the pronoun resolution program. In particular, the scheme infers the gender of a referent from the gender of the pronouns that refer to it and selects referents using the pronoun anaphora program. We present some typical results as well as the more rigorous results of a blind evaluation of its output. ", "conclusion": "", "summary_sents": ["This paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm.", "We incorporate multiple anaphora resolution factors into a statistical framework -- specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition.", "We combine them into a single probability that enables us to identify the referent.", "Our first experiment shows the relative contribution of each source of information and demonstrates a success rate of 82.9% for all sources combined.", "The second experiment investigates a method for unsupervised learning of gender/number/animaticity information.", "We present some experiments illustrating the accuracy of the method and note that with this information added, our pronoun resolution method achieves 84.2% accuracy.", "We add annotation of the antecedents of definite pronouns to Treebank.", "We implement a Hobbs distance feature, which encodes the rank assigned to a candidate antecedent for a pronoun by Hobbs's (1978) seminal syntax-based pronoun resolution algorithm.", "We count the number of times a discourse entities has been mentioned in the discourse already.", "Our probabilistic approach combines three factors (aside from the agreement filter): the result of the Hobbs algorithm, Mention Count dependent on the position of the sentence in the article, and the probability of the antecedent occurring in the local context of the pronoun.", "We describe a supervised probabilistic pronoun resolution algorithm which is based on complete syntactic information."]}
{"title": "Methods For Using Textual Entailment In Open-Domain Question Answering", "abstract": "Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment. In this paper, we demonstrate how computational systems to recognize entailment can be used to enhance the accuracy of current open-domain automatic question answering (Q/A) systems. In our experiments, we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall. ", "introduction": "Open-Domain Question Answering (Q/A) systems return a textual expression, identified from a vast document collection, as a response to a question asked in natural language. In the quest for producing accurate answers, the open-domain Q/A problem has been cast as: (1) a pipeline of linguistic processes pertaining to the processing of questions, relevant passages and candidate answers, interconnected by several types of lexicosemantic feedback (cf. (Harabagiu et al., 2001; Moldovan et al., 2002)); (2) a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf. (Moldovan et al., 2003)); (3) a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)); or (4) a constraint satisfaction problem, where sets of auxiliary questions are used to provide more information and better constrain the answers to individual questions (cf. (Prager et al., 2004)). While different in their approach, each of these frameworks seeks to approximate the forms of semantic inference that will allow them to identify valid textual answers to natural language questions. Recently, the task of automatically recognizing one form of semantic inference \u2013 textual entailment \u2013 has received much attention from groups participating in the 2005 and 2006 PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005). 1 As currently defined, the RTE task requires systems to determine whether, given two text fragments, the meaning of one text could be reasonably inferred, or textually entailed, from the meaning of the other text. We believe that systems developed specifically for this task can provide current question-answering systems with valuable semantic information that can be leveraged to identify exact answers from ranked lists of candidate answers. By replacing the pairs of texts evaluated in the RTE Challenge with combinations of questions and candidate answers, we expect that textual entailment could provide yet another mechanism for approximating the types of inference needed in order answer questions accurately. In this paper, we present three different methods for incorporating systems for textual entailment into the traditional Q/A architecture employed by many current systems. Our experimental results indicate that (even at their current level of performance) textual entailment systems can substantially improve the accuracy of Q/A, even when no other form of semantic inference is employed. The remainder of the paper is organized as follows. Section 2 describes the three methods of using textual entailment in open-domain question answering that we have identified, while Section 3 presents the textual entailment system we have used. Section 4 details our experimental methods and our evaluation results. Finally, Section 5 provides a discussion of our findings, and Section 6 summarizes our conclusions. ", "conclusion": "In this paper, we discussed three different ways that a state-of-the-art textual entailment system could be used to enhance the performance of an open-domain Q/A system. We have shown that when textual entailment information is used to either filter or rank candidate answers returned by a Q/A system, Q/A accuracy can be improved from 32% to 52% (when an answer type can be detected) and from 30% to 40% (when no answer type can be detected). We believe that these results suggest that current supervised machine learning approaches to the recognition of textual entailment may provide open-domain Q/A systems with the inferential information needed to develop viable answer validation systems. ", "summary_sents": ["Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment.", "In this paper, we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering (Q/A) systems.", "In our experiments, we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall.", "we applied a TE component to rerank candidate answers returned by a retrieval step for the task of Question Answering."]}
{"title": "Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora", "abstract": "Current phrase-based SMT systems perform poorly when using small training sets. This is a consequence of unreliable translation estimates and low coverage over source and target phrases. This paper presents a method which alleviates this problem by exploiting multiple translations of the same source Central to our approach is triangulathe process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system. ", "introduction": "Statistical machine translation (Brown et al., 1993) has seen many improvements in recent years, most notably the transition from word- to phrase-based models (Koehn et al., 2003). Modern SMT systems are capable of producing high quality translations when provided with large quantities of training data. With only a small training sample, the translation output is often inferior to the output from using larger corpora because the translation algorithm must rely on more sparse estimates of phrase frequencies and must also \u2018back-off\u2019 to smaller sized phrases. This often leads to poor choices of target phrases and reduces the coherence of the output. Unfortunately, parallel corpora are not readily available in large quantities, except for a small subset of the world\u2019s languages (see Resnik and Smith (2003) for discussion), therefore limiting the potential use of current SMT systems. In this paper we provide a means for obtaining more reliable translation frequency estimates from small datasets. We make use of multi-parallel corpora (sentence aligned parallel texts over three or more languages). Such corpora are often created by international organisations, the United Nations (UN) being a prime example. They present a challenge for current SMT systems due to their relatively moderate size and domain variability (examples of UN texts include policy documents, proceedings of meetings, letters, etc.). Our method translates each target phrase, t, first to an intermediate language, i, and then into the source language, s. We call this two-stage translation process triangulation (Kay, 1997). We present a probabilistic formulation through which we can estimate the desired phrase translation distribution (phrase-table) by marginalisation, p(s|t) _ Ei p(s, i|t). As with conventional smoothing methods (Koehn et al., 2003; Foster et al., 2006), triangulation increases the robustness of phrase translation estimates. In contrast to smoothing, our method alleviates data sparseness by exploring additional multiparallel data rather than adjusting the probabilities of existing data. Importantly, triangulation provides us with separately estimated phrase-tables which could be further smoothed to provide more reliable distributions. Moreover, the triangulated phrase-tables can be easily combined with the standard sourcetarget phrase-table, thereby improving the coverage over unseen source phrases. As an example, consider Figure 1 which shows the coverage of unigrams and larger n-gram phrases when using a standard source target phrase-table, a triangulated phrase-table with one (it) and nine languages (all), and a combination of standard and triangulated phrase-tables (all+standard). The phrases were harvested from a small French-English bitext and evaluated against a test set. Although very few small phrases are unknown, the majority of larger phrases are unseen. The Italian and all results show that triangulation alone can provide similar or improved coverage compared to the standard sourcetarget model; further improvement is achieved by combining the triangulated and standard models (all+standard). These models and datasets will be described in detail in Section 3. We also demonstrate that triangulation can be used on its own, that is without a source-target distribution, and still yield acceptable translation output. This is particularly heartening, as it provides a means of translating between the many \u201clow density\u201d language pairs for which we don\u2019t yet have a source-target bitext. This allows SMT to be applied to a much larger set of language pairs than was previously possible. In the following section we provide an overview of related work. Section 3 introduces a generative formulation of triangulation. We present our evaluation framework in Section 4 and results in Section 5. ", "conclusion": "In this paper we have presented a novel method for obtaining more reliable translation estimates from small datasets. The key premise of our work is that multi-parallel data can be usefully exploited for improving the coverage and quality of phrase-based SMT. Our triangulation method translates from a source to a target via one or many intermediate languages. We present a generative formulation of this process and show how it can be used together with the entries of a standard source-target phrase-table. We observe large performance gains when translating with triangulated models trained on small datasets. Furthermore, when combined with a standard phrase-table, our models also yield performance improvements on larger datasets. Our experiments revealed that triangulation benefits from a large set of intermediate languages and that performance is increased when languages of the same family to the source or target are used as intermediates. We have just scratched the surface of the possibilities for the framework discussed here. Important future directions lie in combining triangulation with richer means of conventional smoothing and using triangulation to translate between low-density language pairs. Acknowledgements The authors acknowledge the support of EPSRC (grants GR/T04540/01 and GR/T04557/01). Special thanks to Markus Becker, Chris Callison-Burch, David Talbot and Miles Osborne for their helpful comments. ", "summary_sents": ["Current phrase-based SMT systems perform poorly when using small training sets.", "This is a consequence of unreliable translation estimates and low coverage over source and target phrases.", "This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase.", "Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language.", "This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods.", "Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system."]}
{"title": "What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA", "abstract": "This paper presents a syntax-driven ap proach to question answering, specifically the answer-sentence selection problem forshort-answer questions. Rather than using syntactic features to augment exist ing statistical classifiers (as in previouswork), we build on the idea that ques tions and their (correct) answers relate toeach other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust non lexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines. ", "introduction": "", "conclusion": "We described a statistical syntax-based model that softly aligns a question sentence with a candidateanswer sentence and returns a score. Discriminative training and a relatively straightforward, barelyengineered feature set were used in the implementation. Our scoring model was found to greatly out perform two state-of-the-art baselines on an answer selection task using the TREC dataset. Acknowledgments The authors acknowledge helpful input from three anonymous reviewers, Kevin Gimpel, and David Smith. This work is supported in part by ARDA/DTO Advanced Question Answering for Intelligence (AQUAINT) program award number NBCHC040164. ", "summary_sents": ["This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions.", "Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations.", "We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model.", "Our model learns soft alignments as a hidden variable in discriminative training.", "Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines.", "We explore the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching.", "We use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer."]}
{"title": "Maximum Entropy Based Phrase Reordering Model For Statistical Machine Translation", "abstract": "We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reordering events of neighbor blocks from bilingual data. In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. ", "introduction": "Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently. Compared with word-based SMT systems, phrase-based systems can easily address reorderings of words within phrases. However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level (Knight, 1999). Many systems use very simple models to reorder phrases 1. One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. which are common between two languages with very different orders. Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either. Flat model assigns constant probabilities for monotone order and non-monotone order. The two probabilities can be set to prefer monotone or non-monotone orientations depending on the language pairs. In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent. Lexicalized reordering model learns local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data. During decoding, the model attempts to finding a Viterbi local orientation sequence. Performance gains have been reported for systems with lexicalized reordering model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem. Another smart reordering model was proposed by Chiang (2005). In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables. This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model. In this paper, we propose a novel solution for phrasal reordering. Here, under the ITG constraint (Wu, 1997; Zens et al., 2004), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks. Therefore reordering can be modelled as a problem of classification with only two labels, straight and inverted. In this paper, we build a maximum entropy based classification model as the reordering model. Different from lexicalized reordering, we do not use the whole block as reordering evidence, but only features extracted from blocks. This is more flexible. It makes our model reorder any blocks, observed in training or not. The whole maximum entropy based reordering model is embedded inside a log-linear phrase-based model of translation. Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically. To create a maximum entropy based reordering model, the first step is learning reordering examples from training data, similar to the lexicalized reordering model. But in our way, any evidences of reorderings will be extracted, not limited to reorderings of bilingual phrases of length less than a predefined number of words. Secondly, features will be extracted from reordering examples according to feature templates. Finally, a maximum entropy classifier will be trained on the features. In this paper we describe our system and the MaxEnt-based reordering model with the associated algorithm. We also present experiments that indicate that the MaxEnt-based reordering model improves translation significantly compared with other reordering approaches and a state-of-the-art distortion-based system (Koehn, 2004). ", "conclusion": "", "summary_sents": ["We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).", "The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.", "We present an algorithm to extract all reordering events of neighbor blocks from bilingual data.", "In our experiments on Chinese-to-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks.", "We improve the reordering model for SMT based on the collocated words crossing the neighboring components.", "We propose a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent.", "In our maximum entropy-based reordering model, MEBTG, three rules are used to derive the translation of each sub sentence: lexical rule, straight rule and inverted rule."]}
{"title": "Aligning Sentences In Parallel Corpora", "abstract": "In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text. We have used this technique to align several million sentences in the English-French Hansard corpora and have achieved an accuracy in excess of 99% in a random selected set of 1000 sentence pairs that we checked by hand. We show that even without the benefit of anchor points the correlation between the lengths of aligned sentences is strong enough that we should expect to achieve an accuracy of between 96% and 97%. Thus, the technique may be applicable to a wider variety of texts than we have yet tried. ", "introduction": "", "conclusion": "", "summary_sents": ["In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora.", "In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain.", "Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text.", "We have used this technique to align several million sentences in the English-French Hansard corpora and have achieved an accuracy in excess of 99% in a random selected set of 1000 sentence pairs that we checked by hand.", "We show that even without the benefit of anchor points the correlation between the lengths of aligned sentences is strong enough that we should expect to achieve an accuracy of between 96% and 97%.", "Thus, the technique may be applicable to a wider variety of texts than we have yet tried.", "We are able to achieve these results while completely ignoring the lexical content of the tests."]}
{"title": "Articles: Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis", "abstract": "Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation). However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word\u2019s prior polarity. Positive words are used in phrases expressing negative sentiments, or vice versa. Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment. The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task. Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity. The evaluation includes assessing the performance of features across multiple machine learning algorithms. For all learning algorithms except one, the combination of all features together gives the best performance. Another facet of the evaluation considers how the presence of neutral instances affects the performance offeatures for distinguishing between positive and negative polarity. These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system\u2019s ability to identify when an instance is neutral. ", "introduction": "", "conclusion": "", "summary_sents": ["Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation).", "However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word\u2019s prior polarity.", "Positive words are used in phrases expressing negative sentiments, or vice versa.", "Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment.", "The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task.", "Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity.", "The evaluation includes assessing the performance of features across multiple machine learning algorithms.", "For all learning algorithms except one, the combination of all features together gives the best performance.", "Another facet of the evaluation considers how the presence of neutral instances affects the performance of features for distinguishing between positive and negative polarity.", "These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system\u2019s ability to identify when an instance is neutral.", "We explore the difference between prior and contextual polarity: words that lose polarity in context, or whose polarity is reversed because of context."]}
{"title": "Estimating Upper And Lower Bounds On The Performance Of Word-Sense Disambiguation Programs", "abstract": "We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good. Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures. Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph. Many papers avoid quantitative evaluations altogether, because it is so difficult to come up with credible estimates of performance. This paper will attempt to establish upper and lower bounds on the level of performance that can be expected in an evaluation. An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases. An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants. Not surprisingly, the upper bound is very dependent on the instructions given to the judges. Jorgensen, for example, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges. ", "introduction": "", "conclusion": "", "summary_sents": ["We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia).", "After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good.", "Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures.", "Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph.", "Many papers avoid quantitative evaluations altogether, because it is so difficult to come up with credible estimates of performance.", "This paper will attempt to establish upper and lower bounds on the level of performance that can be expected in an evaluation.", "An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases.", "An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants.", "Not surprisingly, the upper bound is very dependent on the instructions given to the judges.", "Jorgensen, for example, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected.", "In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often enough so that we could show that they were outperforming the baseline system.", "Under quite different conditions, we have found 96.8% agreement over judges.", "We argue that any wide-coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worth of serious consideration."]}
{"title": "Accurate Methods For The Statistics Of Surprise And Coincidence", "abstract": "Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results. This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text. However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical. ", "introduction": "", "conclusion": "", "summary_sents": ["Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed.", "In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.", "This assumption of normal distribution limits the ability to analyze rare events.", "Unfortunately rare events do make up a large fraction of real text.", "However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms.", "In some cases, these measures perform much better than the methods previously used.", "In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.", "This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.", "Since it was first introduced to the NLP community by us, the G log-likelihood-ratio statistic has been widely used in statistical NLP as a measure of strength of association, particularly lexical associations."]}
{"title": "A Fully Statistical Approach To Natural Language Interfaces", "abstract": "We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames. ", "introduction": "", "conclusion": "", "summary_sents": ["We present a natural language interface system which is based entirely on trained statistical models.", "The system consists of three stages of processing: parsing, semantic interpretation, and discourse.", "Each of these stages is modeled as a statistical process.", "The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.", "Our approach is fully supervised and produces a final meaning representation in SQL.", "We compute the probability that a constituent such as Atlanta filled a semantic slot such as Destination in a semantic frame for air travel."]}
{"title": "Machine Learning Of Temporal Relations", "abstract": "This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions. ", "introduction": "The growing interest in practical NLP applications such as question-answering and text summarization places increasing demands on the processing of temporal information. In multidocument summarization of news articles, it can be useful to know the relative order of events so as to merge and present information from multiple news sources correctly. In questionanswering, one would like to be able to ask when an event occurs, or what events occurred prior to a particular event. A wealth of prior research by (Passoneau 1988), (Webber 1988), (Hwang and Schubert 1992), (Kamp and Reyle 1993), (Lascarides and Asher 1993), (Hitzeman et al. 1995), (Kehler 2000) and others, has explored the different knowledge sources used in inferring the temporal ordering of events, including temporal adverbials, tense, aspect, rhetorical relations, pragmatic conventions, and background knowledge. For example, the narrative convention of events being described in the order in which they occur is followed in (1), but overridden by means of a discourse relation, Explanation in (2). In addition to discourse relations, which often require inferences based on world knowledge, the ordering decisions humans carry out appear to involve a variety of knowledge sources, including tense and grammatical aspect (3a), lexical aspect (3b), and temporal adverbials (3c): (3c) The company announced Tuesday that third-quarter sales had fallen. Clearly, substantial linguistic processing may be required for a system to make these inferences, and world knowledge is hard to make available to a domain-independent program. An important strategy in this area is of course the development of annotated corpora than can facilitate the machine learning of such ordering inferences. This paper 1 investigates a machine learning approach for temporally ordering events in natural language texts. In Section 2, we describe the annotation scheme and annotated corpora, and the challenges posed by them. A basic learning approach is described in Section 3. To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data. As we will discuss in Section 5, there are no standard algorithms for making these inferences that we can compare against. We believe strongly that in such situations, it\u2019s worthwhile for computational linguists to devote considerable effort to developing insightful baselines. Our work is, accordingly, evaluated in comparison against four baselines: (i) the usual majority class statistical baseline, shown along with each result, (ii) a more sophisticated baseline that uses hand-coded rules (Section 4.1), (iii) a hybrid baseline based on hand-coded rules expanded with Google-induced rules (Section 4.2), and (iv) a machine learning version that learns from imperfect annotation produced by (ii) (Section 4.3). TimeML (Pustejovsky et al. 2005) (www.timeml.org) is an annotation scheme for markup of events, times, and their temporal relations in news articles. The TimeML scheme flags tensed verbs, adjectives, and nominals with EVENT tags with various attributes, including the class of event, tense, grammatical aspect, polarity (negative or positive), any modal operators which govern the event being tagged, and cardinality of the event if it\u2019s mentioned more than once. Likewise, time expressions are flagged and their values normalized, based on TIMEX3, an extension of the ACE (2004) (tern.mitre.org) TIMEX2 annotation scheme. For temporal relations, TimeML defines a TLINK tag that links tagged events to other events and/or times. For example, given (3a), a TLINK tag orders an instance of the event of entering to an instance of the drinking with the relation type AFTER. Likewise, given the sentence (3c), a TLINK tag will anchor the event instance of announcing to the time expression Tuesday (whose normalized value will be inferred from context), with the relation IS_INCLUDED. These inferences are shown (in slightly abbreviated form) in the annotations in (4) and (5). The anchor relation is an Event-Time TLINK, and the order relation is an Event-Event TLINK. TimeML uses 14 temporal relations in the TLINK RelTypes, which reduce to a disjunctive classification of 6 temporal relations RelTypes = {SIMULTANEOUS, IBEFORE, BEFORE, BEGINS, ENDS, INCLUDES}. An event or time is SIMULTANEOUS with another event or time if they occupy the same time interval. An event or time INCLUDES another event or time if the latter occupies a proper subinterval of the former. These 6 relations and their inverses map one-toone to 12 of Allen\u2019s 13 basic relations (Allen 1984)2. There has been a considerable amount of activity related to this scheme; we focus here on some of the challenges posed by the TLINK annotation, the part that is directly relevant to the temporal ordering and anchoring problems. The annotation of TimeML information is on a par with other challenging semantic annotation schemes, like PropBank, RST annotation, etc., where high inter-annotator reliability is crucial but not always achievable without massive preprocessing to reduce the user\u2019s workload. In TimeML, inter-annotator agreement for time expressions and events is 0.83 and 0.78 (average of Precision and Recall) respectively, but on TLINKs it is 0.55 (P&R average), due to the large number of event pairs that can be selected for comparison. The time complexity of the human TLINK annotation task is quadratic in the number of events and times in the document. Two corpora have been released based on TimeML: the TimeBank (Pustejovsky et al. 2003) (we use version 1.2.a) with 186 documents and 64,077 words of text, and the Opinion Corpus (www.timeml.org), with 73 documents and 38,709 words. The TimeBank was developed in the early stages of TimeML development, and was partitioned across five annotators with different levels of expertise. The Opinion Corpus was developed very recently, and was partitioned across just two highly trained annotators, and could therefore be expected to be less noisy. In our experiments, we merged the two datasets to produce a single corpus, called OTC. Table 1 shows the distribution of EVENTs and TIMES, and TLINK RelTypes3 in the OTC. The majority class percentages are shown in parentheses. It can be seen that BEFORE and SIMULTANEOUS together form a majority of event-ordering (Event-Event) links, whereas most of the event anchoring (Event-Time) links are INCLUDES. The lack of TLINK coverage in human annotation could be helped by preprocessing, provided it meets some threshold of accuracy. Given the availability of a corpus like OTC, it is natural to try a machine learning approach to see if it can be used to provide that preprocessing. However, the noise in the corpus and the sparseness of links present challenges to a learning approach. ", "conclusion": "Our research has uncovered one new finding: semantic reasoning (in this case, logical axioms for temporal closure), can be extremely valuable in addressing data sparseness. Without it, performance on this task of learning temporal relations is poor; with it, it is excellent. We showed that temporal reasoning can be used as an oversampling method to dramatically expand the amount of training data for TLINK labeling, resulting in labeling predictive accuracy as high as 93% using an off-the-shelf Maximum Entropy classifier. Future research will investigate this effect further, as well as examine factors that enhance or mitigate this effect in different corpora. The paper showed that ME-C performed significantly better than a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions. Our results in these comparisons confirm the lessons learned from the corpus-based revolution, namely that rules based on intuition alone are prone to incompleteness and are hard to tune without access to the distributions found in empirical data. Clearly, lexical rules have a role to play in semantic and pragmatic reasoning from language, as in the discussion of example (2) in Section 1. Such rules, when mined by robust, large corpusbased methods, as in the Google-derived VerbOcean, are clearly relevant, but too specific to apply more than a few times in the OTC corpus. It may be possible to acquire confidence weights for at least some of the intuitive rules in GTag from Google searches, so that we have a level field for integrating confidence weights from the fairly general GTag rules and the fairly specific VerbOcean-like lexical rules. Further, the GTag and VerbOcean rules could be incorporated as features for machine learning, along with features from automatic preprocessing. We have taken pains to use freely downloadable resources like Carafe, VerbOcean, and WEKA to help others easily replicate and quickly ramp up a system. To further facilitate further research, our tools as well as labeled vectors (unclosed as well as closed) are available for others to experiment with. ", "summary_sents": ["This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts.", "To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data.", "This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions.", "While machine learning approaches attempt to improve classification accuracy through feature engineering, we introduce a temporal reasoning component to greatly expand the training data.", "We use the links introduced by closure to boost the amount of training data for a tlink classifier."]}
{"title": "Findings of the 2012 Workshop on Statistical Machine Translation", "abstract": "This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. We introduced a new quality estimation task this year, and evaluated submissions from 11 teams. ", "introduction": "This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at NAACL 2012. This workshop builds on six previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010; CallisonBurch et al., 2011). In the past, the workshops have featured a number of shared tasks: a translation task between English and other languages, a task for automatic evaluation metrics to predict human judgments of translation quality, and a system combination task to get better translation quality by combining the outputs of multiple translation systems. This year we discontinued the system combination task, and introduced a new task in its place: ficulty is not uniform across all input types. It would thus be useful to have some measure of confidence in the quality of the output, which has potential usefulness in a range of settings, such as deciding whether output needs human post-editing or selecting the best translation from outputs from a number of systems. This shared task focused on sentence-level estimation, and challenged participants to rate the quality of sentences produced by a standard Moses translation system on an EnglishSpanish news corpus in one of two tasks: ranking and scoring. Predictions were scored against a blind test set manually annotated with relevant quality judgments. The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation. As with previous workshops, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation or automatic prediction of translation quality. 2 Overview of the Shared Translation Task The recurring task of the workshop examines translation between English and four other languages: German, Spanish, French, and Czech. We created a test set for each language pair by translating newspaper articles. We additionally provided training data and two baseline systems. The test data for this year\u2019s task was created by hiring people to translate news articles that were drawn from a variety of sources from November 15, 2011. A total of 99 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German, and Spanish news sites:2 Czech: Blesk (1), CTK (1), E15 (1), den\u00b4\u0131k (4), iDNES.cz (3), iHNed.cz (3), Ukacko (2), Zheny (1) French: Canoe (3), Croix (3), Le Devoir (3), Les Echos (3), Equipe (2), Le Figaro (3), Liberation (3) Spanish: ABC.es (4), Milenio (4), Noroeste (4), Nacion (3), El Pais (3), El Periodico (3), Prensa Libre (3), El Universal (4) English: CNN (3), Fox News (2), Los Angeles Times (3), New York Times (3), Newsweek (1), Time (3), Washington Post (3) German: Berliner Kurier (1), FAZ (3), Giessener Allgemeine (2), Morgenpost (3), Spiegel (3), Welt (3) The translations were created by the professional translation agency CEET.3 All of the translations were done directly, and not via an intermediate language. Although the translations were done professionally, we observed a number of errors. These errors ranged from minor typographical mistakes (I was terrible... instead of It was terrible... ) to more serious errors of incorrect verb choices and nonsensical constructions. An example of the latter is the French sentence (translated from German): Il a gratt\u00b4e une planche de b\u00b4eton, perdit des pi`eces du v\u00b4ehicule. (He scraped against a concrete crash barrier and lost parts of the car.) Here, the French verb gratter is incorrect, and the phrase planche de b\u00b4eton does not make any sense. We did not quantify errors, but collected a number of examples during the course of the manual evaluation. These errors were present in the data available to all the systems and therefore did not bias the results, but we suggest that next year a manual review of the professionally-collected translations be taken prior to releasing the data in order to correct mistakes and provide feedback to the translation agency. As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some statistics about the training materials are given in Figure 1. We received submissions from 34 groups across 18 institutions. The participants are listed in Table 1. We also included two commercial off-the-shelf MT systems, three online statistical MT systems, and three online rule-based MT systems. Not all systems supported all language pairs. We note that the eight companies that developed these systems did not submit entries themselves, but were instead gathered by translating the test data via their interfaces (web or PC).4 They are therefore anonymized in this paper. The data used to construct these systems is not subject to the same constraints as the shared task participants. It is possible that part of the reference translations that were taken from online news sites could have been included in the systems\u2019 models, for instance. We therefore categorize all commercial systems as unconstrained when evaluating the results. ", "conclusion": "", "summary_sents": ["This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality.", "We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams.", "We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics.", "We introduced a new quality estimation task this year, and evaluated submissions from 11 teams.", "We report for several automatic metrics on the whole WMT12 English-to-Czech dataset."]}
{"title": "A Corpus-Based Investigation Of Definite Description Use", "abstract": "We present the results of a study of the use of definite descriptions in written texts aimed at assessing the feasibility of annotating corpora with information about definite description interpretation. We ran two experiments, in which subjects were asked to classify the uses of definite descriptions in a corpus of 33 newspaper articles, containing a total of 1,412 definite descriptions. We measured the agreement among annotators about the classes assigned to definite descriptions, as well as the agreement about the antecedent assigned to those definites that the annotators classified as being related to an antecedent in the text. The most interesting result of this study a corpus annotation perspective was the rather low agreement = 0.63) we obtained versions of Hawkins's and Prince's classification schemes; better results = 0.76) obtained using the simplified scheme proposed by Fraurud that includes only two classes, firstmention and subsequent-mention. The agreement about antecedents was also not complete. These findings raise questions concerning the strategy of evaluating systems for definite description inby comparing their results with a standardized annotation. a linguistic of view, the most interesting observations were the great number of discourse-new definites in our corpus (in one of our experiments, about 50% of the definites in the collection were classified as discourse-new, 30% as anaphoric, and 18% as associative/bridging) and the presence of definites that did not seem to require a complete disambiguation. ", "introduction": "", "conclusion": "", "summary_sents": ["We present the results of a study of the use of definite descriptions in written texts aimed at assessing the feasibility of annotating corpora with information about definite description interpretation.", "We ran two experiments, in which subjects were asked to classify the uses of definite descriptions in a corpus of 33 newspaper articles, containing a total of l,412 definite descriptions.", "We measured the agreement among annotators about the classes assigned to definite descriptions, as well as the agreement about the antecedent assigned to those definites that the annotators classified as being related to an antecedent in the text.", "The most interesting result of this study from a corpus annotation perspective was the rather low agreement (K = 0.63) that we obtained using versions of Hawkins's and Prince's classification schemes; better results (K = 0.76) were obtained using the simplified scheme proposed by Fraurud that includes only two classes, firstmention and subsequent-mention.", "The agreement about antecedents was also not complete.", "These findings raise questions concerning the strategy of evaluating systems for definite description interpretation by comparing their results with a standardized annotation.", "From a linguistic point of view, the most interesting observations were the great number of discourse-new definites in our corpus (in one of our experiments, about 50% of the definites in the collection were classified as discourse-new, 30% as anaphoric, and 18% as associative/bridging) and the presence of definites that did not seem to require a complete disambiguation.", "We propose an annotation scheme, which is a product of a corpus based analysis of definite description (DD) use showing that more than 50% of the DDs in their corpus are discourse new or unfamiliar."]}
{"title": "Domain Adaptation for Statistical Machine Translation with Monolingual Resources", "abstract": "Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions. The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system. Previous work showed small performance gains by adapting from limited in-domain bilingual data. Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language. We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language. Investigations were conducted on a stateof-the-art phrase-based system trained on the Spanish\u2013English part of the UN corpus, and adapted on the corresponding Europarl data. Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline. By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set. ", "introduction": "A well-known problem of Statistical Machine Translation (SMT) is that performance quickly degrades as soon as testing conditions deviate from training conditions. The very simple reason is that the underlying statistical models always tend to closely approximate the empirical distributions of the training data, which typically consist of bilingual texts and monolingual target-language texts. The former provide a means to learn likely translations pairs, the latter to form correct sentences with translated words. Besides the general difficulties of language translation, which we do not consider here, there are two aspects that make machine learning of this task particularly hard. First, human language has intrinsically very sparse statistics at the surface level, hence gaining complete knowledge on translation phrase pairs or target language n-grams is almost impractical. Second, language is highly variable with respect to several dimensions, style, genre, domain, topics, etc. Even apparently small differences in domain might result in significant deviations in the underlying statistical models. While data sparseness corroborates the need of large language samples in SMT, linguistic variability would indeed suggest to consider many alternative data sources as well. By rephrasing a famous saying we could say that \u201cno data is better than more and assorted data\u201d. The availability of language resources for SMT has dramatically increased over the last decade, at least for a subset of relevant languages and especially for what concerns monolingual corpora. Unfortunately, the increase in quantity has not gone in parallel with an increase in assortment, especially for what concerns the most valuable resource, that is bilingual corpora. Large parallel data available to the research community are for the moment limited to texts produced by international organizations (European Parliament, United Nations, Canadian Hansard), press agencies, and technical manuals. The limited availability of parallel data poses challenging questions regarding the portability of SMT across different application domains and language pairs, and its adaptability with respect to language variability within the same application domain. This work focused on the second issue, namely the adaptation of a Spanish-to-English phrasebased SMT system across two apparently close domains: the United Nation corpus and the European Parliament corpus. Cross-domain adaptation is faced under the assumption that only monolingual texts are available, either in the source language or in the target language. The paper is organized as follows. Section 2 presents previous work on the problem of adaptation in SMT; Section 3 introduces the exemplar task and research questions we addressed; Section 4 describes the SMT system and the adaptation techniques that were investigated; Section 5 presents and discusses experimental results; and Section 6 provides conclusions. ", "conclusion": "This paper investigated cross-domain adaptation of a state-of-the-art SMT system (Moses), by exploiting large but cheap monolingual data. We proposed to generate synthetic parallel data by translating monolingual adaptation data with a background system and to train statistical models from the synthetic corpus. We found that the largest gain (25% relative) is achieved when in-domain data are available for the target language. A smaller performance improvement is still observed (5% relative) if source adaptation data are available. We also observed that the most important role is played by the LM adaptation, while the adaptation of the TM and RM gives consistent but small improvement. We also showed that a very tiny development set of only 200 parallel sentences is adequate enough to get comparable performance as a 2000-sentence set. Finally, we described how to reduce the time for training models from a synthetic corpus generated through Moses by 50% at least, by exploiting word-alignment information provided during decoding. ", "summary_sents": ["Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions.", "The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system.", "Previous work showed small performance gains by adapting from limited in-domain bilingual data.", "Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language.", "We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language.", "Investigations were conducted on a state-of-the-art phrase-based system trained on the Spanish\u2013English part of the UN corpus, and adapted on the corresponding Europarl data.", "Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline.", "By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set.", "In order to use source-side monolingual data, we employ the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence.", "We adapt an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses."]}
{"title": "A Program For Aligning Sentences In Bilingual Corpora", "abstract": "Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language. This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences. It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English\u2014French subcorpus than on the English\u2014German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs. To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the an appendix is provided with detailed c-code of the more difficult core of the program. ", "introduction": "", "conclusion": "", "summary_sents": ["Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English).", "One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.", "This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths.", "The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences.", "A probabilistic score is assigned to each proposed correspondence of sen tences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference.", "This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.", "It is remarkable that such a simple approach works as well as it does.", "An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German.", "The method correctly aligned all but 4% of the sentences.", "Moreover, it is possible to extract a large subcorpus that has a much smaller error rate.", "By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%.", "There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.", "To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI).", "In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program.", "We present a hybrid approach, and the basic hypothesis is that longer sentences in one language tend to be translated into longer sentences in the other language, and shorter sentences tend to be translated into shorter sentences.", "We propose a dynamic programming algorithm for the sentence-level alignment of translations that exploited two facts: the length of translated sentences roughly corresponds to the length of the original sentences and the sequence of sentences in translated text largely corresponds to the original order of sentences."]}
{"title": "SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems", "abstract": "The goal of this task is to allow for comparison across sense-induction and discrim ination systems, and also to compare thesesystems to other supervised and knowledgebased systems. In total there were 6 participating systems. We reused the SemEval 2007 English lexical sample subtask of task17, and set up both clustering-style unsuper vised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping). We provide a comparison to the results ofthe systems participating in the lexical sam ple subtask of task 17. ", "introduction": "Word Sense Disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, butneed large amounts of hand-tagging data. Exist ing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with WordNetsenses (Fellbaum, 1998) allow for a small improve ment over the simple most frequent sense heuristic,as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In the ory, larger amounts of training data (SemCor hasapprox. 500M words) would improve the perfor mance of supervised WSD, but no current projectexists to provide such an expensive resource. An other problem of the supervised approach is that theinventory and distribution of senses changes dra matically from one domain to the other, requiring additional hand-tagging of corpora (Mart??nez and Agirre, 2000; Koeling et al, 2005). Supervised WSD is based on the ?fixed-list of senses? paradigm, where the senses for a target wordare a closed list coming from a dictionary or lex icon. Lexicographers and semanticists have long warned about the problems of such an approach,where senses are listed separately as discrete entities, and have argued in favor of more complex rep resentations, where, for instance, senses are dense regions in a continuum (Cruse, 2000).Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of think ing, and tries to induce word senses directly fromthe corpus. Typical WSID systems involve clustering techniques, which group together similar examples. Given a set of induced clusters (which repre sent word uses or senses1), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense. One of the problems of unsupervised systems isthat of managing to do a fair evaluation. Most of cur rent unsupervised systems are evaluated in-house, with a brief comparison to a re-implementation of aformer system, leading to a proliferation of unsuper vised systems with little ground to compare amongthem. The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other su pervised and knowledge-based systems. The paper is organized as follows. Section 2 presents the evaluation framework used in this task. Section 3 presents the systems that participated in 1WSID approaches prefer the term ?word uses? to ?word senses?. In this paper we use them interchangeably to refer to both the induced clusters, and to the word senses from some reference lexicon. 7 the task, and the official results. Finally, Section 5 draws the conclusions. ", "conclusion": "We have presented the design and results of theSemEval-2007 task 02 on evaluating word sense induction and discrimination systems. 6 systems participated, but one of them was not a sense induction system. We reused the data from the SemEval 2007 English lexical sample subtask of task 17, and. set up both clustering-style unsupervised evaluation(using OntoNotes senses as gold-standard) and a su pervised evaluation (using the training part of thedataset for mapping). We also provide a compari son to the results of the systems participating in the lexical sample subtask of task 17.Evaluating clustering solutions is not straightfor ward. The unsupervised evaluation seems to besensitive to the number of senses in the gold stan dard, and the coarse grained sense inventory usedin the gold standard had a great impact in the results. The supervised evaluation introduces a mapping step which interacts with the clustering solu tion. In fact, the ranking of the participating systems 3All systems in the case of a random train/test split varies according to the evaluation method used. We think the two evaluation results should be taken to be complementary regarding the information learned by the clustering systems, and that the evaluation of word sense induction and discrimination systemsneeds further developments, perhaps linked to a cer tain application or purpose. Acknowledgments We want too thank the organizers of SemEval-2007 task 17 for kindly letting us use their corpus. We are also grateful to Ted Pedersen for his comments on the evaluation results. This work has been partially funded by the Spanish education ministry (project KNOW) and by the regional government of Gipuzkoa (project DAHAD). ", "summary_sents": ["The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems.", "In total there were 6 participating systems.", "We reused the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using Onto Notes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping).", "We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.", "The object of the sense induction task of SENSEVAL-4 is to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes.", "Graph-based methods have been employed for word sense induction."]}
{"title": "Arabic Tokenization Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop", "abstract": "We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties. ", "introduction": "Arabic is a morphologically complex language.1 The morphological analysis of a word consists of determining the values of a large number of (orthogonal) features, such as basic part-of-speech (i.e., noun, verb, and so on), voice, gender, number, information about the clitics, and so on.2 For Arabic, this gives us about 333,000 theoretically possible completely specified morphological analyses, i.e., morphological tags, of which about 2,200 are actually used in the first 280,000 words of the Penn Arabic Treebank (ATB). In contrast, English morphological tagsets usually have about 50 tags, which cover all morphological variation. As a consequence, morphological disambiguation of a word in context, i.e., choosing a complete 1We would like to thank Mona Diab for helpful discussions. The work reported in this paper was supported by NSF Award 0329163. The authors are listed in alphabetical order. 2In this paper, we only discuss inflectional morphology. Thus, the fact that the stem is composed of a root, a pattern, and an infix vocalism is not relevant except as it affects broken plurals and verb aspect. morphological tag, cannot be done successfully using methods developed for English because of data sparseness. Haji\u02c7c (2000) demonstrates convincingly that morphological disambiguation can be aided by a morphological analyzer, which, given a word without any context, gives us the set of all possible morphological tags. The only work on Arabic tagging that uses a corpus for training and evaluation (that we are aware of), (Diab et al., 2004), does not use a morphological analyzer. In this paper, we show that the use of a morphological analyzer outperforms other tagging methods for Arabic; to our knowledge, we present the best-performing wide-coverage tokenizer on naturally occurring input and the bestperforming morphological tagger for Arabic. ", "conclusion": "We have shown how to use a morphological analyzer for tokenization, part-of-speech tagging, and morphological disambiguation in Arabic. We have shown that the use of a morphological analyzer is beneficial in POS tagging, and we believe our results are the best published to date for tokenization of naturally occurring input (in undiacritized orthography) and POS tagging. We intend to apply our approach to Arabic dialects, for which currently no annotated corpora exist, and for which very few written corpora of any kind exist (making the dialects bad candidates even for unsupervised learning). However, there is a fair amount of descriptive work on dialectal morphology, so that dialectal morphological analyzers may be easier to come by than dialect corpora. We intend to explore to what extent we can transfer models trained on Standard Arabic to dialectal morphological disambiguation. ", "summary_sents": ["We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including part-of-speech tagging) Arabic words in one process.", "We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer.", "We obtain accuracy rates on all tasks in the high nineties.", "For choosing the best Buckwalter morphological analyzer (BAMA) results, we simply count the number of predicted values for the set of linguistic features in each candidate analysis."]}
{"title": "MindNet: Acquiring and Structuring Semantic Information from Text", "abstract": "As a lexical knowledge base constructed automatically from the definitions and example sentences in two machine-readable dictionaries (MRDs), MindNet embodies several features that distinguish it from prior work with MRDs. It is, however, more than this static resource alone. MindNet represents a general methodology for acquiring, structuring, accessing, and exploiting semantic information from natural language text. This paper provides an overview of the distinguishing characteristics of MindNet, the steps involved in its creation, and its extension beyond dictionary text. ", "introduction": "In this paper, we provide a description of the salient characteristics and functionality of MindNet as it exists today, together with comparisons to related work. We conclude with a discussion on extending the MindNet methodology to the processing of other corpora (specifically, to the text of the Microsoft Encarta\u00ae 98 Encyclopedia) and on future plans for MindNet. For additional details and background on the creation and use of MindNet, readers are referred to Richardson (1997), Vanderwende (1996), and Dolan et al. (1993). ", "conclusion": "", "summary_sents": ["As a lexical knowledge base constructed automatically from the definitions and example sentences in two machine-readable dictionaries (MRDs), MindNet embodies several features that distinguish it from prior work with MRDs.", "It is, however, more than this static resource alone.", "MindNet represents a general methodology for acquiring, structuring, accessing, and exploiting semantic information from natural language text.", "MindNet is both an extraction methodology and a lexical ontology different from a word net since it was created automatically from a dictionary and its structure is based on such resources."]}
{"title": "Integrating Multiple Knowledge Sources To Disambiguate Word Sense: An Exemplar-Based Approach", "abstract": "In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WORDNET. ", "introduction": "One important problem of Natural Language Processing (NLP) is figuring out what a word means when it is used in a particular context. The different meanings of a word are listed as its various senses in a dictionary. The task of Word Sense Disambiguation (WSD) is to identify the correct sense of a word in context. Improvement in the accuracy of identifying the correct word sense will result in better machine translation systems, information retrieval systems, etc. For example, in machine translation, knowing the correct word sense helps to select the appropriate target words to use in order to translate into a target language. In this paper, we present a new approach for WSD using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech (POS) of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. To evaluate our WSD program, named LEXAS (LEXical Ambiguity-resolving System), we tested it on a common data set involving the noun &quot;interest&quot; used by Bruce and Wiebe (Bruce and Wiebe, 1994). LEXAS achieves a mean accuracy of 87.4% on this data set, which is higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994). Moreover, to test the scalability of LEXAS, we have acquired a corpus in which 192,800 word occurrences have been manually tagged with senses from WORDNET, which is a public domain lexical database containing about 95,000 word forms and 70,000 lexical concepts (Miller, 1990). These sense tagged word occurrences consist of 191 most frequently occurring and most ambiguous nouns and verbs. When tested on this large data set, LEXAS performs better than the default strategy of picking the most frequent sense. To our knowledge, this is the first time that a WSD program has been tested on such a large scale, and yielding results better than the most frequent heuristic on highly ambiguous words with the refined sense distinctions of WORDNET. ", "conclusion": "In this paper, we have presented a new approach for WSD using an exemplar based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense. When tested on a common data set, our WSD program gives higher classification accuracy than previous work on WSD. When tested on a large, separately collected data set, our program performs better than the default strategy of picking the most frequent sense. To our knowledge, this is the first time that a WSD program has been tested on such a large scale, and yielding results better than the most frequent heuristic on highly ambiguous words with the refined senses of WORDNET. ", "summary_sents": ["In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm.", "This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation.", "We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed.", "LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET.", "We obtain an overall accuracy for the noun interest of 87% and find that when our feature sets consists only of co-occurrence features the accuracy only drops to 80%.", "We conclude taht collocational information is more important than syntactic information to WSD.", "Our DSO corpus focuses on 191 frequent and polysemous words (nouns and verbs) and contains around 1,000 sentences per words."]}
{"title": "Weakly Supervised Learning for Hedge Classification in Scientific Literature", "abstract": "We investigate automatic classification of speculative language (\u2018hedging\u2019), in biomedical text using weakly supervised machine learning. Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented. We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research. ", "introduction": "The automatic processing of scientific papers using NLP and machine learning (ML) techniques is an increasingly important aspect of technical informatics. In the quest for a deeper machine-driven \u2018understanding\u2019 of the mass of scientific literature, a frequently occuring linguistic phenomenon that must be accounted for is the use of hedging to denote propositions of a speculative nature. Consider the following: The second example contains a hedge, signaled by the use of suggest and might, which renders the proposition inhibit(XfK89\u2014*Felin-9) speculative. Such analysis would be useful in various applications; for instance, consider a system designed to identify and extract interactions between genetic entities in the biomedical domain. Case 1 above provides clear textual evidence of such an interaction 992 and justifies extraction of inhibit(XfK89\u2014*Felin-9), whereas case 2 provides only weak evidence for such an interaction. Hedging occurs across the entire spectrum of scientific literature, though it is particularly common in the experimental natural sciences. In this study we consider the problem of learning to automatically classify sentences containing instances of hedging, given only a very limited amount of annotatorlabelled \u2018seed\u2019 data. This falls within the weakly supervised ML framework, for which a range of techniques have been previously explored. The contributions of our work are as follows: ", "conclusion": "We have shown that weakly supervised ML is applicable to the problem of hedge classification and that a reasonable level of accuracy can be achieved. The work presented here has application in the wider academic community; in fact a key motivation in this study is to incorporate hedge classification into an interactive system for aiding curators in the construction and population of gene databases. We have presented our initial results on the task using a simple probabilistic model in the hope that this will encourage others to investigate alternative learning models and pursue new techniques for improving accuracy. Our next aim is to explore possibilities of introducing linguistically-motivated knowledge into the sample representation to help the learner identify key hedge-related sentential components, and also to consider hedge classification at the granularity of assertions rather than text sentences. ", "summary_sents": ["We investigate automatic classification of speculative language ('hedging'), in biomedical text using weakly supervised machine learning.", "Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented.", "We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research.", "We use single words as input features in order to classify sentences from biological articles as speculative or non speculative.", "We extend the work of Light et al (2004) by refining their annotation guidelines and creating a publicly available data set (FlyBase data set) for speculative sentence classification.", "We find that our model is unsuccessful in identifying assertive statements of knowledge paucity which are generally marked rather syntactically than lexically."]}
{"title": "A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging", "abstract": "We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline. ", "introduction": "Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages. Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks. Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). Besides the usual character-based features, additional features dependent on POS\u2019s or words can also be employed to improve the performance. However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications. As a result, many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently. To cope with this problem, we propose a cascaded linear model inspired by the log-linear model (Och and Ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources. Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models. We will describe it in detail in Section 4. In this architecture, knowledge sources that are intractable to incorporate into the perceptron, can be easily incorporated into the outside linear model. In addition, as these knowledge sources are regarded as separate features, we can train their corresponding models independently with each other. This is an interesting approach when the training corpus is large as it reduces the time and space consumption. Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T. 2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l < r) denotes character sequence ranges from Cl to Cr. We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag. It is a better idea to perform segmentation and POS tagging jointly in a uniform framework. According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively. In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004). As each tag is now composed of a boundary part and a POS part, the joint S&T problem is transformed to a uniform boundary-POS labelling problem. A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN. ", "conclusion": "We proposed a cascaded linear model for Chinese Joint S&T. Under this model, many knowledge sources that may be intractable to be incorporated into the perceptron directly, can be utilized effectively in the outside-layer linear model. This is a substitute method to use both local and non-local features, and it would be especially useful when the training corpus is very large. However, can the perceptron incorporate all the knowledge used in the outside-layer linear model? If this cascaded linear model were chosen, could more accurate generative models (LMs, word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely, or by self-training on raw corpus in a similar approach to that of McClosky (2006)? In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low, 2004). How can we utilize these knowledge sources effectively? We will investigate these problems in the following work. ", "summary_sents": ["We propose a cascaded linear model for joint Chinese word segmentation and part-of-speech tagging.", "With a character-based perceptron as the core, combined with real-valued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.", "Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of 18.5% on segmentation and 12% on joint segmentation and part-of-speech tagging over the perceptron-only baseline.", "For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j."]}
{"title": "Language Model Adaptation For Statistical Machine Translation Via Structured Query Models", "abstract": "We explore unsupervised language model adaptation techniques for Statistical Machine Translation. The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection. Specific language models are then build from the retrieved data and interpolated with a general background model. Experiments show significant improvements when translating with these adapted language models. ", "introduction": "Language models (LM) are applied in many natural language processing applications, such as speech recognition and machine translation, to encapsulate syntactic, semantic and pragmatic information. For systems which learn from given data we frequently observe a severe drop in performance when moving to a new genre or new domain. In speech recognition a number of adaptation techniques have been developed to cope with this situation. In statistical machine translation we have a similar situation, i.e. estimate the model parameter from some data, and use the system to translate sentences which may not be well covered by the training data. Therefore, the potential of adaptation techniques needs to be explored for machine translation applications. Statistical machine translation is based on the noisy channel model, where the translation hypothesis is searched over the space defined by a translation model and a target language (Brown et al, 1993). Statistical machine translation can be formulated as follows: )()|(maxarg)|(maxarg* tPtsPstPt tt ?== where t is the target sentence, and s is the source sentence. P(t) is the target language model and P(s|t) is the translation model. The argmax operation is the search, which is done by the decoder. In the current study we modify the target language model P(t), to represent the test data better, and thereby improve the translation quality. (Janiszek, et al 2001) list the following approaches to language model adaptation: ? Linear interpolation of a general and a domain specific model (Seymore, Rosenfeld, 1997). Back off of domain specific probabilities with those of a specific model (Besling, Meier, 1995). Retrieval of documents pertinent to the new domain and training a language model on-line with those data (Iyer, Ostendorf, 1999, Mahajan et. al. 1999). Maximum entropy, minimum discrimination adaptation (Chen, et. al., 1998). Adaptation by linear transformation of vectors of bigram counts in a reduced space (DeMori, Federico, 1999). Smoothing and adaptation in a dual space via latent semantic analysis, modeling long-term semantic dependencies, and trigger combinations. (J. Bellegarda, 2000). Our approach can be characterized as unsupervised data augmentation by retrieval of relevant documents from large monolingual corpora, and interpolation of the specific language model, build from the retrieved data, with a background language model. To be more specific, the following steps are carried out to do the language model adaptation. First, a baseline statistical machine translation system, using a large general language model, is applied to generate initial translations. Then these translations hypotheses are reformulated as queries to retrieve similar sentences from a very large text collection. A small domain specific language model is build using the retrieved sentences and linearly interpolated with the background language model. This new interpolated language model in applied in a second decoding run to produce the final translations. There are a number of interesting questions pertaining to this approach: ? Which information can and should used to generate the queries: the first-best translation only, or also translation alternatives. How should we construct the queries, just as simple bag-of-words, or can we incorporate more structure to make them more powerful. How many documents should be retrieved to build the specific language models, and on what granularity should this be done, i.e. what is a document in the information retrieval process. The paper is structured as follows: section 2 outlines the sentence retrieval approach, and three bag-of-words query models are designed and explored; structured query models are introduced in section 3. In section 4 we present translation experiments are presented for the different query. Finally, summary is given in section 5. ", "conclusion": "", "summary_sents": ["We explore unsupervised language model adaptation techniques for Statistical Machine Translation.", "The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.", "Specific language models are then build from the retrieved data and interpolated with a general background model.", "Experiments show significant improvements when translating with these adapted language models.", "We apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus.", "We construct specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora.", "We convert initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection."]}