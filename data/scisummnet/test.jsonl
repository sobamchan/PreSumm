{"title": "D-Theory: Talking About Talking About Trees", "abstract": "Linguists, including computational linguists, have always been fond of talking about trees. In this paper, we outline a theory of linguistic structure which talks about talking about trees; we call theory theory While important issues must be resolved before a complete picture of D-theory emerges (and also before we can build programs which utilize it), we believe that this theory will ultimately provide a framework for explaining the syntax and semantics of natural in a manner which is This paper will focus primarily on one set of motivations for this theory, those engendered by attempts to handle certain syntactic phenomena within the framework of deterministic parsing. ", "introduction": "", "conclusion": "", "summary_sents": ["Linguists, including computational linguists, have always been fond of talking about trees.", "In this paper, we outline a theory of linguistic structure which talks about talking about trees; we call this theory Description theory (D-theory).", "While important issues must be resolved before a complete picture of D-theory emerges (and also before we can build programs which utilize it), we believe that this theory will ultimately provide a framework for explaining the syntax and semantics of natural language in a manner which is intrinsically computational.", "This paper will focus primarily on one set of motivations for this theory, those engendered by attempts to handle certain syntactic phenomena within the framework of deterministic parsing.", "Our D-theory model is powerful in that it allows the right-most daughter of a node to be lowered under a sibling node."]}
{"title": "Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure", "abstract": "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure. While previous work has focused primarily on English, we extend these results to other languages along two dimensions. First, we show that these results hold true for a number of languages across families. Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction. Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%. When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%. ", "introduction": "The ability to predict the linguistic structure of sentences or documents is central to the field of natural language processing (NLP). Structures such as named-entity tag sequences (Bikel et al., 1999) or sentiment relations (Pang and Lee, 2008) are inherently useful in data mining, information retrieval and other user-facing technologies. More fundamental structures such as part-of-speech tag sequences (Ratnaparkhi, 1996) or syntactic parse trees (Collins, 1997; K\u00a8ubler et al., 2009), on the other hand, comprise the core linguistic analysis for many important downstream tasks such as machine translation (Chiang, * The majority of this work was performed while the author was an intern at Google, New York, NY. 2005; Collins et al., 2005). Currently, supervised data-driven methods dominate the literature on linguistic structure prediction (Smith, 2011). Regrettably, the majority of studies on these methods have focused on evaluations specific to English, since it is the language with the most annotated resources. Notable exceptions include the CoNLL shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003; Buchholz and Marsi, 2006; Nivre et al., 2007) and subsequent studies on this data, as well as a number of focused studies on one or two specific languages, as discussed by Bender (2011). While annotated resources for parsing and several other tasks are available in a number of languages, we cannot expect to have access to labeled resources for all tasks in all languages. This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction of linguistic structure. These methods all attempt to benefit from the plethora of unlabeled monolingual and/or cross-lingual data that has become available in the digital age. Unsupervised methods are appealing in that they are often inherently language independent. This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages (Cohen and Smith, 2009; Gillenwater et al., 2010; Naseem et al., 2010; Spitkovsky et al., 2011). However, the performance for most languages is still well below that of supervised systems and recent work has established that the performance is also below simple methods of linguistic transfer (McDonald et al., 2011). In this study we focus on semi-supervised and linguistic-transfer methods for multilingual structure prediction. In particular, we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction: guages for dependency parsing and 4 languages for named-entity recognition (NER). This is the first study with such a broad view on this subject, in terms of language diversity. 2. Cross-lingual word cluster features for transferring linguistic structure from English to other languages. We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across languages. This is achieved by means of a probabilistic model over large amounts of monolingual data in two languages, coupled with parallel data through which cross-lingual word-cluster constraints are enforced. We show that by augmenting the delexicalized direct transfer system of McDonald et al. (2011) with cross-lingual cluster features, we are able to reduce its error by up to 13% relative. Further, we show that by applying the same method to direct-transfer NER, we achieve a relative error reduction of 26%. In line with much previous work on word clusters for tasks such as dependency parsing and NER, for which local syntactic and semantic constraints are of importance, we induce word clusters by means of a probabilistic class-based language model (Brown et al., 1992; Clark, 2003). However, rather than the more commonly used model of Brown et al. (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). The two models are very similar, but whereas the former takes classto-class transitions into account, the latter directly models word-to-class transitions. By ignoring classto-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). This is a useful property, as we later develop an algorithm for inducing cross-lingual word clusters that calls this monolingual algorithm as a subroutine. More formally, let C : V H 1, ... , K be a (hard) clustering function that maps each word type from the vocabulary, V, to one of K cluster identities. With the model of Uszkoreit and Brants (2008), the likelihood of a sequence of word tokens, w = (wi)mi=1, with wi E V U {S}, where S is a designated start-ofsegment symbol, factors as Compare this to the model of Brown et al. (1992): By incorporating cross-lingual cluster features in a m linguistic transfer system, we are for the first time L'(w; C) = P(wi|C(wi))P(C(wi)|C(wi\u22121)) . combining SSL and cross-lingual transfer. i=1 ", "conclusion": "In the first part of this study, we showed that word clusters induced from a simple class-based language model can be used to significantly improve on stateof-the-art supervised dependency parsing and NER for a wide range of languages and even across language families. Although the improvements vary between languages, the addition of word cluster features never has a negative impact on performance. This result has important practical consequences as it allows practitioners to simply plug in word cluster features into their current feature models. Given previous work on word clusters for various linguistic structure prediction tasks, these results are not too surprising. However, to our knowledge this is the first study to apply the same type of word cluster features across languages and tasks. In the second part, we provided two simple methods for inducing cross-lingual word clusters. The first method works by projecting word clusters, induced from monolingual data, from a source language to a target language directly via word alignments. The second method, on the other hand, makes use of monolingual data in both the source and the target language, together with word alignments that act as constraints on the joint clustering. We then showed that by using these cross-lingual word clusters, we can significantly improve on direct transfer of discriminative models for both parsing and NER. As in the monolingual case, both types of cross-lingual word cluster features yield improvements across the board, with the more complex method providing a significantly larger improvement for NER. Although the performance of transfer systems is still substantially below that of supervised systems, this research provides one step towards bridging this gap. Further, we believe that it opens up an avenue for future work on multilingual clustering methods, cross-lingual feature projection and domain adaptation for direct transfer of linguistic structure. ", "summary_sents": ["It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.", "While previous work has focused primarily on English, we extend these results to other languages along two dimensions.", "First, we show that these results hold true for a number of languages across families.", "Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.", "Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.", "When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%."]}
{"title": "Syntax Annotation for the GENIA Corpus", "abstract": "Linguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for bio textmining. As the focus of information extraction is shifting from \"nominal\" information such as named entity to \"verbal\" information such as function and interaction of substances, applica tion of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sen tences is in demand. A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XML based format based on Penn Treebank II (PTB) scheme. Inter-annotator agreement test indicated that the writ ing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of bi ology with appropriate guidelines regarding to linguistic phenomena par ticular to scientific texts. ", "introduction": "Research and development for information extraction from biomedical literature (bio textmining) has been rapidly advancing due to demands caused by information overload in the genome-related field. Natural language process ing (NLP) techniques have been regarded as useful for this purpose. Now that focus of in formation extraction is shifting from extraction of ?nominal? information such as named entity to ?verbal? information such as relations of enti ties including events and functions, syntactic analysis is an important issue of NLP application in biomedical domain. In extraction of rela tion, the roles of entities participating in the relation must be identified along with the verb that represents the relation itself. In text analysis, this corresponds to identifying the subjects, ob jects, and other arguments of the verb. Though rule-based relation information ex traction systems using surface pattern matching and/or shallow parsing can achieve high precision (e.g. Koike et al, 2004) in a particular target domain, they tend to suffer from low recall due to the wide variation of the surface ex pression that describe a relation between a verb and its arguments. In addition, the portability of such systems is low because the system has to be re-equipped with different set of rules when different kind of relation is to be extracted. One solution to this problem is using deep parsers which can abstract the syntactic variation of a relation between a verb and its arguments repre sented in the text, and constructing extraction rule on the abstract predicate-argument structure. To do so, wide-coverage and high-precision parsers are required. While basic NLP techniques are relatively general and portable from domain to domain, customization and tuning are inevitable, especially in order to apply the techniques effec tively to highly specialized literatures such as research papers and abstracts. As recent advances in NLP technology depend on machine learning techniques, annotated corpora from which system can acquire rules (including grammar rules, lexicon, etc.) are indispensable 220 resources for customizing general-purpose NLP tools. In bio-textmining, for example, training on part-of-speech (POS)-annotated GENIA cor pus was reported to improve the accuracy of JunK tagger (English POS tagger) (Kazama et al., 2001) from 83.5% to 98.1% on MEDLINE abstracts (Tateisi and Tsujii, 2004), and the FraMed corpus (Wermter and Hahn, 2004) was used to train TnT tagger on German (Brants, 2000) to improve its accuracy from 95.7% to 98% on clinical reports and other biomedical texts. Corpus annotated for syntactic structures is expected to play a similar role in tuning parsers to biomedical domain, i.e., similar improve ment on the performance of parsers is expected by using domain-specific treebank as a resource for learning. For this purpose, we construct GENA Treebank (GTB), a treebank on research abstracts in biomedical domain. ", "conclusion": "A subset of the GENIA corpus is annotated for syntactic (tree) structure. Inter-annotator agreement test indicated that the annotation can be done stably by linguists without much knowledge in biology, provided that proper guideline is established for linguistic phenomena particular to scientific research abstracts. We have made the 500-abstract corpus in both XML and PTB formats and made it publicly available as ?the GENIA Treebank beta version? (GTB beta). We are in further cleaning up process of the 500-abstract set, and at the same time, initial annotation of the remaining abstracts is being done, so that the full GENIA set of 2000 ab stracts will be annotated with tree structure. For parsers to be useful for information ex traction, they have to establish a map between syntactic structure and more semantic predicate argument structure, and between the linguistic predicate-argument structures to the factual relation to be extracted. Annotation of various in formation on a same set of text can help establish these maps. For the factual relations, we are annotating relations between proteins and genes in cooperation with a group of biologists. For predicate-argument annotation, we are in vestigating the use of the parse results of the Enju parser. Acknowledgments The authors are grateful to annotators and colleagues that helped the construction of the corpus. This work is partially supported by Grant in-Aid for Scientific Research on Priority Area C ?Genome Information Science? from the Min istry of Education, Culture, Sports, Science and Technology of Japan. ", "summary_sents": ["Linguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for bio-textmining.", "As the focus of information extraction is shifting from \"nominal\" information such as named entity to \"verbal\" information such as function and interaction of substances, application of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sentences is in demand.", "A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XML-based format based on Penn Treebank II (PTB) scheme.", "Inter-annotator agreement test indicated that the writing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of biology with appropriate guidelines regarding to linguistic phenomena particular to scientific texts.", "Our GENIA Treebank Corpus is estimated to have no imperative sentences and only seven interrogative sentences."]}
{"title": "A Semantic Approach To IE Pattern Induction", "abstract": "This paper presents a novel algorithm for the acquisition of Information Extraction patterns. The approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant. Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity. Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach. ", "introduction": "Developing systems which can be easily adapted to new domains with the minimum of human intervention is a major challenge in Information Extraction (IE). Early IE systems were based on knowledge engineering approaches but suffered from a knowledge acquisition bottleneck. For example, Lehnert et al. (1992) reported that their system required around 1,500 person-hours of expert labour to modify for a new extraction task. One approach to this problem is to use machine learning to automatically learn the domain-specific information required to port a system (Riloff, 1996). Yangarber et al. (2000) proposed an algorithm for learning extraction patterns for a small number of examples which greatly reduced the burden on the application developer and reduced the knowledge acquisition bottleneck. Weakly supervised algorithms, which bootstrap from a small number of examples, have the advantage of requiring only small amounts of annotated data, which is often difficult and time-consuming to produce. However, this also means that there are fewer examples of the patterns to be learned, making the learning task more challenging. Providing the learning algorithm with access to additional knowledge can compensate for the limited number of annotated examples. This paper presents a novel weakly supervised algorithm for IE pattern induction which makes use of the WordNet ontology (Fellbaum, 1998). Extraction patterns are potentially useful for many language processing tasks, including question answering and the identification of lexical relations (such as meronomy and hyponymy). In addition, IE patterns encode the different ways in which a piece of information can be expressed in text. For example, \u201cAcme Inc. fired Jones\u201d, \u201cAcme Inc. let Jones go\u201d, and \u201cJones was given notice by his employers, Acme Inc.\u201d are all ways of expressing the same fact. Consequently the generation of extraction patterns is pertinent to paraphrase identification which is central to many language processing problems. We begin by describing the general process of pattern induction and an existing approach, based on the distribution of patterns in a corpus (Section 2). We then introduce a new algorithm which makes use of WordNet to generalise extraction patterns (Section 3) and describe an implementation (Section 4). Two evaluation regimes are described; one based on the identification of relevant documents and another which aims to identify sentences in a corpus which are relevant for a particular IE task (Section 5). Results on each of these evaluation regimes are then presented (Sections 6 and 7). ", "conclusion": "", "summary_sents": ["This paper presents a novel algorithm for the acquisition of Information Extraction patterns.", "The approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant.", "Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity.", "Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach.", "We propose a weakly supervised approach to sentence filtering that uses semantic similarity and bootstrapping to acquire IE patterns.", "We use subject-verb-object triples for the features."]}
{"title": "Multilingual Authoring using Feedback Texts", "abstract": "There are obvious reasons for trying to automate the production of multilingual documentation, especially for routine subject-matter in restricted domains (e.g. technical instructions). Two approaches have been adopted: Machine Translation (MT) of a source text, and Multilingual Natural Language Generation (M-NLG) from a knowledge base. For MT, information extraction is a major difficulty, since the meaning must be derived by analysis of the source text; M-NLG avoids this difficulty but seems at first sight to require an expensive phase of knowledge engineering in order to encode the meaning. We introduce here a new technique which employs M-NLG during the phase of knowledge editing. A 'feedback text', generated from a possibly incomplete knowledge base, describes in natural language the knowledge encoded so far, and the options for extending it. This method allows anyone speaking one of the supported languages to produce texts in all of them, requiring from the author only expertise in the subject-matter, not expertise in knowledge engineering. ", "introduction": "The production of multilingual documentation has an obvious practical importance. Companies seeking global markets for their products must provide instructions or other reference materials in a variety of languages. Large political organizations like the European Union are under pressure to provide multilingual versions of official documents, especially when communicating with the public. This need is met mostly by human translation: an author produces a source document which is passed to a number of other people for translation into other languages. Human translation has several well-known disadvantages. It is not only costly but timeconsuming, often delaying the release of the product in some markets; also the quality is uneven and hard to control (Hartley and Paris, 1997). For all these reasons, the production of multilingual documentation is an obvious candidate for automation, at least for some classes of document. Nobody expects that automation will be applied in the foreseeable future for literary texts ranging over wide domains (e.g. novels). However, there is a mass of non-literary material in restricted domains for which automation is already a realistic aim: instructions for using equipment are a good example. The most direct attempt to automize multilingual document production is to replace the human translator by a machine. The source is still a natural language document written by a human author; a program takes this source as input, and produces an equivalent text in another language as output. Machine translation has proved useful as a way of conveying roughly the information expressed by the source, but the output texts are typically poor and over-literal. The basic problem lies in the analysis phase: the program cannot extract from the source all the information that it needs in order to produce a good output text. This may happen either because the source is itself poor (e.g. ambiguous or incomplete), or because the source uses constructions and concepts that lie outside the program's range. Such problems can be alleviated to some extent by constraining the source document, e.g. through use of a 'Controlled Language' such as AECMA (1995). An alternative approach to translation is that of generating the multilingual documents from a non-linguistic source. In the case of automatic Multilingual Natural Language Generation (MNLG), the source will be a knowledge base expressed in a formal language. By eliminating the analysis phase of MT, M-NLG can yield high-quality output texts, free from the 'literal' quality that so often arises from structural imitation of an input text. Unfortunately, this benefit is gained at the cost of a huge increase in the difficulty of obtaining the source. No longer can the domain expert author the document directly by writing a text in natural language. Defining the source becomes a task akin to building an expert system, requiring collaboration between a domain expert (who understands the subjectmatter of the document) and a knowledge engineer (who understands the knowledge representation formalism). Owing to this cost, M-NLG has been applied mainly in contexts where the knowledge base is already available, having been created for another purpose (Iordanskaja et al., 1992; Goldberg et al., 1994); for discussion see Reiter and Mellish (1993). Is there any way in which a domain expert might author a knowledge base without going through this time-consuming and costly collaboration with a knowledge engineer? Assuming that some kind of mediation is needed between domain expert and knowledge formalism, the only alternative is to provide easier tools for editing knowledge bases. Some knowledge management projects have experimented with graphical presentations which allow editing by direct manipulation, so that there is no need to learn the syntax of a programming language see for example Skuce and Lethbridge (1995). This approach has also been adopted in two M-NLG systems: GIST (Power and Cavallotto, 1996), which generates social security forms in English, Italian and German; and DRAFTER (Paris et al., 1995), which generates instructions for software applications in English and French. These projects were the first attempts to produce symbolic authoring systems - that is, systems allowing a domain expert with no training in knowledge engineering to author a knowledge base (or symbolic source) from which texts in many languages can be generated. Although helpful, graphical tools for managing knowledge bases remain at best a compromise solution. Diagrams may be easier to understand than logical formalisms, but they still lack the flexibility and familiarity of natural Ianguage text, as empirical studies on editing diagrammatic representations have shown (Kim, 1990; Petre, 1995); for discussion see Power et al. (1998). This observation has led us to explore a new possibility, at first sight paradoxical: that of a symbolic authoring system in which the current knowledge base is presented through a natural language text generated by the system. This kills two birds with one stone: the source is still a knowledge base, not a text, so no problem of analysis arises; but this source is presented to the author in natural language, through what we will call a feedback text. As we shall see, the feedback text has some special features which allow the author to edit the knowledge base as well as viewing its contents. We have called this editing method `WYSIWYM', or 'What You See Is What You Meant': a natural language text ('what you see') presents a knowledge base that the author has built by purely semantic decisions (' what you meant'). feedback texts to the author. The feedback texts will include mouse-sensitive 'anchors' allowing the author to make semantic decisions, e.g. by selecting options from pop-up menus. The WYSIWYM system allows a domain expert speaking any one of the supported languages to produce good output texts in all of them. A more detailed description of the architecture is given in Scott et al. (1998). 2 Example of a WYSIWYM system The first application of WYSIWYM was DRAFTER-II, a system which generates instuctions for using word processors and diary managers. At present three languages are supported: English, French and Italian. As an example, we will follow a session in which the author encodes instructions for scheduling an appointment with the OpenWindows Calendar Manager. The desired content is shown by the following output text, which the system will generate when the knowledge base is complete: To schedule the appointment: Before starting, open the Appointment Editor window by choosing the Appointment option from the Edit menu. Then proceed as follows: In outline, the knowledge base underlying this text is as follows. The whole instruction is represented by a procedure instance with two attributes: a goal (scheduling the appointment) and a method. The method instance also has two attributes: a precondition (expressed by the sentence beginning 'Before starting') and a sequence of steps (presented by the enumerated list). Preconditions and steps are procedures in their turn, so they may have methods as well as goals. Eventually we arrive at sub-procedures for which no method is specified: it is assumed that the reader of the manual will be able to click on the Insert button without being told how. Since in DRAFTER-H every output text is based on a procedure, a newly initialised knowledge base is seeded with a single procedure instance for which the goal and method are undefined. In Prolog notation, we can represent such a knowledge base by the following assertions: procedure (prod 1) . goal (procl, A) . method (proc I. , B) . Here prod l is an identifier for the procedure instance; the assertion procedure (prod) means that this is an instance of type procedure; and the assertion goal (prod, A) means that prod has a goal attribute for which the value is currently undefined (hence the variable A). When a new knowledge base is created, DRAFTER-II presents it to the author by generating a feedback text in the currently selected language. Assuming that this language is English, the instruction to the generator will be generate (prod, english, feedback) and the feedback text displayed to the author will be Achieve this goal by applying this method. This text has several special features. ing on an anchor, the author obtains a popup menu listing the permissible values of the attribute; by selecting one of these options, the author updates the knowledge base. Although the anchors may be tackled in any order, we will assume that the author proceeds from left to right. Clicking on this goal yields the pop-up menu choose click close create save schedule start (to save space, this figure omits some options), from which the author selects 'schedule'. Each option in the menu is associated with an 'updater', a Prolog term (not shown to the author) that specifies how the knowledge base should be updated if the option is selected. In this case the updater is insert(procl, goal, schedule) meaning that an instance of type schedule should become the value of the goal attribute on prod. Running the updater yields an extended knowledge base, including a new instance schedl with an undefined attribute actee. (Assertions describing attribute values are indented to make the knowledge base easier to read.) procedure (prod). goal (procl, sched1) . schedule (schedl) . actee ( schedl , C) . method (proc1 , B) . From the updated knowledge base, the generator produces a new feedback text. Schedule this event by applying this method. Note that this text has been completely regenerated. It was not produced from the previous text merely by replacing the anchor this goal by a longer string. Continuing to specify the goal, the author now clicks on this event. appointment meeting This time the intended selection is 'appointment', but let us assume that by mistake the author drags the mouse too far and selects 'meeting'. The feedback text Schedule the meeting by applying this method. immediately shows that an error has been made, but how can it be corrected? This problem is solved in WYSIWYM by allowing the author to select any span of the feedback text that represents an attribute with a specified value, and to cut it, so that the attribute becomes undefined, while its previous value is held in a buffer. Even large spans, representing complex attribute values, can be treated in this way, so that complex chunks of knowledge can be copied across from one knowledge base to another. When the author selects the phrase 'the meeting', the system displays a pop-up menu with two options: Cut Copy By selecting 'Cut', the author activates the updater cut (sched1 , actee) which updates the knowledge base by removing the instance meetl, currently the value of the actee attribute on schedl, and holding it in a buffer. With this attribute now undefined, the feedback text reverts to Schedule this event by applying this method. whereupon the author can once again expand this event. This time, however, the pop-up menu that opens on this anchor will include an extra option: that of pasting back the material that has just been cut. Of course this option is only provided if the instance currently held in the buffer is a suitable value for the attribute represented by the anchor. Paste appointment meeting The 'Paste' option here will be associated with the updater paste (sched1 , actee) which would assign the instance currently in the buffer, in this case meet1, as the value of the actee attribute on sched1. Fortunately the author avoids reinstating this error, and selects 'appointment', yielding the following reassuring feedback text: Schedule the appointment by applying this method. Note incidentally that this text presents a knowledge base that is potentially complete, since all obligatory attributes have been specified. This can be immediately seen from the absence of any red (bold) anchors. Intending to add a method, the author now clicks on this method. In this case, the pop-up menu shows only one option: method Running the associated updater yields the following knowledge base: procedure (prod). goal(procl, sched1) . schedule (schedl) . actee(sched1, apptl) . appointment(appt1). method(procl, method1). method(method1). precondition(method1, D). steps(method1, steps1). steps(steps1). first(stepsl, proc2). procedure (proc2) . goal(proc2, F) . method(proc2, G) . rest (stepsl , E) . meeting(meet1) . A considerable expansion has taken place here because the system has been configured to automatically instantiate obligatory attributes that have only one permissible type of value. (In other words, it never presents red anchors with pop-up menus having only one option.) Since the steps attribute on method1 is obligatory, and must have a value of type steps, the instance steps1 is immediately created. In its turn, this instance has the attributes first and rest (it is a list), where first is obligatory and must be filled by a procedure. A second procedure instance proc2 is therefore created, with its own goal and method. To incorporate all this new material, the feedback text is recast in a new pattern, the main goal being expressed by an infinitive construction instead of an imperative: To schedule the appointment: First, achieve this precondition. Then follow these steps. Note that at any stage the author can switch to one of the other supported languages, e.g. French. This will result in a new call to the generator and hence in a new feedback text expressing the procedure prod. Insertion du rendez-vous: Avant de commencer, accomplir cette tciche. Executer les actions suivantes. Clicking for example on cette action will now yield the usual options for instanciating a goal attribute, but expressed in French. The associated updaters are identical to those for the corresponding menu in English. choix cliquer fermer enregistrement insertion lancement The now be clear, so let us advance to a later stage in which the scheduling procedure has been fully encoded. To schedule the appointment: First, open the Appointment Editor window. Then follow these steps. To open the Appointment Editor window: First, achieve this precondition. Then follow these steps. basic mechanism should Two points about this feedback text are worth generate (procl , french, feedback) noting. First, to avoid overcrowding the main paragraph, the text planner has deferred the sub-procedure for opening the Appointment Editor window, which is presented in a separate paragraph. To maintain a connection, the action of opening the Appointment Editor window is mentioned twice (as it happens, through different constructions). Secondly, no red (bold) anchors are left, so the knowledge base is potentially complete. (Of course it could be extended further, e.g. by adding more steps.) This means that the author may now generate an output text by switching the modality from 'Feedback' to 'Output'. The resulting instruction to the generator will be generate(procl, english, output) yielding the output text shown at the beginning of the section. Further output texts can be obtained by switching to another language, e.g. French: Insertion du rendez-vous: Avant de commencer, ouvrir la fenetre Appointment Editor en choisissant l'option Appointment dans le menu Edit. Executer les actions suivantes: 1 Choisir l'heure de fin du rendezvous. 2 Inserer la description du rendezvous dans la zone de texte What. 3 Cliquer sur le bouton Insert. Note that in output modality the generator ignores optional undefined attributes; the method for opening the Appointment Editor window thus reduces to a single action which can be re-united with its goal in the main paragraph. ", "conclusion": "", "summary_sents": ["There are obvious reasons for trying to automate the production of multilingual documentation, especially for routine subject-matter in restricted domains (e.g. technical instructions).", "Two approaches have been adopted: Machine Translation (MT) of a source text, and Multilingual Natural Language Generation (M-NLG) from a knowledge base.", "For MT, information extraction is a major difficulty, since the meaning must be derived by analysis of the source text; M-NLG avoids this difficulty but seems at first sight to require an expensive phase of knowledge engineering in order to encode the meaning.", "We introduce here a new technique which employs M-NLG during the phase of knowledge editing.", "A 'feedback text', generated from a possibly incomplete knowledge base, describes in natural language the knowledge encoded so far, and the options for extending it.", "This method allows anyone speaking one of the supported languages to produce texts in all of them, requiring from the author only expertise in the subject-matter, not expertise in knowledge engineering.", "We propose WYSIWYM (What You See Is What You Mean) as a method for the authoring of semantic information through direct manipulation of structures rendered in natural language text.", "In this system logical forms are entered interactively and the corresponding linguistic realization of the expressions is generated in several languages."]}
{"title": "A Centering Approach To Pronouns", "abstract": "In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns. As described in [GJW86], the process of centering attention on entities in the discourse gives rise to the intersentential states of, retaining shiftpropose an extension to these states which handles some additional cases of multiple ambiguous pronouns. The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application. ", "introduction": "In the approach to discourse structure developed in [Sid83] and [GJW86], a discourse exhibits both global and local coherence. On this view, a key element of local coherence is centering, a system of rules and constraints that govern the relationship between what the discourse is about and some of the linguistic choices made by the discourse participants, e.g. choice of grammatical function, syntactic structure, and type of referring expression (proper noun, definite or indefinite description, reflexive or personal pronoun, etc.). Pronominalization in particular serves to focus attention on what is being talked about; inappropriate use or failure to use pronouns causes communication to be less fluent. For instance, it takes longer for hearers to process a pronominalized noun phrase that is not in focus than one that is, while it takes longer to process a non-pronominalized noun phrase that is in focus than one that is not [G ui85]. The [G3W86] centering model is based on the following assumptions. A discourse segment consists of a sequence of utterances U1, , Um. With each utterance Un is associated a list of forward-looking centers, C f(Un), consisting of those discourse entities that are directly realized or realized' by linguistic expressions in the utterance. Ranking of an entity on this list corresponds roughly to the likelihood that it will be the primary focus of subsequent discourse; the first entity on this list is the preferred center, Cp(Un). Un actually centers, or is &quot;about&quot;, only one entity at a time, the backward-looking center, Cb(Un). The backward center is a confirmation of an entity that has already been introduced into the discourse; more specifically, it must be realized in the immediately preceding utterance, Un_.1. There are several distinct types of transitions from one utterance to the next. The typology of transitions is based on two factors: whether or not the center of attention, Cb, is the same from Un_1 to Un, and whether or not this entity coincides with the preferred center of Un. Definitions of these transition types appear in figure 1. These transitions describe how utterances are linked together in a coherent local segment of discourse. If a speaker has a number of propositions to express, one very simple way to do this coherently is to express all the propositions about a given entity (continuing) before introducing a related entity As is evident in constraint 3, ranking of the items on the forward center list, Cf, is crucial. We rank the items in Cf by obliqueness of grammatical relation of the subcategorized functions of the main verb: that is, first the subject, object, and object2, followed by other subcategorized functions, and finally, adjuncts. This captures the idea in [GJW86] that subjecthood contributes strongly to the priority of an item on the Cf list. (retaining) and then shifting the center to this new entity. See figure 2. Retaining may be a way to signal an intention to shift. While we do not claim that speakers really behave in such an orderly fashion, an algorithm that expects this kind of behavior is more successful than those which depend solely on recency or parallelism of grammatical function. The interaction of centering with global focusing mechanisms and with other factors such as intentional structure, semantic selectional restrictions, verb tense and aspect, modality, intonation and pitch accent are topics for further research. Note that these transitions are more specific than focus movement as described in [Sid83]. The extension we propose makes them more specific still. Note also that the Cb of [GJW86] corresponds roughly to Sidner's discourse focus and the Cf to her potential foci. The formal system of constraints and rules for centering, as we have interpreted them from [GJW86], are as follows. For each Un in U1, , CONTINUING... Un+i: Carl works at HP on the Natural Language We are aware that this ranking usually coincides with surface constituent order in English. It would be of interest to examine data from languages with relatively freer constituent order (e.g. German) to determine the influence of constituent order upon centering when the grammatical functions are held constant. In addition, languages that provide an identifiable topic function (e.g. Japanese) suggest that topic takes precedence over subject. The part of the HPSG system that uses the centering algorithm for pronoun binding is called the pragmatics processor. It interacts with another module called the semantics processor, which computes representations of intrasentential anaphoric relations, (among other things). The semantics processor has access to information such as the surface syntactic structure of the utterance. It provides the pragmatics processor with representations which include of a set of reference markers. Each reference marker is contraindexed2 with expressions with which it cannot co-specify3. Reference markers also carry information about agreement and grammatical function. Each pronominal reference marker has a unique index from A1, , An and is displayed in the figures in the form [POLLARD:Al], where POLLARD is the semantic representation of the co-specifier. For non-pronominal reference markers the surface string is used as the index. Indices for indefinites are generated from X1, .. \u2022 ", "conclusion": "", "summary_sents": ["In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns.", "As described in [GJW86], the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing, retaining and shifting.", "We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns.", "The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application.", "Our centering algorithm extends the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift.", "The most common classification of transitional states are predicted to be less and less coherent in the order of CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT.", "The measure M.BFP uses a lexicographic ordering on 4-tuples to determine the transition state.", "Hard-core centering approaches only deal with the last sentence."]}
{"title": "A Statistical Approach To Machine Translation", "abstract": "this paper, we present a statistical to machine translation. We describe the application of our approach to translation from French to English and give preliminary results. ", "introduction": "", "conclusion": "", "summary_sents": ["In this paper, we present a statistical approach to machine translation.", "We describe the application of our approach to translation from French to English and give preliminary results.", "We estimate parameters for a model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text."]}
{"title": "A General Framework For Distributional Similarity", "abstract": "We present a general framework for distributional similarity based on the concepts of precision and recall. Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored. We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns. ", "introduction": "There are many potential applications of sets of distributionally similar words. In the syntactic domain, language models, which can be used to evaluate alternative interpretations of text and speech, require probabilistic information about words and their co-occurrences which is often not available due to the sparse data problem. In order to overcome this problem, researchers (e.g. Pereira et al. (1993)) have proposed estimating probabilities based on sets of words which are known to be distributionally similar. In the semantic domain, the hypothesis that words which mean similar things behave in similar ways (Levin, 1993), has led researchers (e.g. Lin (1998)) to propose that distributional similarity might be used as a predictor of semantic similarity. Accordingly, we might automatically build thesauruses which could be used in tasks such as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002). However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard's Coefficient (Frakes and Baeza-Yates, 1992), the Dice Coefficient (Frakes and Baeza-Yates, 1992), the KullbackLeibler Divergence (Cover and Thomas, 1991), the Jenson-Shannon Divergence (Rao, 1983), the a-skew Divergence (Lee, 1999), the Confusion Probability (Essen and Steinbiss, 1992), Hindle's Mutual Information(MI)-Based Measure (Hindle, 1990) and Lin's MI-Based Measure (Lin, 1998). Further, there is no clear way of deciding which is the best measure. Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words. We take a generic approach that does not directly reduce distributional similarity to a single dimension. The way dimensions are combined together will depend on parameters tuned to the demands of a given application. Further, different parameter settings will approximate different existing similarity measures as well as many more which have, until now, been unexplored. The contributions of this paper are four-fold. First, we propose a general framework for distributional similarity based on the concepts of precision and recall (Section 2). Second, we evaluate the framework at its optimal parameter settings for two different applications (Section 3), showing that it outperforms existing state-ofthe-art similarity measures for both high and low frequency nouns. Third, we begin to investigate to what extent existing similarity measures might be characterised in terms of parameter settings within the framework (Section 4). Fourth, we provide an understanding of why a single existing measure cannot achieve optimal results in every application of distributional similarity measures. ", "conclusion": "Using the MI-based model for precision and recall and with a parameter setting of -y = 1.0, the general framework for distributional similarity proposed herein closely approximates Lin's (1998) Measure. However, we have shown that using a much lower value of -y so that the combination of precision and recall is closer to a weighted arithmetic mean than a harmonic mean yields better results in the two application tasks considered here. This is because the relative importance of precision and recall can be tuned to the task at hand. Further, we have shown that pseudodisambiguation is a task which requires high precision neighbours whereas WordNet prediction is a task which requires high recall neighbours. Accordingly, it is not clear how a single (unparameterised) similarity measure could give optimum results on both tasks. In the future, we intend to extend the work to the characterisation of other tasks and other existing similarity measures. As well as their, usually implicit, use of precision and recall, the main difference between existing similarity measures will be the models in which precision and recall are defined. We have explored two such models here - a combinatorial model and a MIbased model - and have shown that the MIbased model achieves significantly improved results over the combinatorial model. We propose to investigate other models such as the probabilistic one given in Section 2.3. ", "summary_sents": ["We present a general framework for distributional similarity based on the concepts of precision and recall.", "Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored.", "We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.", "We propose a general framework for distributional similarity that consists of notions of precision and recall."]}
{"title": "Automatic sense prediction for implicit discourse relations in text", "abstract": "We present a series of experiments on auidentifying the sense of imrelations, i.e. relations that are not marked with a discourse connective such as \u201cbut\u201d or \u201cbecause\u201d. We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses. We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications. Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency. ", "introduction": "Implicit discourse relations abound in text and readers easily recover the sense of such relations during semantic interpretation. But automatic sense prediction for implicit relations is an outstanding challenge in discourse processing. Discourse relations, such as causal and contrast relations, are often marked by explicit discourse connectives (also called cue words) such as \u201cbecause\u201d or \u201cbut\u201d. It is not uncommon, though, for a discourse relation to hold between two text spans without an explicit discourse connective, as the example below demonstrates: (1) The 101-year-old magazine has never had to woo advertisers with quite so much fervor before. [because] It largely rested on its hard-to-fault demographics. In this paper we address the problem of automatic sense prediction for discourse relations in newspaper text. For our experiments, we use the Penn Discourse Treebank, the largest existing corpus of discourse annotations for both implicit and explicit relations. Our work is also informed by the long tradition of data intensive methods that rely on huge amounts of unannotated text rather than on manually tagged corpora (Marcu and Echihabi, 2001; Blair-Goldensohn et al., 2007). In our analysis, we focus only on implicit discourse relations and clearly separate these from explicits. Explicit relations are easy to identify. The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008). So reporting results on explicit and implicit relations separately will allow for clearer tracking of progress. In this paper we investigate the effectiveness of various features designed to capture lexical and semantic regularities for identifying the sense of implicit relations. Given two text spans, previous work has used the cross-product of the words in the spans as features. We examine the most informative word pair features and find that they are not the semantically-related pairs that researchers had hoped. We then introduce several other methods capturing the semantics of the spans (polarity features, semantic classes, tense, etc.) and evaluate their effectiveness. This is the first study which reports results on classifying naturally occurring implicit relations in text and uses the natural distribution of the various senses. ", "conclusion": "We have presented the first study that predicts implicit discourse relations in a realistic setting (distinguishing a relation of interest from all others, where the relations occur in their natural distributions). Also unlike prior work, we separate the task from the easier task of explicit discourse prediction. Our experiments demonstrate that features developed to capture word polarity, verb classes and orientation, as well as some lexical features are strong indicators of the type of discourse relation. We analyze word pair features used in prior work that were intended to capture such semantic oppositions. We show that the features in fact do not capture semantic relation but rather give information about function word co-occurrences. However, they are still a useful source of information for discourse relation prediction. The most beneficial application of such features is when they are selected from a large unannotated corpus of explicit relations, but then trained on manually annotated implicit relations. Context, in terms of paragraph boundaries and nearby explicit relations, also proves to be useful for the prediction of implicit discourse relations. It is helpful when added as a feature in a standard, instance-by-instance learning model. A sequence model also leads to over 1% absolute improvement for the task. ", "summary_sents": ["We present a series of experiments on automatically identifying the sense of implicit discourse relations, i.e. relations that are not marked with a discourse connective such as \u201cbut\u201d or \u201cbecause\u201d.", "We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses.", "We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.", "In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.", "Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency.", "Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a).", "Our analysis shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined."]}
{"title": "Intelligent Selection of Language Model Training Data", "abstract": "We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods. ", "introduction": "Statistical N-gram language models are widely used in applications that produce natural-language text as output, particularly speech recognition and machine translation. It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output. This presents a problem, because in virtually any particular application the amount of in-domain data is limited. Thus it has become standard practice to combine in-domain data with other data, either by combining N-gram counts from in-domain and other data (usually weighting the counts in some way), or building separate language models from different data sources, interpolating the language model probabilities either linearly or log-linearly. Log-linear interpolation is particularly popular in statistical machine translation (e.g., Brants et al., 2007), because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as BLEU) by making the log probability according to each language model a separate feature function in the overall translation model. The normal practice when using multiple languages models in machine translation seems to be to train models on as much data as feasible from each source, and to depend on feature weight optimization to down-weight the impact of data that is less well-matched to the translation application. In this paper, however, we show that for a data source that is not entirely in-domain, we can improve the match between the language model from that data source and the desired application output by intelligently selecting a subset of the available data as language model training data. This not only produces a language model better matched to the domain of interest (as measured in terms of perplexity on held-out in-domain data), but it reduces the computational resources needed to exploit a large amount of non-domain-specific data, since the resources needed to filter a large amount of data are much less (especially in terms of memory) than those required to build a language model from all the data. ", "conclusion": "The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested. This study is preliminary, however, in that we have not yet shown improved end-to-end task performance applying this approach, such as improved BLEU scores in a machine translation task. However, we believe there is reason to be optimistic about this. When a language model trained on non-domain-specific data is used in a statistical translation model as a separate feature function (as is often the case), lower perplexity on indomain target language test data derived from reference translations corresponds directly to assigning higher language model feature scores to those reference translations, which should in turn lead to translation system output that matches reference translations better. ", "summary_sents": ["We address the problem of selecting non-domain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.", "Our approach is based on comparing the cross-entropy, according to domain-specific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.", "We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.", "In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy. This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words)."]}
{"title": "Sentence Fusion For Multidocument News Summarization", "abstract": "A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources. ", "introduction": "", "conclusion": "", "summary_sents": ["A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading.", "In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents.", "Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence.", "Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources.", "We represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output.", "We introduce the problem of converting multiple sentences into a single summary sentence."]}
{"title": "An Empirical Model Of Multiword Expression Decomposability", "abstract": "This paper presents a constructioninspecific model of multiword expression decomposability based on latent semantic analysis. We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability. We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet. Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet. ", "introduction": "This paper is concerned with an empirical model of multiword expression decomposability. Multiword expressions (MWEs) are defined to be cohesive lexemes that cross word boundaries (Sag et al., 2002; Copestake et al., 2002; Calzolari et al., 2002). They occur in a wide variety of syntactic configurations in different languages (e.g. in the case of English, compound nouns: post office, verbal idioms: pull strings, verb-particle constructions: push on, etc.). Decomposability is a description of the degree to which the semantics of an MWE can be ascribed to those of its parts (Riehemann, 2001; Sag et al., 2002). Analysis of the semantic correlation between the constituent parts and whole of an MWE is perhaps more commonly discussed under the banner of compositionality (Nunberg et al., 1994; Lin, 1999). Our claim here is that the semantics of the MWE are deconstructed and the parts coerced into often idiosyncratic interpretations to attain semantic alignment, rather than the other way around. One idiom which illustrates this process is spill the beans, where the semantics of reveal'(secret') are decomposed such that spill is coerced into the idiosyncratic interpretation of reveal' and beans into the idiosyncratic interpretation of secret'. Given that these senses for spill and beans are not readily available at the simplex level other than in the context of this particular MWE, it seems fallacious to talk about them composing together to form the semantics of the idiom. Ideally, we would like to be able to differentiate between three classes of MWEs: nondecomposable, idiosyncratically decomposable and simple decomposable (derived from Nunberg et al.\u2019s sub-classification of idioms (1994)). With nondecomposable MWEs (e.g. kick the bucket, shoot the breeze, hot dog), no decompositional analysis is possible, and the MWE is semantically impenetrable. The only syntactic variation that non-decomposable MWEs undergo is verbal inflection (e.g. kicked the bucket, kicks the bucket) and pronominal reflexivisation (e.g. wet oneself, wet themselves). Idiosyncratically decomposable MWEs (e.g. spill the beans, let the cat out of the bag, radar footprint) are decomposable but coerce their parts into taking semantics unavailable outside the MWE. They undergo a certain degree of syntactic variation (e.g. the cat was let out of the bag). Finally, simple decomposable MWEs (also known as \u201cinstitutionalised\u201d MWEs, e.g. kindle excitement, traffic light) decompose into simplex senses and generally display high syntactic variability. What makes simple decomposable expressions true MWEs rather than productive word combinations is that they tend to block compositional alternates with the expected semantics (termed anticollocations by Pearce (2001b)). For example, motor car cannot be rephrased as *engine car or *motor automobile. Note that the existence of anticollocations is also a test for non-decomposable and idiosyncratically decomposable MWEs (e.g. hot dog vs. #warm dog or #hot canine). Our particular interest in decomposability stems from ongoing work on grammatical means for capturing MWEs. Nunberg et al. (1994) observed that idiosyncratically decomposable MWEs (in particular idioms) undergo much greater syntactic variation than non-decomposable MWEs, and that the variability can be partially predicted from the decompositional analysis. We thus aim to capture the decomposability of MWEs in the grammar and use this to constrain the syntax of MWEs in parsing and generation. Note that it is arguable whether simple decomposable MWEs belong in the grammar proper, or should be described instead as lexical affinities between particular word combinations. As the first step down the path toward an empirical model of decomposability, we focus on demarcating simple decomposable MWEs from idiosyncratically decomposable and non-decomposable MWEs. This is largely equivalent to classifying MWEs as being endocentric (i.e., a hyponym of their head) or ezocentric (i.e., not a hyponym of their head: Haspelmath (2002)). We attempt to achieve this by looking at the semantic similarity between an MWE and its constituent words, and hypothesising that where the similarity between the constituents of an MWE and the whole is sufficiently high, the MWE must be of simple decomposable type. The particular similarity method we adopt is latent semantic analysis, or LSA (Deerwester et al., 1990). LSA allows us to calculate the similarity between an arbitrary word pair, offering the advantage of being able to measure the similarity between the MWE and each of its constituent words. For MWEs such as house boat, therefore, we can expect to capture the fact that the MWE is highly similar in meaning to both constituent words (i.e. the modifier house and head noun boat). More importantly, LSA makes no assumptions about the lexical or syntactic composition of the inputs, and thus constitutes a fully construction- and language-inspecific method of modelling decomposability. This has clear advantages over a more conventional supervised classifierstyle approach, where training data would have to be customised to a particular language and construction type. Evaluation is inevitably a difficulty when it comes to the analysis of MWEs, due to the lack of concise consistency checks on what MWEs should and should not be incorporated into dictionaries. While recognising the dangers associated with dictionarybased evaluation, we commit ourselves to this paradigm and focus on searching for appropriate means of demonstrating the correlation between dictionary- and corpus-based similarities. The remainder of this paper is structured as follows. Section 2 describes past research on MWE compositionality of relevance to this effort. Section 3 provides a basic outline of the resources used in this research, LSA, the MWE extraction methods, and measures used to evaluate our method. Section 4 then provides evaluation of the proposed method, and the paper is concluded with a brief discussion in Section 5. ", "conclusion": "", "summary_sents": ["This paper presents a construction-inspecific model of multiword expression decomposability based on latent semantic analysis.", "We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability.", "We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet.", "Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet.", "we studied vector extraction for phrases because they were interested in the decomposability of multi word expressions.", "we propose a LSA-based model for measuring the decomposability of MWEs by examining the similarity between them and their constituent words, with higher similarity indicating the greater decomposability."]}
{"title": "Representing Text Chunks", "abstract": "Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval. (Ramshaw and Marcus, 1995) have introduced a &quot;convenient&quot; data representation for chunking by converting it to a tagging task. In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks. We will show that the the data representation choice has a minor influence on chunking performance. However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set. ", "introduction": "The text corpus tasks parsing, information extraction and information retrieval can benefit from dividing sentences in chunks of words. (Ramshaw and Marcus, 1995) describe an error-driven transformation-based learning (TBL) method for finding NP chunks in texts. NP chunks (or baseNPs) are non-overlapping, non-recursive noun phrases. In their experiments they have modeled chunk recognition as a tagging task: words that are inside a baseNP were marked I, words outside a baseNP received an 0 tag and a special tag B was used for the first word inside a baseNP immediately following another baseNP. A text example: original: In [N early trading NI in [N Hong Kong N] [N Monday NI , [N gold N] was quoted at [N $ 366.50 NI [N an ounce N] \u2022 tagged: Other representations for NP chunking can be used as well. An example is the representation used in (Ratnaparkhi, 1998) where all the chunkinitial words receive the same start tag (analogous to the B tag) while the remainder of the words in the chunk are paired with a different tag. This removes tagging ambiguities. In the Ratnaparkhi representation equal noun phrases receive the same tag sequence regardless of the context in which they appear. The data representation choice might influence the performance of chunking systems. In this paper we discuss how large this influence is. Therefore we will compare seven different data representation formats for the baseNP recognition task. We are particularly interested in finding out whether with one of the representation formats the best reported results for this task can be improved. The second section of this paper presents the general setup of the experiments. The results ean be found in the third section. In the fourth section we will describe some related work. ", "conclusion": "", "summary_sents": ["Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval.", "(Ramshaw and Marcus, 1995) have introduced a \"convenient\" data representation for chunking by converting it to a tagging task.", "In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks.", "We will show that the data representation choice has a minor influence on chunking performance.", "However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set.", "We describe in detail the IOB schemes."]}
{"title": "COGEX: A Logic Prover For Question Answering", "abstract": "Recent TREC results have demonstrated the need for deeper text understanding methods. This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a Question Answering system. The approach is to transform questions and answer passages into logic representations. World knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text. Moreover, the trace of the proofs provide answer justifications. The results show that the prover boosts the performance of the QA system on TREC questions by 30%. ", "introduction": "In spite of significant advances made recently in the Question Answering technology, there still remain many problems to be solved. Some of these are: bridging the gap between question and answer words, pinpointing exact answers, taking into consideration syntactic and semantic roles of words, better answer ranking, answer justification, and others. The recent TREC results (Voorhees 2002) have demonstrated that many performing systems reached a plateau; the systems ranked from 4th to 14th answered correctly between 38.4% to 24.8% of the total number of questions. It is clear that new ideas based on a deeper language understanding are necessary to push further the QA technology. In this paper we introduce one such novel idea, the use of automated reasoning in QA, and show that it is feasible, effective, and scalable. We have implemented a Logic Prover, called COGEX (from the permutation of the first two syllables of the verb excogitate) which uniformly codifies the question and answer text, as well as world knowledge resources, in order to use its inference engine to verify and extract any lexical relationships between the question and its candidate answers. COGEX captures the syntax-based relationships such as the syntactic objects, syntactic subjects, prepositional attachments, complex nominals, and adverbial/adjectival adjuncts provided by the logic representation of text. In addition to the logic representations of questions and candidate answers, the QA Logic Prover needs world knowledge axioms to link question concepts to answer concepts. These axioms are provided by the WordNet glosses represented in logic forms. Additionally, the prover needs rewriting procedures for semantically equivalent lexical patterns. With this deep and intelligent representation, COGEX effectively and efficiently re-ranks candidate answers by their correctness, extracts the exact answer, and ultimately eliminates incorrect answers. In this way, the Logic Prover is a powerful tool in boosting the accuracy of the QA system. Moreover, the trace of a proof constitutes a justification for that answer. The challenges one faces when using automated reasoning in the context of NLP include: logic representation of open text, need of world knowledge axioms, logic representation of semantically equivalent linguistic patterns, and others. Logic proofs are accurate but costly, both in terms of high failure rate due to insufficient input axioms, as well as long processing time. Our solution is to integrate the prover into the QA system and rely on reasoning methods only to augment other previously implemented answer extraction techniques. ", "conclusion": "", "summary_sents": ["Recent TREC results have demonstrated the need for deeper text understanding methods.", "This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a Question Answering system.", "The approach is to transform questions and answer passages into logic representations.", "World knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text.", "Moreover, the trace of the proofs provide answer justifications.", "The results show that the prover boosts the performance of the QA system on TREC questions by 30%.", "COGEX uses its logic prover to extract lexical relationships between the question and its candidate answers."]}
{"title": "Parameter Estimation For Probabilistic Finite-State Transducers", "abstract": "algebraic path problem (shortest paths; matrix inver- 34(3):191\u2013219. Richard Sproat and Michael Riley. 1996. Compilation of weighted finite-state transducers from decision trees. of the 34th Annual Meeting of the Andreas Stolcke and Stephen M. Omohundro. 1994. Best-first model merging for hidden Markov model induction. Tech. Report ICSI TR-94-003, Berkeley, CA. Robert Endre Tarjan. 1981a. A unified approach to path of the 28(3):577\u2013593, July. Robert Endre Tarjan. 1981b. Fast algorithms for solving problems. of the 28(3):594\u2013614, July. G. van Noord and D. Gerdemann. 2001. An extendible regular expression compiler for finite-state approaches natural language processing. In Impleno. 22 in Springer Lecture Notes in CS. ", "introduction": "", "conclusion": "", "summary_sents": ["Weighted finite-state transducers suffer from the lack of a training algorithm.", "Training is even harder for transducers that have been assembled via finite-state operations such as composition, minimization, union, concatenation, and closure, as this yields tricky parameter tying.", "We formulate a \u201cparameterized FST\u201d paradigm and give training algorithms for it, including a general bookkeeping trick (\u201cexpectation semirings\u201d) that cleanly and efficiently computes expectations and gradients.", "We use finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm.", "We claim that parsing under an expectation semiring is equivalent to the Inside-Outside algorithm for PCFGs.", "We give a general EM algorithm for parameter estimation in probabilistic finite-state transducers.", "We describe the expectation semiring for parameter learning."]}
{"title": "Bayesian Inference for PCFGs via Markov Chain Monte Carlo", "abstract": "This paper presents two Markov chain Monte Carlo (MCMC) algorithms forBayesian inference of probabilistic context free grammars (PCFGs) from ter minal strings, providing an alternative to maximum-likelihood estimation usingthe Inside-Outside algorithm. We illus trate these methods by estimating a sparse grammar describing the morphology ofthe Bantu language Sesotho, demonstrat ing that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihoodmethods such as the Inside-Outside algo rithm only produce a trivial grammar. ", "introduction": "The standard methods for inferring the parameters of probabilistic models in computational linguistics arebased on the principle of maximum-likelihood esti mation; for example, the parameters of ProbabilisticContext-Free Grammars (PCFGs) are typically estimated from strings of terminals using the InsideOutside (IO) algorithm, an instance of the Ex pectation Maximization (EM) procedure (Lari andYoung, 1990). However, much recent work in ma chine learning and statistics has turned away from maximum-likelihood in favor of Bayesian methods, and there is increasing interest in Bayesian methods in computational linguistics as well (Finkel et al, 2006). This paper presents two Markov chain Monte Carlo (MCMC) algorithms for inferring PCFGs and their parses from strings alone. These can be viewed as Bayesian alternatives to the IO algorithm. The goal of Bayesian inference is to compute a distribution over plausible parameter values. This ?posterior? distribution is obtained by combining thelikelihood with a ?prior? distribution P(?) over pa rameter values ?. In the case of PCFG inference ? is the vector of rule probabilities, and the prior mightassert a preference for a sparse grammar (see be low). The posterior probability of each value of ? is given by Bayes? rule: P(?|D) ? P(D|?)P(?). (1)In principle Equation 1 defines the posterior prob ability of any value of ?, but computing this may not be tractable analytically or numerically. For this reason a variety of methods have been developed to support approximate Bayesian inference. One of the most popular methods is Markov chain Monte Carlo(MCMC), in which a Markov chain is used to sam ple from the posterior distribution. This paper presents two new MCMC algorithms for inferring the posterior distribution over parses and rule probabilities given a corpus of strings. The first algorithm is a component-wise Gibbs samplerwhich is very similar in spirit to the EM algorithm, drawing parse trees conditioned on the current parameter values and then sampling the param eters conditioned on the current set of parse trees. The second algorithm is a component-wise Hastingssampler that ?collapses? the probabilistic model, in tegrating over the rule probabilities of the PCFG,with the goal of speeding convergence. Both algo 139rithms use an efficient dynamic programming tech nique to sample parse trees. Given their usefulness in other disciplines, we believe that Bayesian methods like these are likelyto be of general utility in computational linguis tics as well. As a simple illustrative example, we use these methods to infer morphological parses for verbs from Sesotho, a southern Bantu language with agglutinating morphology. Our results illustrate that Bayesian inference using a prior that favors sparsitycan produce linguistically reasonable analyses in sit uations in which EM does not. The rest of this paper is structured as follows. The next section introduces the background for our paper, summarizing the key ideas behind PCFGs,Bayesian inference, and MCMC. Section 3 intro duces our first MCMC algorithm, a Gibbs sampler for PCFGs. Section 4 describes an algorithm forsampling trees from the distribution over trees de fined by a PCFG. Section 5 shows how to integrateout the rule weight parameters ? in a PCFG, allowing us to sample directly from the posterior distribution over parses for a corpus of strings. Finally, Sec tion 6 illustrates these methods in learning Sesotho morphology. ", "conclusion": "This paper has described basic algorithms for performing Bayesian inference over PCFGs given ter minal strings. We presented two Markov chain Monte Carlo algorithms (a Gibbs and a Hastings sampling algorithm) for sampling from the posterior distribution over parse trees given a corpus of theiryields and a Dirichlet product prior over the production probabilities. As a component of these algorithms we described an efficient dynamic program ming algorithm for sampling trees from a PCFG which is useful in its own right. We used thesesampling algorithms to infer morphological analy ses of Sesotho verbs given their strings (a task on which the standard Maximum Likelihood estimatorreturns a trivial and linguistically uninteresting so lution), achieving 0.75 unlabeled morpheme f-score and 0.54 exact word match accuracy. Thus this is one of the few cases we are aware of in which a PCFG estimation procedure returns linguistically meaningful structure. We attribute this to the ability of the Bayesian prior to prefer sparse grammars.We expect that these algorithms will be of inter est to the computational linguistics community both because a Bayesian approach to PCFG estimation ismore flexible than the Maximum Likelihood meth ods that currently dominate the field (c.f., the use of a prior as a bias towards sparse solutions), and because these techniques provide essential building blocks for more complex models. ", "summary_sents": ["This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm.", "We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.", "We describe Gibbs samplers for Bayesian inference of PCFG rule probabilities.", "We introduce adaptor grammars, a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree."]}
{"title": "Automatic Retrieval and Clustering of Similar Words", "abstract": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is. ", "introduction": "The meaning of an unknown word can often be inferred from its context. Consider the following (slightly modified) example in (Nida, 1975, p.167): Everyone likes tezgiiino. Tezgiiino makes you drunk. We make tezgiiino out of corn. The contexts in which the word tezgiiino is used suggest that tezgiiino may be a kind of alcoholic beverage made from corn mash. Bootstrapping semantics from text is one of the greatest challenges in natural language learning. It has been argued that similarity plays an important role in word acquisition (Gentner, 1982). Identifying similar words is an initial step in learning the definition of a word. This paper presents a method for making this first step. For example, given a corpus that includes the sentences in (1), our goal is to be able to infer that tezgiiino is similar to &quot;beer&quot;, &quot;wine&quot;, &quot;vodka&quot;, etc. In addition to the long-term goal of bootstrapping semantics from text, automatic identification of similar words has many immediate applications. The most obvious one is thesaurus construction. An automatically created thesaurus offers many advantages over manually constructed thesauri. Firstly, the terms can be corpus- or genre-specific. Manually constructed general-purpose dictionaries and thesauri include many usages that are very infrequent in a particular corpus or genre of documents. For example, one of the 8 senses of &quot;company&quot; in WordNet 1.5 is a &quot;visitor/visitant&quot;, which is a hyponym of &quot;person&quot;. This usage of the word is practically never used in newspaper articles. However, its existance may prevent a co-reference recognizer to rule out the possiblity for personal pronouns to refer to &quot;company&quot;. Secondly, certain word usages may be particular to a period of time, which are unlikely to be captured by manually compiled lexicons. For example, among 274 occurrences of the word &quot;westerner&quot; in a 45 million word San Jose Mercury corpus, 55% of them refer to hostages. If one needs to search hostage-related articles, &quot;westerner&quot; may well be a good search term. Another application of automatically extracted similar words is to help solve the problem of data sparseness in statistical natural language processing (Dagan et al., 1994; Essen and Steinbiss, 1992). When the frequency of a word does not warrant reliable maximum likelihood estimation, its probability can be computed as a weighted sum of the probabilities of words that are similar to it. It was shown in (Dagan et al., 1997) that a similarity-based smoothing method achieved much better results than backoff smoothing methods in word sense disambiguation. The remainder of the paper is organized as follows. The next section is concerned with similarities between words based on their distributional patterns. The similarity measure can then be used to create a thesaurus. In Section 3, we evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri. Section 4 briefly discuss future work in clustering similar words. Finally, Section 5 reviews related work and summarize our contributions. ", "conclusion": "There have been many approaches to automatic detection of similar words from text corpora. Ours is similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collocations and their associated scores were evaluated indirectly by their use in parse tree selection. The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs. The main contribution of this paper is a new evaluation methodology for automatically constructed thesaurus. While previous methods rely on indirect tasks or subjective judgments, our method allows direct and objective comparison between automatically and manually constructed thesauri. The results show that our automatically created thesaurus is significantly closer to WordNet than Roget Thesaurus is. Our experiments also surpasses previous experiments on automatic thesaurus construction in scale and (possibly) accuracy. ", "summary_sents": ["Bootstrapping semantics from text is one of the greatest challenges in natural language learning.", "We first define a word similarity measure based on the distributional pattern of words.", "The similarity measure allows us to construct a thesaurus using a parsed corpus.", "We then present a new evaluation methodology for the automatically constructed thesaurus.", "The evaluation results show that the thesaurns is significantly closer to WordNet than Roget Thesaurus is.", "We use dependency relation as word features to compute word similarities from large corpora."]}
{"title": "Minimum Cut Model For Spoken Lecture Segmentation", "abstract": "We consider the task of unsupervised lecture segmentation. We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion. Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies. Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors. ", "introduction": "The development of computational models of text structure is a central concern in natural language processing. Text segmentation is an important instance of such work. The task is to partition a text into a linear sequence of topically coherent segments and thereby induce a content structure of the text. The applications of the derived representation are broad, encompassing information retrieval, question-answering and summarization. Not surprisingly, text segmentation has been extensively investigated over the last decade. Following the first unsupervised segmentation approach by Hearst (1994), most algorithms assume that variations in lexical distribution indicate topic changes. When documents exhibit sharp variations in lexical distribution, these algorithms are likely to detect segment boundaries accurately. For example, most algorithms achieve high performance on synthetic collections, generated by concatenation of random text blocks (Choi, 2000). The difficulty arises, however, when transitions between topics are smooth and distributional variations are subtle. This is evident in the performance of existing unsupervised algorithms on less structured datasets, such as spoken meeting transcripts (Galley et al., 2003). Therefore, a more refined analysis of lexical distribution is needed. Our work addresses this challenge by casting text segmentation in a graph-theoretic framework. We abstract a text into a weighted undirected graph, where the nodes of the graph correspond to sentences and edge weights represent the pairwise sentence similarity. In this framework, text segmentation corresponds to a graph partitioning that optimizes the normalized-cut criterion (Shi and Malik, 2000). This criterion measures both the similarity within each partition and the dissimilarity across different partitions. Thus, our approach moves beyond localized comparisons and takes into account long-range changes in lexical distribution. Our key hypothesis is that global analysis yields more accurate segmentation results than local models. We tested our algorithm on a corpus of spoken lectures. Segmentation in this domain is challenging in several respects. Being less structured than written text, lecture material exhibits digressions, disfluencies, and other artifacts of spontaneous communication. In addition, the output of speech recognizers is fraught with high word error rates due to specialized technical vocabulary and lack of in-domain spoken data for training. Finally, pedagogical considerations call for fluent transitions between different topics in a lecture, further complicating the segmentation task. Our experimental results confirm our hypothesis: considering long-distance lexical dependencies yields substantial gains in segmentation performance. Our graph-theoretic approach compares favorably to state-of-the-art segmentation algorithms and attains results close to the range of human agreement scores. Another attractive property of the algorithm is its robustness to noise: the accuracy of our algorithm does not deteriorate significantly when applied to speech recognition output. ", "conclusion": "In this paper we studied the impact of long-range dependencies on the accuracy of text segmentation. We modeled text segmentation as a graphpartitioning task aiming to simultaneously optimize the total similarity within each segment and dissimilarity across various segments. We showed that global analysis of lexical distribution improves the segmentation accuracy and is robust in the presence of recognition errors. Combining global analysis with advanced methods for smoothing (Ji and Zha, 2003) and weighting could further boost the segmentation performance. Our current implementation does not automatically determine the granularity of a resulting segmentation. This issue has been explored in the past (Ji and Zha, 2003; Utiyama and Isahara, 2001), and we will explore the existing strategies in our framework. We believe that the algorithm has to produce segmentations for various levels of granularity, depending on the needs of the application that employs it. Our ultimate goal is to automatically generate tables of content for lectures. We plan to investigate strategies for generating titles that will succinctly describe the content of each segment. We will explore how the interaction between the generation and segmentation components can improve the performance of such a system as a whole. ", "summary_sents": ["We consider the task of unsupervised lecture segmentation.", "We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion.", "Our approach moves beyond localized comparisons and takes into account long-range cohesion dependencies.", "Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.", "We optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences.", "Our problem is to find topical boundaries in transcripts of course lectures.", "We create a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity."]}
{"title": "Tuning as Ranking", "abstract": "We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO\u2019s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios. ", "introduction": "The MERT algorithm (Och, 2003) is currently the most popular way to tune the parameters of a statistical machine translation (MT) system. MERT is well-understood, easy to implement, and runs quickly, but can behave erratically and does not scale beyond a handful of features. This lack of scalability is a significant weakness, as it inhibits systems from using more than a couple dozen features to discriminate between candidate translations and stymies feature development innovation. Several researchers have attempted to address this weakness. Recently, Watanabe et al. (2007) and Chiang et al. (2008b) have developed tuning methods using the MIRA algorithm (Crammer and Singer, 2003) as a nucleus. The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009). However, the technique is complex and architecturally quite different from MERT. Tellingly, in the entire proceedings of ACL 2010 (Haji\u02c7c et al., 2010), only one paper describing a statistical MT system cited the use of MIRA for tuning (Chiang, 2010), while 15 used MERT.1 Here we propose a simpler approach to tuning that scales similarly to high-dimensional feature spaces. We cast tuning as a ranking problem (Chen et al., 2009), where the explicit goal is to learn to correctly rank candidate translations. Specifically, we follow the pairwise approach to ranking (Herbrich et al., 1999; Freund et al., 2003; Burges et al., 2005; Cao et al., 2007), in which the ranking problem is reduced to the binary classification task of deciding between candidate translation pairs. Of primary concern to us is the ease of adoption of our proposed technique. Because of this, we adhere as closely as possible to the established MERT architecture and use freely available machine learning software. The end result is a technique that scales and performs just as well as MIRA-based tuning, but which can be implemented in a couple of hours by anyone with an existing MERT implementation. Mindful that many would-be enhancements to the state-of-the-art are false positives that only show improvement in a narrowly defined setting or with limited data, we validate our claims on both syntax and phrase-based systems, using multiple language pairs and large data sets. We describe tuning in abstract and somewhat formal terms in Section 2, describe the MERT algorithm in the context of those terms and illustrate its scalability issues via a synthetic experiment in Section 3, introduce our pairwise ranking optimization method in Section 4, present numerous large-scale MT experiments to validate our claims in Section 5, discuss some related work in Section 6, and conclude in Section 7. ", "conclusion": "We have described a simple technique for tuning an MT system that is on par with the leading techniques, exhibits reliable behavior, scales gracefully to high-dimension feature spaces, and is remarkably easy to implement. We have demonstrated, via a litany of experiments, that our claims are valid and that this technique is widely applicable. It is our hope that the adoption of PRO tuning leads to fewer headaches during tuning and motivates advanced MT feature engineering research. ", "summary_sents": ["We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999).", "Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features.", "Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement.", "It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours.", "We establish PRO\u2019s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.", "PRO casts the problem of tuning as a ranking problem between pairs of translation candidates.", "We optimize ranking in n-best lists, but learn parameters in an online fashion.", "We minimize logistic loss sampled from the merged n-bests, and sentence-BLEU is used for determining ranks."]}
{"title": "Using Syntax to Disambiguate Explicit Discourse Connectives in Text", "abstract": "Discourse connectives are words or such as and contrary explicitly signal the presence of a discourse relation. There are two types of ambiguity that need to be resolved during discourse processing. First, a word can be ambiguous between discourse or non-discourse usage. For be either a temporal discourse connective or a simply a word meaning \u201cformerly\u201d. Secondly, some connectives are ambiguous in terms of the they mark. For example can serve as either a temporal or causal connective. We demonstrate that syntactic features improve performance in both disambiguation tasks. We report state-ofthe-art results for identifying discourse vs. non-discourse usage and human-level performance on sense disambiguation. ", "introduction": "Discourse connectives are often used to explicitly mark the presence of a discourse relation between two textual units. Some connectives are largely unambiguous, such as although and additionally, which are almost always used as discourse connectives and the relations they signal are unambiguously identified as comparison and expansion, respectively. However, not all words and phrases that can serve as discourse connectives have these desirable properties. Some linguistic expressions are ambiguous between DISCOURSE AND NON-DISCOURSE USAGE. Consider for example the following sentences containing and and once. \u2217This work was partially supported by NSF grants IIS0803159, IIS-0705671 and IGERT 0504487. In sentence (1a), and is a discourse connective between the two clauses linked by an elaboration/expansion relation; in sentence (1b), the occurrence of and is non-discourse. Similarly in sentence (2a), once is a discourse connective marking the temporal relation between the clauses \u201cThe asbestos fiber, crocidolite is unusually resilient\u201d and \u201cit enters the lungs\u201d. In contrast, in sentence (2b), once occurs with a non-discourse sense, meaning \u201cformerly\u201d and modifying \u201cused\u201d. The only comprehensive study of discourse vs. non-discourse usage in written text1 was done in the context of developing a complete discourse parser for unrestricted text using surface features (Marcu, 2000). Based on the findings from a corpus study, Marcu\u2019s parser \u201cignored both cue phrases that had a sentential role in a majority of the instances in the corpus and those that were too ambiguous to be explored in the context of a surface-based approach\u201d. The other ambiguity that arises during discourse processing involves DISCOURSE RELATION SENSE. The discourse connective since for 1The discourse vs. non-discourse usage ambiguity is even more problematic in spoken dialogues because there the number of potential discourse markers is greater than that in written text, including common words such as now, well and okay. Prosodic and acoustic features are the most powerful indicators of discourse vs. non-discourse usage in that genre (Hirschberg and Litman, 1993; Gravano et al., 2007) instance can signal either a temporal or a causal relation as shown in the following examples from Miltsakaki et al. (2005): (3a) There have been more than 100 mergers and acquisitions within the European paper industry since the most recent wave of friendly takeovers was completed in the U.S. in 1986. (3b) It was a far safer deal for lenders since NWA had a healthier cash flow and more collateral on hand. Most prior work on relation sense identification reports results obtained on data consisting of both explicit and implicit relations (Wellner et al., 2006; Soricut and Marcu, 2003). Implicit relations are those inferred by the reader in the absence of a discourse connective and so are hard to identify automatically. Explicit relations are much easier (Pitler et al., 2008). In this paper, we explore the predictive power of syntactic features for both the discourse vs. nondiscourse usage (Section 3) and discourse relation sense (Section 4) prediction tasks for explicit connectives in written text. For both tasks we report high classification accuracies close to 95%. 2 Corpus and features In our work we use the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the largest public resource containing discourse annotations. The corpus contains annotations of 18,459 instances of 100 explicit discourse connectives. Each discourse connective is assigned a sense from a threelevel hierarchy of senses. In our experiments we consider only the top level categories: Expansion (one clause is elaborating information in the other), Comparison (information in the two clauses is compared or contrasted), Contingency (one clause expresses the cause of the other), and Temporal (information in two clauses are related because of their timing). These top-level discourse relation senses are general enough to be annotated with high inter-annotator agreement and are common to most theories of discourse. Syntactic features have been extensively used for tasks such as argument identification: dividing sentences into elementary discourse units among which discourse relations hold (Soricut and Marcu, 2003; Wellner and Pustejovsky, 2007; Fisher and Roark, 2007; Elwell and Baldridge, 2008). Syntax has not been used for discourse vs. non-discourse disambiguation, but it is clear from the examples above that discourse connectives appear in specific syntactic contexts. The syntactic features we used were extracted from the gold standard Penn Treebank (Marcus et al., 1994) parses of the PDTB articles: Self Category The highest node in the tree which dominates the words in the connective but nothing else. For single word connectives, this might correspond to the POS tag of the word, however for multi-word connectives it will not. For example, the cue phrase in addition is parsed as (PP (IN In) (NP (NN addition) )). While the POS tags of \u201cin\u201d and \u201caddition\u201d are preposition and noun, respectively, together the Self Category of the phrase is prepositional phrase. Parent Category The category of the immediate parent of the Self Category. This feature is especially helpful for disambiguating cases similar to example (1b) above in which the parent of and would be an NP (the noun phrase \u201cblue and green\u201d), which will rarely be the case when and has a discourse function. Left Sibling Category The syntactic category of the sibling immediately to the left of the Self Category. If the left sibling does not exist, this features takes the value \u201cNONE\u201d. Note that having no left sibling implies that the connective is the first substring inside its Parent Category. In example (1a), this feature would be \u201cNONE\u201d, while in example (1b), the left sibling of and is \u201cNP\u201d. Right Sibling Category The syntactic category of the sibling immediately to the right of the Self Category. English is a right-branching language, and so dependents tend to occur after their heads. Thus, the right sibling is particularly important as it is often the dependent of the potential discourse connective under investigation. If the connective string has a discourse function, then this dependent will often be a clause (SBAR). For example, the discourse usage in \u201cAfter I went to the store, I went home\u201d can be distinguished from the nondiscourse usage in \u201cAfter May, I will go on vacation\u201d based on the categories of their right siblings. Just knowing the syntactic category of the right sibling is sometimes not enough; experiments on the development set showed improvements by including more features about the right sibling. Consider the example below: and where. The syntactic category of \u201cwhere\u201d is SBAR, so the set of features above could not distinguish the single word \u201cwhere\u201d from a full embedded clause like \u201cI went to the store\u201d. In order to address this deficiency, we include two additional features about the contents of the right sibling, Right Sibling Contains a VP and Right Sibling Contains a Trace. ", "conclusion": "We have shown that using a few syntactic features leads to state-of-the-art accuracy for discourse vs. non-discourse usage classification. Including syntactic features also helps sense class identification, and we have already attained results at the level of human annotator agreement. These results taken ", "summary_sents": ["Discourse connectives are words or phrases such as once, since, and on the contrary that explicitly signal the presence of a discourse relation.", "There are two types of ambiguity that need to be resolved during discourse processing.", "First, a word can be ambiguous between discourse or non-discourse usage.", "For example, once can be either a temporal discourse connective or a simply a word meaning \u201cformerly\u201d.", "Secondly, some connectives are ambiguous in terms of the relation they mark.", "For example since can serve as either a temporal or causal connective.", "We demonstrate that syntactic features improve performance in both disambiguation tasks.", "We report state-ofthe-art results for identifying discourse vs. non-discourse usage and human-level performance on sense disambiguation.", "We show that where explicit markers exist, the class of the relation can be disambiguated with f-scores higher than 90%. Predicting the class of implicit discourse relations, however, is much more difficult."]}
{"title": "Factored Language Models And Generalized Parallel Backoff", "abstract": "We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc. ), and induces a probability model covering sequences of bundles rather than just words. GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed. These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit. This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles. Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams. In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant. ", "introduction": "The art of statistical language modeling (LM) is to create probability models over words and sentences that tradeoff statistical prediction with parameter variance. The field is both diverse and intricate (Rosenfeld, 2000; Chen and Goodman, 1998; Jelinek, 1997; Ney et al., 1994), with many different forms of LMs including maximumentropy, whole-sentence, adaptive and cache-based, to name a small few. Many models are simply smoothed conditional probability distributions for a word given its preceding history, typically the two preceding words. In this work, we introduce two new methods for language modeling: factored language model (FLM) and generalized parallel backoff (GPB). An FLM considers a word as a bundle of features, and GPB is a technique that generalized backoff to arbitrary conditional probability tables. While these techniques can be considered in isolation, the two methods seem particularly suited to each other \u2014 in particular, the method of GPB can greatly facilitate the production of FLMs with better performance. ", "conclusion": "", "summary_sents": ["We introduce factored language models (FLMs) and generalized parallel backoff (GPB).", "An FLM represents words as bundles of features (e.g. , morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words.", "GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed.", "These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit.", "This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles.", "Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams.", "In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.", "We show that factored language models are able to outperform standard n-gram techniques in terms of perplexity.", "A factored language model (FLM) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework."]}
{"title": "Seeing Stars When There Aren\u2019t Many Stars: Graph-Based Semi-Supervised Learning For Sentiment Categorization", "abstract": "We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., \u201c4 stars\u201d), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve ratinginference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training. ", "introduction": "Sentiment analysis of text documents has received considerable attention recently (Shanahan et al., 2005; Turney, 2002; Dave et al., 2003; Hu and Liu, 2004; Chaovalit and Zhou, 2005). Unlike traditional text categorization based on topics, sentiment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as consumer product or movie reviews. In particular Pang and Lee proposed the rating-inference problem (2005). Rating inference is harder than binary positive / negative opinion classification. The goal is to infer a numerical rating from reviews, for example the number of \u201cstars\u201d that a critic gave to a movie. Pang and Lee showed that supervised machine learning techniques (classification and regression) work well for rating inference with large amounts of training data. However, review documents often do not come with numerical ratings. We call such documents unlabeled data. Standard supervised machine learning algorithms cannot learn from unlabeled data. Assigning labels can be a slow and expensive process because manual inspection and domain expertise are needed. Often only a small portion of the documents can be labeled within resource constraints, so most documents remain unlabeled. Supervised learning algorithms trained on small labeled sets suffer in performance. Can one use the unlabeled reviews to improve rating-inference? Pang and Lee (2005) suggested that doing so should be useful. We demonstrate that the answer is \u2018Yes.\u2019 Our approach is graph-based semi-supervised learning. Semi-supervised learning is an active research area in machine learning. It builds better classifiers or regressors using both labeled and unlabeled data, under appropriate assumptions (Zhu, 2005; Seeger, 2001). This paper contains three contributions: Workshop on TextGraphs, at HLT-NAACL 2006, pages 45\u201352, New York City, June 2006. c\ufffd2006 Association for Computational Linguistics to the sentiment analysis domain, extending past supervised learning work by Pang and Lee (2005); ", "conclusion": "", "summary_sents": ["We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference.", "Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., \u201c4 stars\u201d), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text.", "In particular, we are interested in the situation where labeled data is scarce.", "We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating inference performance.", "We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task.", "We then solve an optimization problem to obtain a smooth rating function over the whole graph.", "When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.", "We adapt semi-supervised graph-based methods for sentiment analysis but do not incorporate lexical prior knowledge in the form of labeled features.", "We propose a semisupervised learning approach to the rating inference problem in scenarios where labeled training data is scarce."]}
{"title": "Shallow Semantic Parsing Using Support Vector Machines", "abstract": "In this paper, we propose a machine learning algorithm for shallow semantic parsing, extend ing the work of Gildea and Jurafsky (2002),Surdeanu et al (2003) and others. Our al gorithm is based on Support Vector Machineswhich we show give an improvement in performance over earlier classifiers. We show perfor mance improvements through a number of newfeatures and measure their ability to general ize to a new test set drawn from the AQUAINT corpus. ", "introduction": "Automatic, accurate and wide-coverage techniques thatcan annotate naturally occurring text with semantic argu ment structure can play a key role in NLP applications such as Information Extraction, Question Answering and Summarization. Shallow semantic parsing ? the process of assigning a simple WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW, etc. structure to sentences in text,is the process of producing such a markup. When pre sented with a sentence, a parser should, for each predicatein the sentence, identify and label the predicate?s seman tic arguments. This process entails identifying groups ofwords in a sentence that represent these semantic argu ments and assigning specific labels to them. In recent work, a number of researchers have cast thisproblem as a tagging problem and have applied vari ous supervised machine learning techniques to it (Gildea and Jurafsky (2000, 2002); Blaheta and Charniak (2000); Gildea and Palmer (2002); Surdeanu et al (2003); Gildea and Hockenmaier (2003); Chen and Rambow (2003); Fleischman and Hovy (2003); Hacioglu and Ward (2003); Thompson et al (2003); Pradhan et al (2003)). In this ?This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grant IS-9978025 paper, we report on a series of experiments exploring this approach. For the initial experiments, we adopted the approachdescribed by Gildea and Jurafsky (2002) (G&J) and evaluated a series of modifications to improve its performance. In the experiments reported here, we first re placed their statistical classification algorithm with one that uses Support Vector Machines and then added to theexisting feature set. We evaluate results using both hand corrected TreeBank syntactic parses, and actual parses from the Charniak parser. ", "conclusion": "We have described an algorithm which significantly im proves the state-of-the-art in shallow semantic parsing. Like previous work, our parser is based on a supervised machine learning approach. Key aspects of our results include significant improvement via an SVM classifier, improvement from new features and a series of analytic experiments on the contributions of the features. Addingfeatures that are generalizations of the more specific features seemed to help. These features were named enti ties, head word part of speech and verb clusters. We also analyzed the transferability of the features to a new text source. We would like to thank Ralph Weischedel and Scott Miller ofBBN Inc. for letting us use their named entity tagger ? Iden tiFinder; Martha Palmer for providing us with the PropBank data, Valerie Krugler for tagging the AQUAINT test set with PropBank arguments, and all the anonymous reviewers for their helpful comments. ", "summary_sents": ["In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others.", "Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers.", "We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus.", "We first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system."]}
{"title": "Learning Surface Text Patterns For A Question Answering System", "abstract": "In this paper we explore the power of surface text patterns for open-domain question answering systems. In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically. A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista. Patterns are then automatically extracted from the returned documents and standardized. We calculate the precision of each pattern, and the average precision for each question type. These patterns are then applied to find answers to new questions. Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web. ", "introduction": "Most of the recent open domain questionanswering systems use external knowledge and tools for answer pinpointing. These may include named entity taggers, WordNet, parsers, hand-tagged corpora, and ontology lists (Srihari and Li, 00; Harabagiu et al., 01; Hovy et al., 01; Prager et al., 01). However, at the recent TREC-10 QA evaluation (Voorhees, 01), the winning system used just one resource: a fairly extensive list of surface patterns (Soubbotin and Soubbotin, 01). The apparent power of such patterns surprised many. We therefore decided to investigate their potential by acquiring patterns automatically and to measure their accuracy. It has been noted in several QA systems that certain types of answer are expressed using characteristic phrases (Lee et al., 01; Wang et al., 01). For example, for BIRTHDATEs (with questions like \u201cWhen was X born?\u201d), typical answers are \u201cMozart was born in 1756.\u201d \u201cGandhi (1869\u20131948)...\u201d These examples suggest that phrases like \u201c<NAME> was born in <BIRTHDATE>\u201d \u201c<NAME> (<BIRTHDATE>\u2013\u201d when formulated as regular expressions, can be used to locate the correct answer. In this paper we present an approach for automatically learning such regular expressions (along with determining their precision) from the web, for given types of questions. Our method uses the machine learning technique of bootstrapping to build a large tagged corpus starting with only a few examples of QA pairs. Similar techniques have been investigated extensively in the field of information extraction (Riloff, 96). These techniques are greatly aided by the fact that there is no need to hand-tag a corpus, while the abundance of data on the web makes it easier to determine reliable statistical estimates. Our system assumes each sentence to be a simple sequence of words and searches for repeated word orderings as evidence for useful answer phrases. We use suffix trees for extracting substrings of optimal length. We borrow the idea of suffix trees from computational biology (Gusfield, 97) where it is primarily used for detecting DNA sequences. Suffix trees can be processed in time linear on the size of the corpus and, more importantly, they do not restrict the length of substrings. We then test the patterns learned by our system on new unseen questions from the TREC-10 set and evaluate their results to determine the precision of the patterns. ", "conclusion": "The web results easily outperform the TREC results. This suggests that there is a need to integrate the outputs of the Web and the TREC corpus. Since the output from the Web contains many correct answers among the top ones, a simple word count could help in eliminating many unlikely answers. This would work well for question types like BIRTHDATE or LOCATION but is not clear for question types like DEFINITION. The simplicity of this method makes it perfect for multilingual QA. Many tools required by sophisticated QA systems (named entity taggers, parsers, ontologies, etc.) are language specific and require significant effort to adapt to a new language. Since the answer patterns used in this method are learned using only a small number of manual training terms, one can rapidly learn patterns for new languages, assuming the web search engine is appropriately switched. ", "summary_sents": ["In this paper we explore the power of surface text patterns for open-domain question answering systems.", "In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically.", "A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista.", "Patterns are then automatically extracted from the returned documents and standardized.", "We calculate the precision of each pattern, and the average precision for each question type.", "These patterns are then applied to find answers to new questions.", "Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.", "We present an alternatve ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns."]}
{"title": "Pseudo-Projective Dependency Parsing", "abstract": "In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures. We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures. Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy. This leads to the best reported performance for robust non-projective parsing of Czech. ", "introduction": "It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order, where discontinuous syntactic constructions are more common than in languages like English (Mel\u2019\u02c7cuk, 1988; Covington, 1990). However, this argument is only plausible if the formal framework allows non-projective dependency structures, i.e. structures where a head and its dependents may correspond to a discontinuous constituent. From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness. Thus, most broad-coverage parsers based on dependency grammar have been restricted to projective structures. This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). It is also true of the adaptation of the Collins parser for Czech (Collins et al., 1999) and the finite-state dependency parser for Turkish by Oflazer (2003). This is in contrast to dependency treebanks, e.g. Prague Dependency Treebank (Haji\u02c7c et al., 2001b), Danish Dependency Treebank (Kromann, 2003), and the METU Treebank of Turkish (Oflazer et al., 2003), which generally allow annotations with nonprojective dependency structures. The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions. While the proportion of sentences containing non-projective dependencies is often 15\u201325%, the total proportion of non-projective arcs is normally only 1\u20132%. As long as the main evaluation metric is dependency accuracy per word, with state-of-the-art accuracy mostly below 90%, the penalty for not handling non-projective constructions is almost negligible. Still, from a theoretical point of view, projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal. There exist a few robust broad-coverage parsers that produce non-projective dependency structures, notably Tapanainen and J\u00a8arvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech. In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003). Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004). In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques. First, the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al., 1998) and encoding information about these lifts in arc labels. When the parser is trained on the transformed data, it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts. By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures. We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998). The rest of the paper is structured as follows. In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs, and in section 3 we describe the data-driven dependency parser that is the core of our system. We then evaluate the approach in two steps. First, in section 4, we evaluate the graph transformation techniques in themselves, with data from the Prague Dependency Treebank and the Danish Dependency Treebank. In section 5, we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank. ", "conclusion": "We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques. The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, especially with respect to the exact match criterion, leading to the best reported performance for robust non-projective parsing of Czech. ", "summary_sents": ["In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures.", "We show how a data-driven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.", "Experiments using data from the Prague Dependency Treebank show that the combined system can handle non-projective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.", "This leads to the best reported performance for robust non-projective parsing of Czech.", "In our pseudo-projective approach, non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time.", "We show how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing.", "For handling non-projective relations, we suggest applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective.", "We note that since the number of non-projective dependencies is much smaller than the number of projective dependencies, it is not efficient to perform non-projective parsing for all cases."]}
{"title": "Global Thresholding And Multiple-Pass Parsing", "abstract": "We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level. We also present a new thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement. We use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms. ", "introduction": "In this paper, we examine thresholding techniques for statistical parsers. While there exist theoretically efficient (0(n3)) algorithms for parsing Probabilistic Context-Free Grammars (PCFGs) and related formalisms, practical parsing algorithms usually make use of pruning techniques, such as beam thresholding, for increased speed. We introduce two novel thresholding techniques, global thresholding and multiple-pass parsing, and one significant variation on traditional beam thresholding. We examine the value of these techniques when used separately, and when combined. In order to examine the combined techniques, we also introduce an algorithm for optimizing the settings comments on earlier drafts, and the anonymous reviewers for their extensive comments. of multiple thresholds. When all three thresholding methods are used together, they yield very significant speedups over traditional beam thresholding, while achieving the same level of performance. We apply our techniques to CKY chart parsing, one of the most commonly used parsing methods in natural language processing. In a CKY chart parser, a two-dimensional matrix of cells, the chart, is filled in. Each cell in the chart corresponds to a span of the sentence, and each cell of the chart contains the nonterminals that could generate that span. Cells covering shorter spans are filled in first, so we also refer to this kind of parser as a bottom-up chart parser. The parser fills in a cell in the chart by examining the nonterminals in lower, shorter cells, and combining these nonterminals according to the rules of the grammar. The more nonterminals there are in the shorter cells, the more combinations of nonterminals the parser must consider. In some grammars, such as PCFGs, probabilities are associated with the grammar rules. This introduces problems, since in many PCFGs, almost any combination of nonterminals is possible, perhaps with some low probability. The large number of possibilities can greatly slow parsing. On the other hand, the probabilities also introduce new opportunities. For instance, if in a particular cell in the chart there is some nonterminal that generates the span with high probability, and another that generates that span with low probability, then we can remove the less likely nonterminal from the cell. The less likely nonterminal will probably not be part of either the correct parse or the tree returned by the parser, so removing it will do little harm. This technique is called beam thresholding. If we use a loose beam threshold, removing only those nonterminals that are much less probable than the best nonterminal in a cell, our parser will run only slightly faster than with no thresholding, while performance measures such as precision and recall will remain virtually unchanged. On the other hand, if we use a tight threshold, removing nonterminals that are almost as probable as the best nonterminal in a cell, then we can get a considerable speedup, but at a considerable cost. Figure 1 shows the tradeoff between accuracy and time. In this paper, we will consider three different kinds of thresholding. The first of these is a variation on traditional beam search. In traditional beam search, only the probability of a nonterminal generating the terminals of the cell's span is used. We have found that a minor variation, introduced in Section 2, in which we also consider the prior probability that each nonterminal is part of the correct parse, can lead to nearly an order of magnitude improvement. The problem with beam search is that it only compares nonterminals to other nonterminals in the same cell. Consider the case in which a particular cell contains only bad nonterminals, all of roughly equal probability. We can't threshold out these nodes, because even though they are all bad, none is much worse than the best. Thus, what we want is a thresholding technique that uses some global information for thresholding, rather than just using information in a single cell. The second kind of thresholding we consider is a novel technique, global thresholding, described in Section 3. Global thresholding makes use of the observation that for a nonterminal to be part of the correct parse, it must be part of a sequence of reasonably probable nonterminals covering the whole sentence. The last technique we consider, multiple-pass parsing, is introduced in Section 4. The basic idea is that we can use information from parsing with one grammar to speed parsing with another. We run two passes, the first of which is fast and simple, eliminating from consideration many unlikely potential constituents. The second pass is more complicated and slower, but also more accurate. Because we have already eliminated many nodes in our first pass, the second pass can run much faster, and, despite the fact that we have to run two passes, the added savings in the second pass can easily outweigh the cost of the first one. Experimental comparisons of these techniques show that they lead to considerable speedups over traditional thresholding, when used separately. We also wished to combine the thresholding techniques; this is relatively difficult, since searching for the optimal thresholding parameters in a multi-dimensional space is potentially very time consuming. We .designed a variant on a gradient descent search algorithm to find the optimal parameters. Using all three thresholding methods together, and the parameter search algorithm, we achieved our best results, running an estimated 30 times faster than traditional beam search, at the same performance level. ", "conclusion": "In this paper, we only considered applying multiplepass and global thresholding techniques to parsing probabilistic context-free grammars. However, just about any probabilistic grammar formalism for which inside and outside probabilities can be computed can benefit from these techniques. For instance, Probabilistic Link Grammars (Lafferty, Sleator, and Temperley, 1992) could benefit from our algorithms. We have however had trouble using global thresholding with grammars that strongly violated the independence assumptions of global thresholding. One especially interesting possibility is to apply multiple-pass techniques to formalisms that require >> 0(n3) parsing time, such as Stochastic Bracketing Transduction Grammar (SBTG) (Wu, 1996) and Stochastic Tree Adjoining Grammars (STAG) (Resnik, 1992; Schabes, 1992). SBTG is a contextfree-like formalism designed for translation from one language to another; it uses a four dimensional chart to index spans in both the source and target language simultaneously. It would be interesting to try speeding up an SBTG parser by running an 0(n3) first pass on the source language alone, and using this to prune parsing of the full SBTG. The STAG formalism is a mildly context-sensitive formalism, requiring 0 (n6 ) time to parse. Most STAG productions in practical grammars are actually context-free. The traditional way to speed up STAG parsing is to use the context-free subset of an STAG to form a Stochastic Tree Insertion Grammar (STIG) (Schabes and Waters, 1994), an 0 (n3 ) formalism, but this method has problems, because the STIG undergenerates since it is missing some elementary trees. A different approach would be to use multiple-pass parsing. We could first find a contextfree covering grammar for the STAG, and use this as a first pass, and then use the full STAG for the second pass. The grammars described here are fairly simple, presented for purposes of explication. In other work in preparation, in which we have used a significantly more complicated grammar, which we call the Probabilistic Feature Grammar (PFG), the improvements from multiple-pass parsing are even more dramatic: single pass experiments are simply too slow to run at all. We have also found the automatic thresholding parameter optimization algorithm to be very useful. Before writing the parameter optimization algorithm, we developed the PFG grammar and the multiple-pass parsing technique and ran a series of experiments using hand optimized parameters. We recently ran the optimization algorithm and reran the experiments, achieving a factor of two speedup with no performance loss. While we had not spent a great deal of time hand optimizing these parameters, we are very encouraged by the optimization algorithm's practical utility. This paper introduces four new techniques: beam thresholding with priors, global thresholding, multiple-pass parsing, and automatic search for thresholding parameters. Beam thresholding with priors can lead to almost an order of magnitude improvement over beam thresholding without priors. Global thresholding can be up to three times as efficient as the new beam thresholding technique, although the typical improvement is closer to 50%. When global thresholding and beam thresholding are combined, they are usually two to three times as fast as beam thresholding alone. Multiple-pass parsing can lead to up to an additional 50% improvement with the grammars in this paper. We expect the parameter optimization algorithm to be broadly useful. ", "summary_sents": ["We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level.", "We also present a new thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement.", "We use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms.", "we describe a method for producing a simple but crude approximate grammar of a standard context-free grammar."]}
{"title": "A Decoder For Syntax-Based Statistical MT", "abstract": "This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001). The model has been extended to incorporate phrasal translations as presented here. In contrast to a conventional word-to-word statistical model, a decoder for the syntaxbased model builds up an English parse tree given a sentence in a foreign language. As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for each word alignment, several pruning techniques are necessary. We tested our decoder in a Chinese-to-English translation system, and obtained better results than IBM Model 4. We also discuss issues concerning the relation between this decoder and a language model. ", "introduction": "A statistical machine translation system based on the noisy channel model consists of three components: a language model (LM), a translation model (TM), and a decoder. For a system which translates from a foreign language to English, the LM gives a prior probability P and the TM gives a channel translation probability P . These models are automatically trained using monolingual (for the LM) and bilingual (for the TM) corpora. A decoder then finds the best English sentence given a foreign are not simple probability tables but are parameterized models, a decoder must conduct a search over the space defined by the models. For the IBM models defined by a pioneering paper (Brown et al., 1993), a decoding algorithm based on a left-to-right search was described in (Berger et al., 1996). Recently (Yamada and Knight, 2001) introduced a syntax-based TM which utilized syntactic structure in the channel input, and showed that it could outperform the IBM model in alignment quality. In contrast to the IBM models, which are word-to-word models, the syntax-based model works on a syntactic parse tree, so the decoder builds up an English parse tree given a sentencein a foreign language. This paper describes an algorithm for such a decoder, and reports experimental results. Other statistical machine translation systems such as (Wu, 1997) and (Alshawi et al., 2000) also produce a tree given a sentence. Their models are based on mechanisms that generate two languages at the same time, so an English tree is obtained as a subproduct of parsing. However, their use of the LM is not mathematically motivated, since their models do not decompose into P and unlike the noisy channel model. Section 2 briefly reviews the syntax-based TM, and Section 3 describes phrasal translation as an extension. Section 4 presents the basic idea for decoding. As in other statistical machine translation systems, the decoder has to cope with a huge search sentence that maximizes P , which also maximizes P according to Bayes\u2019 rule. A different decoder is needed for different choices of LM and TM. Since P and P space. Section 5 describes how to prune the search space for practical decoding. Section 6 shows experimental results. Section 7 discusses LM issues, and is followed by conclusions. ", "conclusion": "We have presented a decoding algorithm for a syntax-based statistical machine translation. The translation model was extended to incorporate phrasal translations. Because the input of the channel model is an English parse tree, the decoding algorithm is based on conventional syntactic parsing, and the grammar is expanded by the channel operations of the TM. As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for a word alignment, efficient pruning is necessary. We applied several pruning techniques and obtained good decoding quality and coverage. The choice of the LM is an important issue in implementing a decoder for the syntaxbased TM. At present, the best result is obtained by using trigrams, but a more sophisticated LM seems promising. ", "summary_sents": ["This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001).", "The model has been extended to incorporate phrasal translations as presented here.", "In contrast to a conventional word-to-word statistical model, a decoder for the syntax-based model builds up an English parse tree given a sentence in a foreign language.", "As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for each word alignment, several pruning techniques are necessary.", "We tested our decoder in a Chinese-to-English translation system, and obtained better results than IBM Model 4.", "We also discuss issues concerning the relation between this decoder and a language model.", "We propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence."]}
{"title": "Determining The Sentiment Of Opinions", "abstract": "Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results. ", "introduction": "What is an opinion? The many opinions on opinions are reflected in a considerable literature (Aristotle 1954; Perelman 1970; Toulmin et al 1979; Wallace 1975; Toulmin 2003). Recent computational work either focuses on sentence ?subjectivity? (Wiebe et al 2002; Riloff et al 2003), concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al 2002), or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives. We wish to study opinion in general; our work most closely resembles that of (Yu and Hatzivassiloglou 2003). Since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion. For our purposes, we describe an opinion as a quadruple [Topic, Holder, Claim, Sentiment] in which the Holder believes a Claim about the Topic, and in many cases associates a Sentiment, such as good or bad, with the belief. For example, the following opinions contain Claims but no Sentiments: ?I believe the world is flat? ?The Gap is likely to go bankrupt? ?Bin Laden is hiding in Pakistan? ?Water always flushes anti-clockwise in the southern hemisphere? Like Yu and Hatzivassiloglou (2003), we want to automatically identify Sentiments, which in this work we define as an explicit or implicit expression in text of the Holder?s positive, negative, or neutral regard toward the Claim about the Topic. (Other sentiments we plan to study later.) Sentiments always involve the Holder?s emotions or desires, and may be present explicitly or only implicitly: ?I think that attacking Iraq would put the US in a difficult position? (implicit) ?The US attack on Iraq is wrong? (explicit) ?I like Ike? (explicit) ?We should decrease our dependence on oil? (implicit) ?Reps. Tom Petri and William F. Goodling asserted that counting illegal aliens violates citizens? basic right to equal representation? (implicit) In this paper we address the following challenge problem. Given a Topic (e.g., ?Should abortion be banned??) and a set of texts about the topic, find the Sentiments expressed about (claims about) the Topic (but not its supporting subtopics) in each text, and identify the people who hold each sentiment. To avoid the problem of differentiating between shades of sentiments, we simplify the problem to: identify just expressions of positive, negative, or neutral sentiments, together with their holders. In addition, for sentences that do not express a sentiment but simply state that some sentiment(s) exist(s), return these sentences in a separate set. For example, given the topic ?What should be done with Medicare?? the sentence ?After years of empty promises, Congress has rolled out two Medicare prescription plans, one from House Republicans and the other from the Democratic Sentence POS Tagger verbs nounsAdjectives Adjective Senti ment classifier sentiment sentiment Sentence sentiment classifier Opinion region + polarity + holder Holder finder Named Entity Tagger Sentence Sentence texts + topic sentiment sentiment sentiment V rbs Verb Senti ment classifier Nouns Noun Senti ment classifier WordNet Sentence : Figure 1: System architecture. Sens. Bob Graham of Florida and Zell Miller of Georgia? should be returned in the separate set. We approach the problem in stages, starting with words and moving on to sentences. We take as unit sentiment carrier a single word, and first classify each adjective, verb, and noun by its sentiment. We experimented with several classifier models. But combining sentiments requires additional care, as Table 1 shows. California Supreme Court agreed that the state?s new term-limit law was constitutional. California Supreme Court disagreed that the state?s new term-limit law was constitutional. California Supreme Court agreed that the state?s new term-limit law was unconstitutional. California Supreme Court disagreed that the state?s new term-limit law was unconstitutional. Table 1: Combining sentiments. A sentence might even express opinions of different people. When combining word-level sentiments, we therefore first determine for each Holder a relevant region within the sentence and then experiment with various models for combining word sentiments. We describe our models and algorithm in Section 2, system experiments and discussion in Section 3, and conclude in Section 4. ", "conclusion": "Sentiment recognition is a challenging and difficult part of understanding opinions. We plan to extend our work to more difficult cases such as sentences with weak-opinion-bearing words or sentences with multiple opinions about a topic. To improve identification of the Holder, we plan to use a parser to associate regions more reliably with holders. We plan to explore other learning techniques, such as decision lists or SVMs. Nonetheless, as the experiments show, encouraging results can be obtained even with relatively simple models and only a small amount of manual seeding effort. ", "summary_sents": ["Identifying sentiments (the affective parts of opinions) is a challenging problem.", "We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion.", "The system contains a module for determining word sentiment and another for combining sentiments within a sentence.", "We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results.", "We try to determine the final sentiment orientation of a given sentence by combining sentiment words within it.", "We start with two lists of positive and negative seed words.", "We use WordNet synonyms and antonyms to expand two lists of positive and negative seed words."]}
{"title": "Extracting Aspect-Evaluation and Aspect-Of Relations in Opinion Mining", "abstract": "The technology of opinion extraction allowsusers to retrieve and analyze people?s opinions scattered over Web documents. We define an opinion unit as a quadruple consist ing of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of theevaluation that expresses a positive or neg ative assessment. We use this definition as the basis for our opinion extraction task. We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluationrelations, and (b) extracting aspect-of re lations, and we approach each task usingmethods which combine contextual and sta tistical clues. Our experiments on Japaneseweblog posts show that the use of contex tual clues improve the performance for both tasks. ", "introduction": "The explosive increase in Web communication hasattracted increasing interest in technologies for automatically mining personal opinions from Web doc uments such as product reviews and weblogs. Such technologies would benefit users who seek reviews on certain consumer products of interest.Previous approaches to the task of mining a large scale document collection of customer opinions (or ? Currently, NTT Cyber Space Laboratories, 1-1, Hikarinooka, Yokosuka, Kanagawa, 239-0847 Japanreviews) can be classified into two approaches: Doc ument classification and information extraction. Theformer is the task of classifying documents or pas sages according to their semantic orientation such aspositive vs. negative. This direction has been form ing the mainstream of research on opinion-sensitive text processing (Pang et al, 2002; Turney, 2002, etc.). The latter, on the other hand, focuses on the task of extracting opinions consisting of informationabout, for example, ?who feels how about which as pect of what product? from unstructured text data.In this paper, we refer to this information extraction oriented task as opinion extraction. In contrast to sentiment classification, opinion extraction aims atproducing richer information and requires an in depth analysis of opinions, which has only recently been attempted by a growing but still relatively small research community (Yi et al, 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005, etc.).Most previous work on customer opinion ex traction assumes the source of information to be customer reviews collected from customer review sites (Popescu and Etzioni, 2005; Hu and Liu, 2004;Liu et al, 2005). In contrast, in this paper, we con sider the task of extracting customer opinions fromunstructured weblog posts. Compared with extrac tion from review articles, extraction from weblogs is more challenging because weblog posts tend toexhibit greater diversity in topics, goals, vocabulary, style, etc. and are much more likely to include descriptions irrelevant to the subject in question. In this paper, we first describe our task set ting of opinion extraction. We conducted a corpusstudy and investigated the feasibility of the task def 1065 inition by showing the statistics and inter-annotator agreement of our corpus annotation. Next, we showthat the crucial body of the above opinion extraction task can be decomposed into two kinds of relation extraction, i.e. aspect-evaluation relation extraction and aspect-of relation extraction. For exam ple, the passage ?I went out for lunch at the Deli and ordered a curry with chicken. It was pretty good? has an aspect-evaluation relation ?curry with chicken, was good? and an aspect-of relation ?The Deli, curry with the chicken?. The former task can be regarded as a special type of predicate-argument structure analysis or semantic role labeling. Thelatter, on the other hand, can be regarded as bridg ing reference resolution (Clark, 1977), which is the task of identifying relations between definite noun phrases and discourse-new entities implicitly related to some previously mentioned entities. Most of the previous work on customer opinionextraction, however, does not adopt the state-of-theart techniques in those fields, relying only on sim ple proximity-based or pattern-based methods. Inthis context, this paper empirically shows that incor porating machine learning-based techniques devisedfor predicate-argument structure analysis and bridg ing reference resolution improve the performanceof both aspect-evaluation and aspect-of relation extraction. Furthermore, we also show that combin ing contextual clues with a common co-occurrencestatistics-based technique for bridging reference resolution makes a significant improvement on aspect of relation extraction. ", "conclusion": "", "summary_sents": ["The technology of opinion extraction allows users to retrieve and analyze people's opinions scattered over Web documents.", "We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment.", "We use this definition as the basis for our opinion extraction task.", "We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we approach each task using methods which combine contextual and statistical clues.", "Our experiments on Japanese weblog posts show that the use of contextual clues improve the performance for both tasks.", "We analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains.", "We adopt a supervised learning technique to search for useful syntactic patterns as contextual clues."]}
{"title": "Combining Outputs from Multiple Machine Translation Systems", "abstract": "Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting information from -best lists, system scores and target-to-source phrase alignments. The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods. ", "introduction": "In recent years, machine translation systems based on new paradigms have emerged. These systems employ more than just the surface-level information used by the state-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2006) systems have recently improved in both accuracy and scalability. Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). System combination has been successfully applied in state-of-the-art speech recognition evaluation systems for several years (Fiscus, 1997). Even though the underlying modeling techniques are similar, many systems produce very different outputs with approximately the same accuracy. One of the most successful approaches is consensus network decoding (Mangu et al., 2000) which assumes that the confidence of a word in a certain position is based on the sum of confidences from each system output having the word in that position. This requires aligning the system outputs to form a consensus network and \u2013 during decoding \u2013 simply finding the highest scoring path through this network. The alignment of speech recognition outputs is fairly straightforward due to the strict constraint in word order. However, machine translation outputs do not have this constraint as the word order may be different between the source and target languages. MT systems employ various re-ordering (distortion) models to take this into account. Three MT system combination methods are presented in this paper. They operate on the sentence, phrase and word level. The sentence-level combination is based on selecting the best hypothesis out of the merged N-best lists. This method does not generate new hypotheses \u2013 unlike the phrase and word-level methods. The phrase-level combination is based on extracting sentence-specific phrase translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on consensus network decoding. Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell\u2019s method (Brent, 1973) on -best lists. Both sentence and phrase-level combination methods can generate best lists which may also be used as new system outputs in the word-level combination. Experiments on combining six machine translation system outputs were performed. Three systems were phrasal, two hierarchical and one syntaxbased. The systems were evaluated on NIST MT05 and the newsgroup portion of the GALE 2006 dryrun sets. The outputs were evaluated on both TER and BLEU. As the target evaluation metric in the GALE program was human-mediated TER (HTER) (Snover et al., 2006), it was found important to improve both of these automatic metrics. This paper is organized as follows. Section 2 describes the evaluation metrics and a generic discriminative optimization technique used in tuning of the various system combination weights. Sentence, phrase and word-level system combination methods are presented in Sections 3, 4 and 5. Experimental results on Arabic and Chinese to English newswire and newsgroup test data are presented in Section 6. ", "conclusion": "Three methods for machine translation system combination were presented in this paper. The sentencelevel combination was based on re-ranking a merged -best list using generalized linear models with features derived from each system\u2019s output. The combination yields slight gains on the tuning set. However, the gains were very small, if any, on the test sets. The re-ranked -best lists were used successfully in the word-level combination method as new system outputs. Various other features may be explored in this framework although the tuning may be limited by the chosen optimization method in the higher dimensional parameter space. The phrase-level combination was based on deriving a new phrase translation table from the alignments to source provided in all system outputs. The phrase translation scores were based on the level of agreement between the system outputs and sentence posterior estimates. A standard phrasal decoder with the new phrase table was used to produce the final combination output. The handling of the alignments from non-phrasal decoders may not be optimal, though. The phrase-level combination yields fairly good gains on the tuning sets. However, the performance does not seem to generalize to the test sets used in this work. As usual, the phrasal decoder can generate -best lists which were used successfully in the word-level combination method as new system outputs. The word-level combination method based on consensus network decoding seems to be very robust and yield good gains over the best single system even without any tunable weights. The decision of the skeleton is crucial. Minimum Bayes Risk decoding under translation edit rate was used to select the skeleton. Compared to the best possible skeleton decision \u2013 according to an oracle experiment \u2013 further gains might be obtained by using better decision approach. Also, the alignment may be improved by taking the target-to-source alignments into account and allowing synonyms to align. The confusion network decoding at the word level does not necessarily retain coherent phrases as no language model constraints are taken into account. LM re-scoring might alleviate this problem. This paper has provided evidence that outputs from six very different MT systems, tuned for two different evaluation metrics, may be combined to yield better outputs in terms of different evaluation metrics. The focus of the future work will be to address the individual issues in the combination methods mentioned above. It would also be interesting to investigate how much different systems contribute to the overall gain achieved via system combination. ", "summary_sents": ["Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based.", "These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge.", "The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems.", "This paper describes three different approaches to MT system combination.", "These combination methods operate on sentence, phrase and word level exploiting information from N-best lists, system scores and target-to-source phrase alignments.", "The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods.", "We use minimum Translation Error Rate (TER) (Snover et al, 2006) alignment to build the confusion network.", "We collect source-to-target correspondences from the input systems, create a new translation option table using only these phrases, and re-decode the source sentence to generate better translations."]}
{"title": "Functional Centering Grounding Referential Coherence In Information Structure", "abstract": "Considering empirical evidence from a free-word-order language (German) we propose a revision of the principles guiding the ordering of discourse entities in the forward-looking center list within the centering model. We claim that grammatical role criteria should be replaced by criteria that reflect the functional information structure of the utterances. These new criteria are based on the distinction between hearer-old and hearer-new discourse entities. We demonstrate that such a functional model of centering can be successfully applied to the analysis of several forms of referential text phenomena, viz. pronominal, nominal, and functional anaphora. Our methodological and empirical claims are substantiated by two evaluation studies. In the first one, we compare success rates for the resolution of pronominal anaphora that result from a grammaticalrole-driven centering algorithm and from a functional centering algorithm. The second study deals with a new cost-based evaluation methodology for the assessment of centering data, one which can be directly derived from and justified by the cognitive load premises of the centering model. ", "introduction": "", "conclusion": "", "summary_sents": ["Considering empirical evidence from a free-word-order language (German) we propose a revision of the principles guiding the ordering of discourse entities in the forward-looking center list within the centering model.", "We claim that grammatical role criteria should be replaced by criteria that reflect the functional information structure of the utterances.", "These new criteria are based on the distinction between hearer-old and hearer-new discourse entities.", "We demonstrate that such a functional model of centering can be successfully applied to the analysis of several forms of referential text phenomena, viz. pronominal, nominal, and functional anaphora.", "Our methodological and empirical claims are substantiated by two evaluation studies. In the first one, we compare success rates for the resolution of pronominal anaphora that result from a grammatical role-driven centering algorithm and from a functional centering algorithm.", "The second study deals with a new cost-based evaluation methodology for the assessment of centering data, one which can be directly derived from and justified by the cognitive load premises of the centering model.", "we introduce Functional Centering, a variant of Centering Theory which utilizes information status distinctions between hearer-old and hearer-new entities."]}
{"title": "Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking", "abstract": "Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less. ", "introduction": "We describe a reranking parser which uses a regularized MaxEnt reranker to select the best parse from the 50-best parses returned by a generative parsing model. The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees (according to the parser\u2019s model) have an f-score of 0.897 on section 23 of the Penn Treebank (Charniak, 2000), which is still state-of-the-art. However, the 50 best (i.e., the 50 highest probability) parses of a sentence often contain considerably better parses (in terms of f-score); this paper describes a 50-best parsing algorithm with an oracle f-score of 96.8 on the same data. The reranker attempts to select the best parse for a sentence from the 50-best list of possible parses for the sentence. Because the reranker only has to consider a relatively small number of parses per sentences, it is not necessary to use dynamic programming, which permits the features to be essentially arbitrary functions of the parse trees. While our reranker does not achieve anything like the oracle f-score, the parses it selects do have an f-score of 91.0, which is considerably better than the maximum probability parses of the n-best parser. In more detail, for each string s the n-best parsing algorithm described in section 2 returns the n highest probability parses Y(s) = {y1(s), ... , yn(s)} together with the probability p(y) of each parse y according to the parser\u2019s probability model. The number n of parses was set to 50 for the experiments described here, but some simple sentences actually received fewer than 50 parses (so n is actually a function of s). Each yield or terminal string in the training, development and test data sets is mapped to such an n-best list of parse/probability pairs; the cross-validation scheme described in Collins (2000) was used to avoid training the n-best parser on the sentence it was being used to parse. A feature extractor, described in section 3, is a vector of m functions f = (fl, ... , fm), where each fj maps a parse y to a real number fj(y), which is the value of the jth feature on y. So a feature extractor maps each y to a vector of feature values f(y) = (f1(y), ..., fm(y)). Our reranking parser associates a parse with a score v\u03b8(y), which is a linear function of the feature values f(y). That is, each feature fj is associated with a weight \u03b8j, and the feature values and weights define the score v\u03b8(y) of each parse y as follows: Given a string s, the reranking parser\u2019s output \u02c6y(s) on string s is the highest scoring parse in the n-best parses Y(s) for s, i.e., The feature weight vector \u03b8 is estimated from the labelled training corpus as described in section 4. Because we use labelled training data we know the correct parse y? (s) for each sentence s in the training data. The correct parse y? (s) is not always a member of the n-best parser\u2019s output Y(s), but we can identify the parses Y+(s) in Y(s) with the highest f-scores. Informally, the estimation procedure finds a weight vector \u03b8 that maximizes the score v\u03b8(y) of the parses y E Y+(s) relative to the scores of the other parses in Y(s), for each s in the training data. ", "conclusion": "This paper has described a dynamic programming n-best parsing algorithm that utilizes a heuristic coarse-to-fine refinement of parses. Because the coarse-to-fine approach prunes the set of possible parse edges beforehand, a simple approach which enumerates the n-best analyses of each parse edge is not only practical but quite efficient. We use the 50-best parses produced by this algorithm as input to a MaxEnt discriminative reranker. The reranker selects the best parse from this set of parses using a wide variety of features. The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework. This result is only slightly higher than the highest reported result for this test-set, Bod\u2019s (.907) (Bod, 2003). More to the point, however, is that the system we describe is reasonably efficient so it can be used for the kind of routine parsing currently being handled by the Charniak or Collins parsers. A 91.0 f-score represents a 13% reduction in fmeasure error over the best of these parsers.2 Both the 50-best parser, and the reranking parser can be found at ftp://ftp.cs.brown.edu/pub/nlparser/, named parser and reranker respectively. Acknowledgements We would like to thanks Michael Collins for the use of his data and many helpful comments, and Liang Huang for providing an early draft of his paper and very useful comments on our paper. Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276). ", "summary_sents": ["Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).", "A discriminative reranker requires a source of candidate parses for each sentence.", "This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).", "This method generates 50-best lists that are of substantially higher quality than previously obtainable.", "We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.", "We use pruning, where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories.", "We show accuracy improvements from composed local tree features on top of a lexicalized base parser.", "To improve performance and robustness, features are pruned so that selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times."]}
{"title": "Inducing History Representations For Broad Coverage Statistical Parsing", "abstract": "We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser. The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge. Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions. ", "introduction": "Unlike most problems addressed with machine learning, parsing natural language sentences requires choosing between an unbounded (or even infinite) number of possible phrase structure trees. The standard approach to this problem is to decompose this choice into an unbounded sequence of choices between a finite number of possible parser actions. This sequence is the parse for the phrase structure tree. We can then define a probabilistic model of phrase structure trees by defining a probabilistic model of each parser action in its parse context, and apply machine learning techniques to learn this model of parser actions. Many statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000) are based on a history-based model of parser actions. In these models, the probability of each parser action is conditioned on the history of previous actions in the parse. But here again we are faced with an unusual situation for machine learning problems, conditioning on an unbounded amount of information. A major challenge in designing a history-based statistical parser is choosing a finite representation of the unbounded parse history from which the probability of the next parser action can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the parse history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000). In the work presented here, we automatically induce a finite set of real valued features to represent the parse history. We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001; Henderson, 2000). This machine learning method is specifically designed for processing unbounded structures. It allows us to avoid making a priori independence assumptions, unlike with hand-crafted history features. But it also allows us to make use of our a priori knowledge by imposing structurally specified and linguistically appropriate biases on the search for a good history representation. The combination of automatic feature induction and linguistically appropriate biases results in a history-based parser with state-of-the-art performance. When trained on just part-of-speech tags, the resulting parser achieves the best current performance of a non-lexicalized parser on the Penn Treebank. When a relatively small vocabulary of words is used, performance is only marginally below the best current parser accuracy. If either the biases are reduced or the induced history representations are replaced with hand-crafted features, performance degrades. ", "conclusion": "This paper has presented a method for estimating the parameters of a history-based statistical parser which does not require any a priori independence assumptions. A neural network is trained simultaneously to estimate the probabilities of parser actions and to induce a finite representation of the unbounded parse history. The probabilities of parser actions are conditioned on this induced history representation, rather than being conditioned on a set of hand-crafted history features chosen a priori. A beam search is used to search for the most probable parse given the neural network\u2019s probability estimates. When trained and tested on the standard Penn Treebank datasets, the parser\u2019s performance (89.1% F-measure) is only 0.6% below the best current parsers for this task, despite using a smaller vocabulary and less prior linguistic knowledge. The neural network architecture we use, Simple Synchrony Networks, not only allows us to avoid imposing hard independence assumptions, it also allows us to impose linguistically appropriate soft biases on the learning process. SSNs are specifically designed for processing structures, which allows us to design the SSN so that the induced representations of the parse history are biased towards recording structurally local information about the parse. When we modify these biases so that some structurally local information tends to be ignored, performance degrades. When we introduce independence assumptions by cutting off access to information from more distant parts of the structure, performance degrades dramatically. On the other hand, we find that biasing the learning to pay more attention to lexical heads does not improve performance. ", "summary_sents": ["We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.", "The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge.", "Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.", "Of the previous work on using neural net works for parsing natural language, the most empirically successful has been our work using Simple Synchrony Networks.", "We test the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs."]}
{"title": "A Statistical Model For Multilingual Entity Detection And Tracking", "abstract": "Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages. ", "introduction": "Detecting entities, whether named, nominal or pronominal, in unrestricted text is a crucial step toward understanding the text, as it identifies the important conceptual objects in a discourse. It is also a necessary step for identifying the relations present in the text and populating a knowledge database. This task has applications in information extraction and summarization, information retrieval (one can get all hits for Washington/person and not the ones for Washington/state or Washington/city), data mining and question answering. The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (Bikel et al., 1997; Borthwick et al., 1998; Mikheev et al., 1999; Miller et al., 1998; Aberdeen et al., 1995; Ng and Cardie, 2002; Soon et al., 2001), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL\u201902 and CoNLL\u201903 shared tasks. Usually, in computational linguistic literature, a named entity represents an instance of a name, either a location, a person, an organization, and the NER task consists of identifying each individual occurrence of such an entity. We will instead adopt the nomenclature of the Automatic Content Extraction program' (NIST, 2003a): we will call the instances of textual references to objects or abstractions mentions, which can be either named (e.g. John Mayor), nominal (e.g. the president) or pronominal (e.g. she, it). An entity consists of all the mentions (of any level) which refer to one conceptual entity. For instance, in the sentence there are two mentions: John Smith and he (in the order of appearance, their levels are named and pronominal), but one entity, formed by the set {John Smith, he}. In this paper, we present a general statistical framework for entity detection and tracking in unrestricted text. The framework is not language specific, as proved by applying it to three radically different languages: Arabic, Chinese and English. We separate the EDT task into a mention detection part \u2013 the task of finding all mentions in the text \u2013 and an entity tracking part \u2013 the task of combining the detected mentions into groups of references to the same object. The work presented here is motivated by the ACE evaluation framework, which has the more general goal of building multilingual systems which detect not only entities, but also relations among them and, more recently, events in which they participate. The EDT task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominals and pronouns) and the requirement of grouping mentions into entities. We present and evaluate empirically statistical models for both mention detection and entity tracking problems. For mention detection we use approaches based on Maximum Entropy (MaxEnt henceforth) (Berger et al., 1996) and Robust Risk Minimization (RRM henceforth) 'For a description of the ACE program see http://www.nist.gov/speech/tests/ace/. (Zhang et al., 2002). The task is transformed into a sequence classification problem. We investigate a wide array of lexical, syntactic and semantic features to perform the mention detection and classification task including, for all three languages, features based on pre-existing statistical semantic taggers, even though these taggers have been trained on different corpora and use different semantic categories. Moreover, the presented approach implicitly learns the correlation between these different semantic types and the desired output types. We propose a novel MaxEnt-based model for predicting whether a mention should or should not be linked to an existing entity, and show how this model can be used to build entity chains. The effectiveness of the approach is tested by applying it on data from the above mentioned languages \u2014 Arabic, Chinese, English. The framework presented in this paper is languageuniversal \u2013 the classification method does not make any assumption about the type of input. Most of the feature types are shared across the languages, but there are a small number of useful feature types which are languagespecific, especially for the mention detection task. The paper is organized as follows: Section 2 describes the algorithms and feature types used for mention detection. Section 3 presents our approach to entity tracking. Section 4 describes the experimental framework and the systems\u2019 results for Arabic, Chinese and English on the data from the latest ACE evaluation (September 2003), an investigation of the effect of using different feature types, as well as a discussion of the results. ", "conclusion": "This paper presents a language-independent framework for the entity detection and tracking task, which is shown to obtain top-tier performance on three radically different languages: Arabic, Chinese and English. The task is separated into two sub-tasks: a mention detection part, which is modeled through a named entity-like approach, and an entity tracking part, for a which a novel modeling approach is proposed. This statistical framework is general and can incorporate heterogeneous feature types \u2014 the models were built using a wide array of lexical, syntactic and semantic features extracted from texts, and further enhanced by adding the output of pre-existing semantic classifiers as feature streams; additional feature types help improve the performance significantly, especially in terms of ACE value. The experimental results show that the systems perform remarkably well, for both well investigated languages, such as English, and for the relatively new additions Arabic and Chinese. ", "summary_sents": ["Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks.", "In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text.", "Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features.", "In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers.", "The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages."]}
{"title": "Correcting ESL Errors Using Phrasal SMT Techniques", "abstract": "This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL). Using examples of mass noun errors in the Learner Error Cor- (CLEC) guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers. Our system was able to correct 61.81% of mistakes in a set of naturallyoccurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect alignable corpora of preand post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners. ", "introduction": "Every day, in schools, universities and businesses around the world, in email and on blogs and websites, people create texts in languages that are not their own, most notably English. Yet, for writers of English as a Second Language (ESL), useful editorial assistance geared to their needs is surprisingly hard to come by. Grammar checkers such as that provided in Microsoft Word have been designed primarily with native speakers in mind. Moreover, despite growing demand for ESL proofing tools, there has been remarkably little progress in this area over the last decade. Research into computer feedback for ESL writers remains largely focused on smallscale pedagogical systems implemented within the framework of CALL (Computer Aided Language Learning) (Reuer 2003; Vanderventer Faltin, 2003), while commercial ESL grammar checkers remain brittle and difficult to customize to meet the needs of ESL writers of different first-language (L1) backgrounds and skill levels. Some researchers have begun to apply statistical techniques to identify learner errors in the context of essay evaluation (Chodorow & Leacock, 2000; Lonsdale & Strong-Krause, 2003), to detect non-native text (Tomokiyo & Jones, 2001), and to support lexical selection by ESL learners through first-language translation (Liu et al., 2000). However, none of this work appears to directly address the more general problem of how to robustly provide feedback to ESL writers\u2014and for that matter non-native writers in any second language\u2014in a way that is easily tailored to different L1 backgrounds and secondlanguage (L2) skill levels. In this paper, we show that a noisy channel model instantiated within the paradigm of Statistical Machine Translation (SMT) (Brown et al., 1993) can successfully provide editorial assistance for non-native writers. In particular, the SMT approach provides a natural mechanism for suggesting a correction, rather than simply stranding the user with a flag indicating that the text contains an error. Section 2 further motivates the approach and briefly describes our SMT system. Section 3 discusses the data used in our experiment, which is aimed at repairing a common type of ESL error that is not well-handled by current grammar checking technology: mass/count noun confusions. Section 4 presents experimental results, along with an analysis of errors produced by the system. Finally we present discussion and some future directions for investigation. ", "conclusion": "In this pilot study we have shown that SMT techniques have potential to provide error correction and stylistic writing assistance to L2 learners. The next step will be to obtain a large dataset of pre- and post-editing ESL text with which to train a model that does not rely on engineered data. A major purpose of the present study has been to determine whether our hypothesis is robust enough to warrant the cost and effort of a collection or data creation effort. Although we anticipate that it will take a significant lead time to assemble the necessary aligned data, once a sufficiently large corpus is in hand, we expect to begin exploring ways to improve our SMT system by tailoring it more specifically to the demands of editorial assistance. In particular, we expect to be looking into alternative word alignment models and possibly enhancing our system\u2019s decoder using some of the richer, more structured language models that are beginning to emerge. ", "summary_sents": ["This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL).", "Using examples of mass noun errors found in the Chinese Learner Error Corpus (CLEC) to guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers.", "Our system was able to correct 61.81% of mistakes in a set of naturally-occurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners.", "We utilize phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrate that this data-intensive SMT approach is very promising, but we also point out SMT approach relies on the availability of large amount of training data."]}
{"title": "CoNLL-X Shared Task On Multilingual Dependency Parsing", "abstract": "Each year the Conference on Computational Natural Language Learning features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser? Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski, the other two organizers of the shared task, for discussions, converting treebanks, writing and helping with the also to Alexander Yeh for additional help with the paper reviews. His work was made possible by the MITRE Cor ", "introduction": "Previous CoNLL shared tasks focused on NP chunking (1999), general chunking (2000), clause identification (2001), named entity recognition (2002, 2003), and semantic role labeling (2004, 2005). This shared task on full (dependency) parsing is the logical next step. Parsing is an important preprocessing step for many NLP applications and therefore of considerable practical interest. It is a complex task and as it is not straightforwardly mappable to a \u201cclassical\u201d segmentation, classification or sequence prediction problem, it also poses theoretical challenges to machine learning researchers. During the last decade, much research has been done on data-driven parsing and performance has increased steadily. For training these parsers, syntactically annotated corpora (treebanks) of thousands to tens of thousands of sentences are necessary; so initially, research has focused on English. During the last few years, however, treebanks for other languages have become available and some parsers have been applied to several different languages. See Section 2 for a more detailed overview of related previous research. So far, there has not been much comparison between different dependency parsers on exactly the same data sets (other than for English). One of the reasons is the lack of a de-facto standard for an evaluation metric (labeled or unlabeled, separate root accuracy? ), for splitting the data into training and testing portions and, in the case of constituency treebanks converted to dependency format, for this conversion. Another reason are the various annotation schemes and logical data formats used by different treebanks, which make it tedious to apply a parser to many treebanks. We hope that this shared task will improve the situation by introducing a uniform approach to dependency parsing. See Section 3 for the detailed task definition and Section 4 for information about the conversion of all 13 treebanks. In this shared task, participants had two to three months3 to implement a parsing system that could be trained for all these languages and four days to parse unseen test data for each. 19 participant groups submitted parsed test data. Of these, all but one parsed all 12 required languages and 13 also parsed the optional Bulgarian data. A wide variety of parsing approaches were used: some are extensions of previously published approaches, others are new. See Section 5 for an overview. Systems were scored by computing the labeled attachment score (LAS), i.e. the percentage of \u201cscoring\u201d tokens for which the system had predicted the correct head and dependency label. Punctuation tokens were excluded from scoring. Results across languages and systems varied widely from 37.8% (worst score on Turkish) to 91.7% (best score on Japanese). See Section 6 for detailed results. However, variations are consistent enough to allow us to draw some general conclusions. Section 7 discusses the implications of the results and analyzes the remaining problems. Finally, Section 8 describes possible directions for future research. ", "conclusion": "", "summary_sents": ["Each year the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems.", "The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.", "In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured.", "We also give an overview of the parsing approaches that participants took and the results that they achieved.", "Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?", "The CoNLL-X shared tasks focused on multilingual dependency parsing."]}
{"title": "First-Order Probabilistic Models for Coreference Resolution", "abstract": "Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases. In this paper, we propose a machine learning method enables features over noun phrases, resulting in a first-order probabilistic model for coreference. We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases. This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently. ", "introduction": "Noun phrase coreference resolution is the problem of clustering noun phrases into anaphoric sets. A standard machine learning approach is to perform a set of independent binary classifications of the form \u201cIs mention a coreferent with mention b?\u201d This approach of decomposing the problem into pairwise decisions presents at least two related difficulties. First, it is not clear how best to convert the set of pairwise classifications into a disjoint clustering of noun phrases. The problem stems from the transitivity constraints of coreference: If a and b are coreferent, and b and c are coreferent, then a and c must be coreferent. This problem has recently been addressed by a number of researchers. A simple approach is to perform the transitive closure of the pairwise decisions. However, as shown in recent work (McCallum and Wellner, 2003; Singla and Domingos, 2005), better performance can be obtained by performing relational inference to directly consider the dependence among a set of predictions. For example, McCallum and Wellner (2005) apply a graph partitioning algorithm on a weighted, undirected graph in which vertices are noun phrases and edges are weighted by the pairwise score between noun phrases. A second and less studied difficulty is that the pairwise decomposition restricts the feature set to evidence about pairs of noun phrases only. This restriction can be detrimental if there exist features of sets of noun phrases that cannot be captured by a combination of pairwise features. As a simple example, consider prohibiting coreferent sets that consist only of pronouns. That is, we would like to require that there be at least one antecedent for a set of pronouns. The pairwise decomposition does not make it possible to capture this constraint. In general, we would like to construct arbitrary features over a cluster of noun phrases using the full expressivity of first-order logic. Enabling this sort of flexible representation within a statistical model has been the subject of a long line of research on first-order probabilistic models (Gaifman, 1964; Halpern, 1990; Paskin, 2002; Poole, 2003; Richardson and Domingos, 2006). Conceptually, a first-order probabilistic model can be described quite compactly. A configuration of the world is represented by a set of prediChoosing the closest preceding phrase is common because nearby phrases are a priori more likely to be coreferent. We refer to the training and inference methods described in this section as the Pairwise Model. ", "conclusion": "We have presented learning and inference procedures for coreference models using first-order features. By relying on sampling methods at training time and approximate inference methods at testing time, this approach can be made scalable. This results in a coreference model that can capture features over sets of noun phrases, rather than simply pairs of noun phrases. This is an example of a model with extremely flexible representational power, but for which exact inference is intractable. The simple approximations we have described here have enabled this more flexible model to outperform a model that is simplified for tractability. A short-term extension would be to consider features over entire clusterings, such as the number of clusters. This could be incorporated in a ranking scheme, as in Ng (2005). Future work will extend our approach to a wider variety of tasks. The model we have described here is specific to clustering tasks; however a similar formulation could be used to approach a number of language processing tasks, such as parsing and relation extraction. These tasks could benefit from first-order features, and the present work can guide the approximations required in those domains. Additionally, we are investigating more sophisticated inference algorithms that will reduce the greediness of the search procedures described here. ", "summary_sents": ["Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases.", "In this paper, we propose a machine learning method that enables features over sets of noun phrases, resulting in a first-order probabilistic model for coreference.", "We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases.", "This result demonstrates an example of how a first-order logic representation can be incorporated into a probabilistic model and scaled efficiently.", "We present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not.", "We introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities."]}
{"title": "Deterministic Parsing Of Syntactic Non-Fluencies", "abstract": "128 24% 161 29% 47 9% 148 27% 32 6% 17 3% 11 2% 6. Discussion the rules for Fidditch are written as deterministic pattern-action rules of the same sort as the rules in the parsing grammar, their operation is in a sense isolable. The patterns of the self-correction rules are checked first, before any of the grammar rule patterns are checked, at each step in the parse. Despite this independence in terms of rule ordering, the operation of the self-corr,:ction component is closely tied to the grammar of the parser; for it is the parsing grammar that specifies what sort of constituents count as the same for copying. example, if the grammar did not treat a noun phrase when it is subject of a sentence, the self-correction rules could not properly resolve a sentence like People-a people from Kennsington the editing rules would never recognize that the same sort of element. (Note (13) treated as a Restart because lexical trigger is not present.) Thus, the observed pattern of self-correction introduces empirical constraints on the set of features that are available for syntactic rules. The self-correction rules impose constraints not only on what linguistic elements must count as the same, but also on what must count as different. For example, in sentence be recognized as different sorts of elements in the grammar for the AUX node to be correctly the grammar assigned the words exactly the same part of speech, then the Category C;.7y Editor necessarily apply, incorrectly expunging (14) Kid could-be a brain in school. It appears therefore that the pattern of self-corrections that occur represents a potentially rich source of evidence about the nature of syntactic categories. the patterns of self-correction count as about the nature of categories for the linguist, then this data must be equally available to the language learner. This would suggest that, far from being an impediment to language learning, non-fluencies may in fact facilitate language acquisition by highlighting equivalent classes. expunction of edit signal only surface copy category copy stack copy restart failures remaining unclear and ungrammatical 127 This raises the general question of how children can acquire a language in the face of unrestrained non-fluency. How can a language learner sort out the grammatical from the ungrammatical strings? (The non-fluencies of speech are of course but one aspect of the degeneracy of input that makes language acquisition a puzzle.) The self-correction system I have described suggests that many non-fluent strings can be resolved with little detailed linguistic knowledge. As Table 1 shows, about a quarter of the editing signals result in expunction of only non-linguistic material. This requires only an ability to distinguish linguistic from nonlinguistic stuff, and it introduces the idea that edit signals signal an expunction site. Almost a third are resolved by the Surface Copying rule, which can be viewed simply as an instance of the general non-linguistic rule that multiple instances of the same thing count as a single instance. The category copying rules are generalizations of simple copying, applied to a knowledge of linguistic categories. Making the transition from surface copies to category copies is aided by the fact that there is considerable overlap in coverage, defining a path of expanding generalization. Thus at the earliest stages of learning, only the simplest, non-linguistic self-correction rules would come into play, and gradually the more syntactically integrated would be acquired. Contrast this self-correction system to an approach that handles non-fluencies by some general problem solving routines, for example Granger (1982), who proposes reasoning from what a speaker might be expected to say. Besides the obvious inefficiencies of general problem solving approaches, it is worth giving special emphasis to the problem with learnability. A general problem solving approach depends crucially on evaluating the likelihood of possible deviations from the norms. But a language learner has by definition only partial and possibly incorrect knowledge of the syntax, and is therefore unable to consistently identify deviations from the grammatical system. With the editing system I describe, the learner need not have the ability to recognize deviations from grammatical norms, but merely the non-linguistic ability to recognize copies of the same thing. far, I have considered the selfcorrection component from the standpoint of parsing. However, it is clear that the origins are in the process of generation. The mechanism for editing self-corrections that I have proposed has as its essential operation expunging one two identical is unable to expunge a sequence of two elements. (The Surface Copy Editor might be viewed as a counterexample to this claim, but see below.) Consider expunction now from the standpoint of the generator. Suppose self-correction bears a one-to-one relationship to a possible action of the generator (initiated by some monitoring component) which could be called ABANDON CONSTRUCT X. And suppose that this action can be initiated at any time up until CONSTRUCT X is completed, when a signal is returned that the construction is complete. Further suppose that ABANDON CONSTRUCT X causes an editing signal. When the speaker decides in the middle of some linguistic element to abandon it and start again, an editing signal is produced. If this is an appropriate model, then the elements which are self-corrected should be exactly those elements that xist at some stage in the generation process. Thus, we should be able to find evidence for the units involved in generation by looking at the data of self-correction. And indeed, such evidence should be available to the language learner as well. Summary I have described the nature of self-corrected speech (which is a major source of spoken non-fluencies) and how it can be resolved by simple editing rules within the context of a deterministic parser. Two features are essential to the self-correction system: 1) every self-correction site (whether it results in the expunction of words or not) is marked by a phonetically identifiable signal placed at the right edge of the potential expunction site; and 2) the expunged part is the left-hand member of a pair of copies, one on each side of the editing signal. The copies may be of three types: 1) identical surface strings, which are edited by a matching rule that applies before syntactic analysis begins; 2) complete constituents, when two constituents of the same type appear in the parser's buffer; or 3) incomplete constituents, when the parser finds itself trying to complete a constituent of the same type as a constituent it has just completed. Whenever two such copies appear in such a configuration, and the first one ends with an editing signal, the first is expunged from further analysis. This editing system has been implemented as part of a deterministic parser, and tested on a wide range of sentences from transcribed speech. Further study of the self-correction system promises to provide insights into the units of production and the nature of linguistic categories. ", "introduction": "", "conclusion": "", "summary_sents": ["It is often remarked that natural language, used naturally, is unnaturally ungrammatical.", "Spontaneous speech contains all manner of false starts, hesitations, and self-corrections that disrupt the well-formedness of strings.", "It is a mystery then, that despite this apparent wide deviation from grammatical norms, people have little difficulty understanding the non-fluent speech that is the essential medium of everyday life.", "And it is a still greater mystery that children can succeed in acquiring the grammar of a language on the basis of evidence provided by a mixed set of apparently grammatical and ungrammatical strings.", "We address the problem of correcting self repairs by adding rules to a deterministic parser that would remove the necessary text.", "We define a typology of repairs and associated correction strategies in terms of extensions to a deterministic parser."]}
{"title": "Learning Multilingual Subjective Language via Cross-Lingual Projections", "abstract": "This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English. Given a bridge between English and the selected target language (e.g., a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language. ", "introduction": "There is growing interest in the automatic extraction of opinions, emotions, and sentiments in text (subjectivity), to provide tools and support for various natural language processing applications. Most of the research to date has focused on English, which is mainly explained by the availability of resources for subjectivity analysis, such as lexicons and manually labeled corpora. In this paper, we investigate methods to automatically generate resources for subjectivity analysis for a new target language by leveraging on the resources and tools available for English, which in many cases took years of work to complete. Specifically, through experiments with cross-lingual projection of subjectivity, we seek answers to the following questions. First, can we derive a subjectivity lexicon for a new language using an existing English subjectivity lexicon and a bilingual dictionary? Second, can we derive subjectivity-annotated corpora in a new language using existing subjectivity analysis tools for English and a parallel corpus? Finally, third, can we build tools for subjectivity analysis for a new target language by relying on these automatically generated resources? We focus our experiments on Romanian, selected as a representative of the large number of languages that have only limited text processing resources developed to date. Note that, although we work with Romanian, the methods described are applicable to any other language, as in these experiments we (purposely) do not use any language-specific knowledge of the target language. Given a bridge between English and the selected target language (e.g., a bilingual dictionary or a parallel corpus), the methods can be applied to other languages as well. After providing motivations, we present two approaches to developing sentence-level subjectivity classifiers for a new target language. The first uses a subjectivity lexicon translated from an English one. The second uses an English subjectivity classifier and a parallel corpus to create target-language training data for developing a statistical classifier. ", "conclusion": "In this paper, we described two approaches to generating resources for subjectivity annotations for a new language, by leveraging on resources and tools available for English. The first approach builds a target language subjectivity lexicon by translating an existing English lexicon using a bilingual dictionary. The second generates a subjectivity-annotated corpus in a target language by projecting annotations from an automatically annotated English corpus. These resources were validated in two ways. First, we carried out annotation studies measuring the extent to which subjectivity is preserved across languages in each of the two resources. These studies show that only a relatively small fraction of the entries in the lexicon preserve their subjectivity in the translation, mainly due to the ambiguity in both the source and the target languages. This is consistent with observations made in previous work that subjectivity is a property associated not with words, but with word meanings (Wiebe and Mihalcea, 2006). In contrast, the sentence-level subjectivity was found to be more reliably preserved across languages, with cross-lingual inter-annotator agreements comparable to the monolingual ones. Second, we validated the two automatically generated subjectivity resources by using them to build a tool for subjectivity analysis in the target language. Specifically, we developed two classifiers: a rulebased classifier that relies on the subjectivity lexicon described in Section 3.1, and a machine learning classifier trained on the subjectivity-annotated corpus described in Section 4.1. While the highest precision for the subjective classification is obtained with the rule-based classifier, the overall best result of 67.85 F-measure is due to the machine learning approach. This result is consistent with the annotation studies, showing that the corpus projections preserve subjectivity more reliably than the lexicon translations. Finally, neither one of the classifiers relies on language-specific information, but rather on knowledge obtained through projections from English. A similar method can therefore be used to derive tools for subjectivity analysis in other languages. ", "summary_sents": ["This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English.", "Given a bridge between English and the selected target language (e.g., a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language.", "We discuss different shortcomings of lexicon-based translation scheme for the more semantic-oriented task subjective analysis. Instead, we proposed to use a parallel-corpus, apply the classifier in the source language and use the corresponding sentences in the target language to train a new classifier.", "We use a bilingual lexicon and a manually translated parallel corpus to generate a sentence classifier according to their level of subjectivity for Romanian."]}
{"title": "A Classifier-Based Parser With Linear Run-Time Complexity", "abstract": "We present a classifier-based parser that produces constituent trees in linear time. The parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar. This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing. We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers. We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively. ", "introduction": "Two classifier-based deterministic dependency parsers for English have been proposed recently (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003). Although they use different parsing algorithms, and differ on whether or not dependencies are labeled, they share the idea of greedily pursuing a single path, following parsing decisions made by a classifier. Despite their greedy nature, these parsers achieve high accuracy in determining dependencies. Although state-of-the-art statistical parsers (Collins, 1997; Charniak, 2000) are more accurate, the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast, light-weight parsing, or parsing of large amounts of data. However, dependency analyses lack important information contained in constituent structures. For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002). We present a parser that shares much of the simplicity and efficiency of the deterministic dependency parsers, but produces both dependency and constituent structures simultaneously. Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time. While it may seem that the larger search space of constituent trees (compared to the space of dependency trees) would make it unlikely that accurate parse trees could be built deterministically, we show that the precision and recall of constituents produced by our parser are close to those produced by statistical parsers with higher run-time complexity. One desirable characteristic of our parser is its simplicity. Compared to other successful approaches to corpus-based constituent parsing, ours is remarkably simple to understand and implement. An additional feature of our approach is its modularity with regard to the algorithm and the classifier that determines the parser\u2019s actions. This makes it very simple for different classifiers and different sets of features to be used with the same parser with very minimal work. Finally, its linear runtime complexity allows our parser to be considerably faster than lexicalized PCFG-based parsers. On the other hand, a major drawback of the classifier-based parsing framework is that, depending on node (NP) with four children. In the transformed tree, internal structure (marked by nodes with asterisks) was added to the subtree rooted by the node with more than two children. The word \u201cdog\u201d is the head of the original NP, and it is kept as the head of the transformed NP, as well as the head of each NP* node. the classifier used, its training time can be much longer than that of other approaches. Like other deterministic parsers (and unlike many statistical parsers), our parser considers the problem of syntactic analysis separately from partof-speech (POS) tagging. Because the parser greedily builds trees bottom-up in one pass, considering only one path at any point in the analysis, the task of assigning POS tags to words is done before other syntactic analysis. In this work we focus only on the processing that occurs once POS tagging is completed. In the sections that follow, we assume that the input to the parser is a sentence with corresponding POS tags for each word. ", "conclusion": "We have presented a simple shift-reduce parser that uses a classifier to determine its parsing actions and runs in linear time. Using SVMs for classification, the parser has labeled constituent precision and recall higher than 87% when using the correct part-of-speech tags, and slightly higher than 86% when using automatically assigned partof-speech tags. Although its accuracy is not as high as those of state-of-the-art statistical parsers, our classifier-based parser is considerably faster than several well-known parsers that employ search or dynamic programming approaches. At the same time, it is significantly more accurate than previously proposed deterministic parsers for constituent structures. We have also shown that much of the success of a classifier-based parser depends on what classifier is used. While this may seem obvious, the differences observed here are much greater than what would be expected from looking, for example, at results from chunking/shallow parsing (Zhang et al., 2001; Kudo and Matsumoto, 2001; Veenstra and van den Bosch, 2000). Future work includes the investigation of the effects of individual features, the use of additional classification features, and the use of different classifiers. In particular, the use of tree features seems appealing. This may be accomplished with SVMs using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004). Additionally, we plan to investigate the use of the beam strategy of Ratnaparkhi (1997) to pursue multiple parses while keeping the run-time linear. ", "summary_sents": ["We present a classifier-based parser that produces constituent trees in linear time.", "The parser uses a basic bottom-up shift-reduce algorithm, but employs a classifier to determine parser actions instead of a grammar.", "This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing.", "We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers.", "We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.", "we propose a constituency based parsing method to determine sentence dependency structures."]}
{"title": "Monotonic Semantic Interpretation", "abstract": "hiyan@cam. sri . corn ralcam. sri. corn ABSTRACT Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved. ", "introduction": "", "conclusion": "", "summary_sents": ["Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations.", "The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation.", "Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.", "We make use of Quasi Logical Form, a monotonic representation for compositional semantics.", "A quasi logical form allows the under-specification of several types of information, such as anaphoric references, ellipsis and semantic relations."]}
{"title": "A Maximum Entropy Word Aligner For Arabic-English Machine Translation", "abstract": "This paper presents a maximum entropyword alignment algorithm for Arabic English based on supervised training data.We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of su pervised and unsupervised methods yields superior performance. The probabilisticmodel used in the alignment directly models the link decisions. Significant improvement over traditional word alignment tech niques is shown as well as improvement onseveral machine translation tests. Perfor mance of the algorithm is contrasted with human annotation performance. ", "introduction": "Machine translation takes a source sequence, S = [s1 s2 . . . sK ] and generates a target sequence, T = [t1 t2 . . . tM ] that renders the meaning of the source sequence into the target sequence. Typically, algorithms operate on sentences. In the most general setup, one or more source words can generate 0, 1 or more target words. Current state of the art machine translation systems (Och, 2003) use phrasal (n-gram) features extracted automatically from parallel corpora. These phrases are extracted using word alignment algorithms that are trained on parallel corpora. Phrases, or phrasal features, represent a mapping of source sequences into a target sequences which are typically a few words long.In this paper, we investigate the feasibility of training alignment algorithms based on supervised alignment data. Although there is a modest cost associ ated with annotating data, we show that a reduction of 40% relative in alignment error (AER) is possible over the GIZA++ aligner (Och and Ney, 2003). Although there are a number of other applications for word alignment, for example in creating bilingual dictionaries, the primary application continues to be as a component in a machine translation system. We test our aligner on several machine translation tests and show encouraging improvements. ", "conclusion": "This paper presented a word aligner trained on anno tated data. While the performance of the aligner isshown to be significantly better than other unsuper vised algorithms, the utility of these alignments in machine translation is still an open subject although gains are shown in two of the test sets. Since featuresare extracted from a parallel corpus, most of the in formation relating to the specific sentence alignment is lost in the aggregation of features across sentences. Improvements in capturing sentence context could allow the machine translation system to use a rare but correct link appropriately. Another significant result is that a small amount (5K sentences) of word-aligned data is sufficient for this algorithm since a provision is made to handle 95 Figure 3: An example sentence with human output on the left and system output on the right. unknown words appropriately. ", "summary_sents": ["This paper presents a maximum entropy word alignment algorithm for Arabic-English based on supervised training data.", "We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance.", "The probabilistic model used in the alignment directly mod-els the link decisions.", "Significant improvement over traditional word alignment techniques is shown as well as improvement on several machine translation tests.", "Performance of the algorithm is contrasted with human annotation performance.", "We present a discriminatively trained 1-to-N model with feature functions specifically designed for Arabic.", "We train a discriminative model on a corpus of ten thousand word aligned Arabic-English sentence pairs that outperforms a GIZA++ baseline."]}
{"title": "Discriminative Sentence Compression With Soft Syntactic Evidence", "abstract": "We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers. The parsers are trained out-of-domain and contain a significant amount of noise. We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly. This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters. ", "introduction": "The ability to compress sentences grammatically with minimal information loss is an important problem in text summarization. Most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate. Thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted. We focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005). In this framework, the goal is to find the shortest substring of the original sentence that conveys the most important aspects of the meaning. We will work in a supervised learning setting and assume as input a training set T=(xt,yt)|? | t\ufffd1 of original sentences xt and their compressions yt. We use the Ziff-Davis corpus, which is a set of 1087 pairs of sentence/compression pairs. Furthermore, we use the same 32 testing examples from Knight and Marcu (2000) and the rest for training, except that we hold out 20 sentences for the purpose of development. A handful of sentences occur twice but with different compressions. We randomly select a single compression for each unique sentence in order to create an unambiguous training set. Examples from this data set are given in Figure 1. Formally, sentence compression aims to shorten a sentence x = x1 ... xn into a substring y = y1 ... ym, where yi E {x1, ... , xn}. We define the function I(yi) E {1, ... , n} that maps word yi in the compression to the index of the word in the original sentence. Finally we include the constraint I(yi) < I(yi+1), which forces each word in x to occur at most once in the compression y. Compressions are evaluated on three criteria, Typically grammaticality and importance are traded off with compression rate. The longer our compressions, the less likely we are to remove important words or phrases crucial to maintaining grammaticality and the intended meaning. The paper is organized as follows: Section 2 discusses previous approaches to sentence compression. In particular, we discuss the advantages and disadvantages of the models of Knight and Marcu (2000). In Section 3 we present our discriminative large-margin model for sentence compression, including the learning framework and an efficient decoding algorithm for searching the space of compressions. We also show how to extract a rich feature set that includes surfacelevel bigram features of the compressed sentence, dropped words and phrases from the original sentence, and features over noisy dependency and phrase-structure trees for the original sentence. We argue that this rich feature set allows the model to learn which words and phrases should be dropped and which should remain in the compression. Section 4 presents an experimental evaluation of our model compared to the models of Knight and Marcu (2000) and finally Section 5 discusses some areas of future work. ", "conclusion": "", "summary_sents": ["We present a model for sentence compression that uses a discriminative large-margin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.", "The parsers are trained out-of-domain and contain a significant amount of noise.", "We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly.", "This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters.", "We provide a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint.", "We use the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words.", "We use semi-Markov model which allows incorporating a language model for the compression."]}
{"title": "The Language Of Bioscience: Facts Speculations And Statements In Between", "abstract": "We explore the use of speculative language in MEDLINE abstracts. Results from a manual annotation experiment suggest that the notion of speculative sentence can be reliably annotated by humans. In addition, an experiment with automated methods also suggest that reliable automated methods might also be developed. Distributional observations are also presented as well as a discussion of possible uses for a system that can recognize speculative language. ", "introduction": "The scientific process involves making hypotheses, gathering evidence, using inductive reasoning to reach a conclusion based on the data, and then making new hypotheses. Scientist are often not completely certain of a conclusion. This lack of definite belief is often reflected in the way scientists discuss their work. In this paper, we focus on expressions of levels of belief: the expressions of hypotheses, tentative conclusions, hedges, and speculations. \u201cAffect\u201d is used in linguistics as a label for this topic. This is not a well-known topic in the field of text processing of bioscience literature. Thus, we present a large number of examples to elucidate the variety and nature of the phenomena. We then return to a discussion of the goals, importance, and possible uses of this research. The sentences in the following box contain fragments expressing a relatively high level of speculation. The level of belief expressed by an author is often difficult to ascertain from an isolated sentence and often the context of the abstract is needed. All examples in the paper are from abstracts available at the Nation Library of Medicine PubMed webpage (currently http://www.ncbi.nlm.nih.gov/PubMed/). The PubMed identifier is provided following each sentence. Pdcd4 may thus constitute a useful molecular target for cancer prevention. (1131400) As the GT box has also previously been shown to play a role in gene regulation of other genes, these newly isolated Sp2 and Sp3 proteins might regulate expression not only of the TCR gene but of other genes as well. (1341900) On the basis of these complementary results, it has been concluded that curcumin shows very high binding to BSA, probably at the hydrophobic cavities inside the protein. (12870844) Curcumin down-regulates Ki67, PCNA and mutant p53 mRNAs in breast cancer cells, these properties may underlie chemopreventive action. (14532610) The next examples contain fragments that are speculative but probably less so than those above. (As we will discuss later, it is difficult to agree on levels of speculation.) The containing sentence does sibility. The examples above are speculative and the sentence below expresses a definite statement about two possibilities. provide some context but the rest of the abstract if not the full text is often necessary along with enough knowledge of field to understand text. Removal of the carboxy terminus enables ERP to interact with a variety of ets-binding sites including the E74 site, the IgH enhancer pi site, and the lck promoter ets site, suggesting a carboxy-terminal negative regulatory domain. (7909357) In addition, we show that a component of the Ras-dependent mitogen-activated protein kinase pathway, nerve growth factor-inducible c-Jun, exerts its effects on receptor gene promoter activity most likely through protein-protein interactions with Sp1. (11262397) Results suggest that one of the mechanisms of curcumin inhibition of prostate cancer may be via inhibition ofAkt. (12682902) The previous examples contain phrases such as most likely and suggesting, which in these cases, explicitly mark a level of belief less than 100%. The next examples are not as explicitly marked: to date and such as can also be used in purely definite statements. To date, we find that the signaling pathway triggered by each type of insult is distinct. (10556169) However, the inability of IGF-1, insulin and PMA to stimulate 3beta-HSD type 1 expression by themselves in the absence of IL-4 indicates that the multiple pathways downstream ofIRS-1 and IRS-2 must act in cooperation with an IL4-specific signaling molecule, such as the transcription factor Stat6. (11384880) These findings highlight the feasibility of modulating HO-1 expression during hypothermic storage to confer tissues a better protection to counteract the damage characteristic of organ transplantation. (12927811) The words may and might were both used to express speculation in the examples above but are ambiguous between expressing speculation versus posThe level of LFB1 binding activity in adenoidcystic as well as trabecular tumours shows some variation and may either be lower or higher than in the non-tumorous tissue. (7834800) The sentence below involves the adjective putative in an apositive noun phrase modifier, a different syntactic form that in the previous examples. It also clearly shows that the speculative portion is often confined to only a part of the information provided in a sentence. We report here the isolation ofhuman zinc finger 2 (HZF2), a putative zinc-finger transcription factor, by motif-directed differential display of mRNA extracted from histamine-stimulated human vein endothelial cells. (11121585) Of course, definite sentences also come in a variety. The definite sentences below vary in topic and form. Affinity chromatography and coimmunoprecipitation assays demonstrated that c-Jun and T-Ag physically interact with each other. (12692226) However, NF-kappaB was increased at 3 h while AP-1 (Jun B and Jun D) and CREB were increased at 15 h. (10755711) We studied the transcript distribution of c-jun, junB and junD in the rat brain. (1719462) An inclusive model for all steps in the targeting ofproteins to subnuclear sites cannot yet be proposed. (11389536) We have been talking about speculative fragments and speculative sentences. For the rest of the paper, we define a speculative sentence to be one that contains at least one speculative fragment. A definite sentence contains no speculative fragments. In this study we only considered annotations at the sentence level. However, in future work, we plan to work on sub-sentential annotations. Our general goal is to investigate speculative speech in bioscience literature and explore how it might be used in HLT applications for bioscientists. A more specific goal is to investigate the use of speculative speech in MEDLINE abstracts because of their accessibility. There are a number of reasons supporting the importance of understanding speculative speech: In the following, we expand upon these points in the contexts of i) information retrieval, ii) information extraction, and iii) knowledge discovery. In the context of information retrieval, an example information need might be \u201cI am looking for speculations about the X gene in liver tissue.\u201d One of the authors spoke at a research department of a drug company and the biologists present expressed this sort of information need. On the other hand, one of the authors has also encountered the opposite need: \u201cI am looking for definite statements about transcription factors that interact with NF Kappa B.\u201d Both these information needs would be easier to fulfill if automated annotation of speculative passages was possible. In the context of information extraction, a similar situation exists. For example, extracting tables of protein-protein interactions would benefit from knowing which interactions were speculative and which were definite. In the context of knowledge discovery (KR), speculation might play a number of roles. One possibility would be to use current speculative statements about a topic of interest as a seed for the automated knowledge discovery process. For example, terms could be extracted from speculative fragments and used to guide the initial steps of the knowledge discovery process. A less direct but perhaps even more important use is in building test/train datasets for knowledge discovery systems. For example, let us assume that in a 1985 publication we find a speculation about two topics/concepts A and C being related and later in a 1995 document there is a definite statement declaring that A and C are connected via B. This pair of statements can then form the basis of a discovery problem. We may use it to test a KR system\u2019s ability to predict B as the connecting aspect between A and C and to do this using data prior to the 1995 publication. The same example could also be used differently: KR systems could be assessed on their ability to make a speculation between A and C using data up to 1985 excluding the particular publication making the speculation. In this way such pairs of temporally ordered speculative-definite statements may be of value in KR research. Differentiating between speculative and definite statements is one part of finding such statement pairs. ", "conclusion": "The work presented here is preliminary but promising. It seems that the notion of speculative sentence can be characterized enabling manual annotation. However, we did not manage to characterize the distinction between high and low speculation. In addition, it seems likely that automated systems will be able to achieve useful accuracy. Finally, abstracts seem to include a fair amount of speculative information. Future work concerning manual annotation would include revising the guidelines, throwing out the High vs. Low distinction, annotating more data, annotating sub-sentential units, annotating the focus of the speculation (e.g., a gene), and annotating full text articles. We are also ignorant of work in linguistics that almost certainly exists and may be informative. We have started this process by considering (Hyland, 1998) and (Harris et al., 1989). Future work concerning automatic annotation includes expanding the substring system with more substrings and perhaps more complicated regular expressions, expanding the feature set of the SVM, trying out other classification methods such as decision trees. Finally, we plan on building some of the applications mentioned: a speculation search engine, transcription factor interaction tables with a speculation/definite column, and knowledge discovery test sets. ", "summary_sents": ["We explore the use of speculative language in MEDLINE abstracts.", "Results from a manual annotation experiment suggest that the notion of speculative sentence can be reliably annotated by humans.", "In addition, an experiment with automated methods also suggest that reliable automated methods might also be developed.", "Distributional observations are also presented as well as a discussion of possible uses for a system that can recognize speculative language.", "we focus on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, and present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts.", "we explore issues with annotating speculative language in biomedicine and outline potential applications.", "we present a study on annotating hedges in biomedical documents."]}
{"title": "Towards Answering Opinion Questions: Separating Facts From Opinions And Identifying The Polarity Of Opinion Sentences", "abstract": "Opinion question answering is a challenging task for natural language processing. In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level. We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level. We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion. Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy). ", "introduction": "Newswire articles include those that mainly present opinions or ideas, such as editorials and letters to the editor, and those that mainly report facts such as daily news articles. Text materials from many other sources also contain mixed facts and opinions. For many natural language processing applications, the ability to detect and classify factual and opinion sentences offers distinct advantages in deciding what information to extract and how to organize and present this information. For example, information extraction applications may target factual statements rather than subjective opinions, and summarization systems may list separately factual information and aggregate opinions according to distinct perspectives. At the document level, information retrieval systems can target particular types of articles and even utilize perspectives in focusing queries (e.g., filtering or retrieving only editorials in favor of a particular policy decision). Our motivation for building the opinion detection and classification system described in this paper is the need for organizing information in the context of question answering for complex questions. Unlike questions like \u201cWho was the first man on the moon?\u201d which can be answered with a simple phrase, more intricate questions such as \u201cWhat are the reasons for the US-Iraq war?\u201d require long answers that must be constructed from multiple sources. In such a context, it is imperative that the question answering system can discriminate between opinions and facts, and either use the appropriate type depending on the question or combine them in a meaningful presentation. Perspective information can also help highlight contrasts and contradictions between different sources\u2014there will be significant disparity in the material collected for the question mentioned above between Fox News and the Independent, for example. Fully analyzing and classifying opinions involves tasks that relate to some fairly deep semantic and syntactic analysis of the text. These include not only recognizing that the text is subjective, but also determining who the holder of the opinion is, what the opinion is about, and which of many possible positions the holder of the opinion expresses regarding that subject. In this paper, we are presenting three of the components of our opinion detection and organization subsystem, which have already been integrated into our larger question-answering system. These components deal with the initial tasks of classifying articles as mostly subjective or objective, finding opinion sentences in both kinds of articles, and determining, in general terms and without reference to a specific subject, if the opinions are positive or negative. The three modules of the system discussed here provide the basis for ongoing work for further classification of opinions according to subject and opinion holder and for refining the original positive/negative attitude determination. We review related work in Section 2, and then present our document-level classifier for opinion or factual articles (Section 3), three implemented techniques for detecting opinions at the sentence level (Section 4), and our approach for rating an opinion as positive or negative (Section 5). We have evaluated these methods using a large collection of news articles without additional annotation (Section 6) and an evaluation corpus of 400 sentences annotated for opinion classifications (Section 7). The results, presented in Section 8, indicate that we achieve very high performance (more than 97%) at document-level classification and respectable performance (86\u201391%) at detecting opinion sentences and classifying them according to orientation. ", "conclusion": "We presented several models for distinguishing between opinions and facts, and between positive and negative opinions. At the document level, a fairly straightforward Bayesian classifier using lexical information can distinguish between mostly factual and mostly opinion documents with very high precision and recall (F-measure of 97%). The task is much harder at the sentence level. For that case, we described three novel techniques for opinion/fact classification achieving up to 91% precision and recall on the detection of opinion sentences. We also examined an automatic method for assigning polarity information to single words and sentences, accurately discriminating between positive, negative, and neutral opinions in 90% of the cases. Our work so far has focused on characterizing opinions and facts in a generic manner, without examining who the opinion holder is or what the opinion is about. While we have found presenting information organized in separate opinion and fact classes useful, our goal is to introduce further analysis of each sentence so that opinion sentences can be linked to particular perspectives on a specific subject. We intend to cluster together sentences from the same perspective and present them in summary form as answers to subjective questions. ", "summary_sents": ["Opinion question answering is a challenging task for natural language processing.", "In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level.", "We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level.", "We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion.", "Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).", "At sentence level, we propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences."]}
{"title": "The Interaction Of Knowledge Sources In Word Sense Disambiguation", "abstract": "Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial intelligence research. An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results. We present a sense tagger which uses several knowledge sources. Tested accuracy exceeds 94% on our evaluation corpus. Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words. It is argued that this approach is more likely to assist the creation of practical systems. ", "introduction": "", "conclusion": "", "summary_sents": ["Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial intelligence research.", "An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results.", "We present a sense tagger which uses several knowledge sources.", "Tested accuracy exceeds 94% on our evaluation corpus", "Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words.", "It is argued that this approach is more likely to assist the creation of practical systems.", "We present a classifier combination framework where disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al, 1999).", "We use Longman Dictionary of Contemporary English (LDOCE) as sense inventory.", "We use POS tags of the focus word itself to aid sense disambiguations related to syntactic differences.", "We suggest that use of both syntactic and lexical features will improve disambiguation accuracies."]}
{"title": "Parsing As Deduction", "abstract": "By exploring the relationship between parsing and deduction, a new and more general view of chart parsing is obtained, which encompasses parsing for grammar formalisms based on unification, and is the basis of the Earley Deduction proof procedure for definite clauses. efficiency of this approach for an interesting class grammars is discussed. ", "introduction": "", "conclusion": "", "summary_sents": ["By exploring the relationship between parsing and deduction, a new and more general view of chart parsing is obtained, which encompasses parsing for grammar formalisms based on unification, and is the basis of the Earley Deduction proof procedure for definite clauses.", "The efficiency of this approach for an interesting class of grammars is discussed.", "We extend Earley deduction work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters.", "We present versions of Earley's algorithm for unification grammars, in which unification is the sole operation responsible for attribute valuation."]}
{"title": "Tagging English Text With A Probabilistic Model", "abstract": "In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined: \u2022 using text that has been tagged by hand and computing relative frequency counts, \u2022 using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle. Experiments show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available. ", "introduction": "", "conclusion": "", "summary_sents": ["In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence.", "The main novelty of these experiments is the use of untagged text in the training of the model.", "We have used a simple triclass Marlcov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided.", "Two approaches in particular are compared and combined:", "using text that has been tagged by hand and computing relative frequency counts,", "using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.", "Experiments show that the best training is obtained by using as much tagged text as possible.", "They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy.", "In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.", "we attempted to improve HMM POS tagging by expectation maximization with unlabeled data.", "we introduced the still standard procedure of using a bigram Hidden Markov Model (HMM) trained via Expectation Maximization.", "In the context of POS tagging, we introduce a method that he calls maximum likelihood tagging."]}
{"title": "Improved Automatic Keyword Extraction Given More Linguistic Knowledge", "abstract": "In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed. The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on (such as term frequency and grams), a better result is obtained as measured by keywords previously assigned by professional indexers. In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied. ", "introduction": "Automatic keyword assignment is a research topic that has received less attention than it deserves, considering keywords\u2019 potential usefulness. Keywords may, for example, serve as a dense summary for a document, lead to improved information retrieval, or be the entrance to a document collection. However, relatively few documents have keywords assigned, and therefore finding methods to automate the assignment is desirable. A related research area is that of terminology extraction (see e.g., Bourigault et al. (2001)), where all terms describing a domain are to be extracted. The aim of keyword assignment is to find a small set of terms that describes a specific document, independently of the domain it belongs to. However, the latter may very well benefit from the results of the former, as appropriate keywords often are of a terminological character. In this work, the automatic keyword extraction is treated as a supervised machine learning task, an approach first proposed by Turney (2000). Two important issues are how to define the potential terms, and what features of these terms are considered discriminative, i.e., how to represent the data, and consequently what is given as input to the learning algorithm. In this paper, experiments with three term selection approaches are presented: n-grams; noun phrase (NP) chunks; and terms matching any of a set of part-of-speech (POS) tag sequences. Four different features are used: term frequency, collection frequency, relative position of the first occurrence, and the POS tag(s) assigned to the term. ", "conclusion": "", "summary_sents": ["In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.", "The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on statistics (such as term frequency and n-grams), a better result is obtained as measured by keywords previously assigned by professional indexers.", "In more detail, extracting NP-chunks gives a better precision than n-grams, and by adding the POS tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.", "We propose a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results."]}
{"title": "Thumbs Up Or Thumbs Down? Semantic Orientation Applied To Unsupervised Classification Of Reviews", "abstract": "This paper presents a simple unsupervised learning algorithm for classifying reviews up) or recdown). The classification of a review is predicted by the orientation the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \u201csubtle nuances\u201d) and a negative semantic orientation when it has bad associations (e.g., \u201cvery cavalier\u201d). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \u201cexcellent\u201d minus the mutual information between the given phrase and the word \u201cpoor\u201d. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews. ", "introduction": "If you are considering a vacation in Akumal, Mexico, you might go to a search engine and enter the query \u201cAkumal travel review\u201d. However, in this case, Google1 reports about 5,000 matches. It would be useful to know what fraction of these matches recommend Akumal as a travel destination. With an algorithm for automatically classifying a review as \u201cthumbs up\u201d or \u201cthumbs down\u201d, it would be possible for a search engine to report such summary statistics. This is the motivation for the research described here. Other potential applications include recognizing \u201cflames\u201d (abusive newsgroup messages) (Spertus, 1997) and developing new kinds of search tools (Hearst, 1992). In this paper, I present a simple unsupervised learning algorithm for classifying a review as recommended or not recommended. The algorithm takes a written review as input and produces a classification as output. The first step is to use a part-of-speech tagger to identify phrases in the input text that contain adjectives or adverbs (Brill, 1994). The second step is to estimate the semantic orientation of each extracted phrase (Hatzivassiloglou & McKeown, 1997). A phrase has a positive semantic orientation when it has good associations (e.g., \u201cromantic ambience\u201d) and a negative semantic orientation when it has bad associations (e.g., \u201chorrific events\u201d). The third step is to assign the given review to a class, recommended or not recommended, based on the average semantic orientation of the phrases extracted from the review. If the average is positive, the prediction is that the review recommends the item it discusses. Otherwise, the prediction is that the item is not recommended. The PMI-IR algorithm is employed to estimate the semantic orientation of a phrase (Turney, 2001). PMI-IR uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words or phrases. The semantic orientation of a given phrase is calculated by comparing its similarity to a positive reference word (\u201cexcellent\u201d) with its similarity to a negative reference word (\u201cpoor\u201d). More specifically, a phrase is assigned a numerical rating by taking the mutual information between the given phrase and the word \u201cexcellent\u201d and subtracting the mutual information between the given phrase and the word \u201cpoor\u201d. In addition to determining the direction of the phrase\u2019s semantic orientation (positive or negative, based on the sign of the rating), this numerical rating also indicates the strength of the semantic orientation (based on the magnitude of the number). The algorithm is presented in Section 2. Hatzivassiloglou and McKeown (1997) have also developed an algorithm for predicting semantic orientation. Their algorithm performs well, but it is designed for isolated adjectives, rather than phrases containing adjectives or adverbs. This is discussed in more detail in Section 3, along with other related work. The classification algorithm is evaluated on 410 reviews from Epinions2, randomly sampled from four different domains: reviews of automobiles, banks, movies, and travel destinations. Reviews at Epinions are not written by professional writers; any person with a Web browser can become a member of Epinions and contribute a review. Each of these 410 reviews was written by a different author. Of these reviews, 170 are not recommended and the remaining 240 are recommended (these classifications are given by the authors). Always guessing the majority class would yield an accuracy of 59%. The algorithm achieves an average accuracy of 74%, ranging from 84% for automobile reviews to 66% for movie reviews. The experimental results are given in Section 4. The interpretation of the experimental results, the limitations of this work, and future work are discussed in Section 5. Potential applications are outlined in Section 6. Finally, conclusions are presented in Section 7. ", "conclusion": "This paper introduces a simple unsupervised learning algorithm for rating a review as thumbs up or down. The algorithm has three steps: (1) extract phrases containing adjectives or adverbs, (2) estimate the semantic orientation of each phrase, and (3) classify the review based on the average semantic orientation of the phrases. The core of the algorithm is the second step, which uses PMI-IR to calculate semantic orientation (Turney, 2001). In experiments with 410 reviews from Epinions, the algorithm attains an average accuracy of 74%. It appears that movie reviews are difficult to classify, because the whole is not necessarily the sum of the parts; thus the accuracy on movie reviews is about 66%. On the other hand, for banks and automobiles, it seems that the whole is the sum of the parts, and the accuracy is 80% to 84%. Travel reviews are an intermediate case. Previous work on determining the semantic orientation of adjectives has used a complex algorithm that does not readily extend beyond isolated adjectives to adverbs or longer phrases (Hatzivassiloglou and McKeown, 1997). The simplicity of PMI-IR may encourage further work with semantic orientation. The limitations of this work include the time required for queries and, for some applications, the level of accuracy that was achieved. The former difficulty will be eliminated by progress in hardware. The latter difficulty might be addressed by using semantic orientation combined with other features in a supervised classification algorithm. ", "summary_sents": ["This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down).", "The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs.", "A phrase has a positive semantic orientation when it has good associations (e.g., \u201csubtle nuances\u201d) and a negative semantic orientation when it has bad associations (e.g., \u201cvery cavalier\u201d).", "In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \u201cexcellent\u201d minus the mutual information between the given phrase and the word \u201cpoor\u201d.", "A review is classified as recommended if the average semantic orientation of its phrases is positive.", "The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations).", "The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.", "We describe a way to automatically build a lexicon based on looking at co-occurrences of words with other words whose sentiment is known."]}
{"title": "A Prosodic Analysis Of Discourse Segments In Direction-Giving Monologues", "abstract": "This paper reports on corpus-based research into the relationship between intonational variation and discourse structure. We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship. We also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment. ", "introduction": "This paper presents empirical support for the assumption long held by computational linguists, that intonation can provide valuable cues for discourse processing. The relationship between intonational variation and discourse structure has been explored in a new corpus of direction-giving monologues. We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship. We also compare the acousticprosodic features of initial, medial, and final utterances in a discourse segment. A better understanding of the role of intonation in conveying discourse structure will enable improvements in the naturalness of intonational variation in text-to-speech systems as well as in algorithms for recognizing discourse structure in speech-understanding systems. ", "conclusion": "Although this paper reports results from only a single speaker, the findings are promising. We have demonstrated that a theory-based method for discourse analysis can provide reliable segmentations of spontaneous as well as read speech. In addition, the availability of speech in the text-and-speech labeling method led to significantly higher reliability scores. The stronger correlations found for intonational features of the text-and-speech labelings suggest not only that discourse labelers make use of prosody in their analyses, but also that obtaining such data can lead to more robust modeling of the relationship between intonation and discourse structure. The following preliminary results can be considered for incorporation in such a model. First, segment-initial utterances differ from medial and final utterances in both prominence and rhythmic properties. Segment-medial and segment-final utterances are distinguished more clearly by rhythmic features, primarily pause. Finally, all correlations found for global parameters can also be computed based on relative change in acoustic-prosodic parameters in a window of two phrases. Ongoing research is addressing the development of automatic classification algorithms for discourse boundary type; the role of prosody in conveying hierarchical relationships among discourse segments; individual speaker differences; and discourse segmentation methods that can be used by naive subjects. ", "summary_sents": ["This paper reports on corpus-based research into the relationship between intonational variation and discourse structure.", "We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship.", "We also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment.", "We find that speech is able to improve inter-annotator agreement in discourse segmentation of monologues.", "We introduce the Boston Directions Corpus, a publicly available speech corpora with manual ToBI annotations intended for experiments in automatic prosody labeling."]}
{"title": "Robust Pronoun Resolution with Limited Knowledge", "abstract": "Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledgebased system, however, is that it is a very labourintensive and time-consuming task. This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the success rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be successfully adapted for other languages with minimum modifications. ", "introduction": "", "conclusion": "", "summary_sents": ["Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.", "One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and time-consuming task.", "This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.", "Input is checked against agreement and for a number of antecedent indicators.", "Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.", "Evaluation reports a success rate of 89.7% which is better than the success rates of the approaches selected for comparison and tested on the same data.", "In addition, preliminary experiments show that the approach can be successfully adapted for other languages with minimum modifications.", "We first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.", "We find that the current evaluation of anaphora resolution algorithms and systems is befeft of any common ground for comparison due to the difference in evaluation data as well as the diversity of pre-processing tools employed by each anaphora resolution system."]}
{"title": "Named Entity Recognition in Tweets: An Experimental Study", "abstract": "People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to recognition. Our novel doubles compared with the NER system. the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms coincreasing 25% over ten common entity types. NLP tools are available at: ", "introduction": "Status Messages posted on Social Media websites such as Facebook and Twitter present a new and challenging style of text for language technology due to their noisy and informal nature. Like SMS (Kobus et al., 2008), tweets are particularly terse and difficult (See Table 1). Yet tweets provide a unique compilation of information that is more upto-date and inclusive than news articles, due to the low-barrier to tweeting, and the proliferation of mobile devices.1 The corpus of tweets already exceeds the size of the Library of Congress (Hachman, 2011) and is growing far more rapidly. Due to the volume of tweets, it is natural to consider named-entity recognition, information extraction, and text mining over tweets. Not surprisingly, the performance of \u201coff the shelf\u201d NLP tools, which were trained on news corpora, is weak on tweet corpora. In response, we report on a re-trained \u201cNLP pipeline\u201d that leverages previously-tagged out-ofdomain text, 2 tagged tweets, and unlabeled tweets to achieve more effective part-of-speech tagging, chunking, and named-entity recognition. 1 The Hobbit has FINALLY started filming! I cannot wait! 2 Yess! Yess! Its official Nintendo announced today that they Will release the Nintendo 3DS in north America march 27 for $250 3 Government confirms blast n nuclear plants n japan...don\u2019t knw wht s gona happen nw... We find that classifying named entities in tweets is a difficult task for two reasons. First, tweets contain a plethora of distinctive named entity types (Companies, Products, Bands, Movies, and more). Almost all these types (except for People and Locations) are relatively infrequent, so even a large sample of manually annotated tweets will contain few training examples. Secondly, due to Twitter\u2019s 140 character limit, tweets often lack sufficient context to determine an entity\u2019s type without the aid of background knowledge. To address these issues we propose a distantly supervised approach which applies LabeledLDA (Ramage et al., 2009) to leverage large amounts of unlabeled data in addition to large dictionaries of entities gathered from Freebase, and combines information about an entity\u2019s context across its mentions. We make the following contributions: LabeledLDA is applied, utilizing constraints based on an open-domain database (Freebase) as a source of supervision. This approach increases F1 score by 25% relative to co-training (Blum and Mitchell, 1998; Yarowsky, 1995) on the task of classifying named entities in Tweets. The rest of the paper is organized as follows. We successively build the NLP pipeline for Twitter feeds in Sections 2 and 3. We first present our approaches to shallow syntax \u2013 part of speech tagging (\u00a72.1), and shallow parsing (\u00a72.2). \u00a72.3 describes a novel classifier that predicts the informativeness of capitalization in a tweet. All tools in \u00a72 are used as features for named entity segmentation in \u00a73.1. Next, we present our algorithms and evaluation for entity classification (\u00a73.2). We describe related work in \u00a74 and conclude in \u00a75. ", "conclusion": "", "summary_sents": ["People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner.", "The performance of standard NLP tools is severely degraded on tweets.", "This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition.", "Our novel T-NER system doubles F1 score compared with the Stanford NER system.", "T-NER leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision.", "LabeledLDA outperforms co-training, increasing F1 by 25% over ten common entity types.", "Our NLP tools are available at: http:// github.com/aritter/twitter_nlp", "We use token unigrams as features, including any hash tags, but ignoring twitter mentions, URLs and purely numeric tokens.", "Our system exploits a CRF model to segment named entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities."]}
{"title": "Automatic Construction Of A Hypernym-Labeled Noun Hierarchy From Text", "abstract": "Previous work has shown that automatic methods can be used in building semantic lexicons. This work goes a step further by automatically creating not just clusters of related words, but a hierarchy of nouns and their hypernyms, akin to the hand-built hierarchy in WordNet. ", "introduction": "The purpose of this work is to build something like the hypernym-labeled noun hierarchy of WordNet (Fellbaum, 1998) automatically from text using no other lexical resources. WordNet has been an important research tool, but it is insufficient for domainspecific text, such as that encountered in the MUCs (Message Understanding Conferences). Our work develops a labeled hierarchy based on a text corpus. In this project, nouns are clustered into a hierarchy using data on conjunctions and appositives appearing in the Wall Street Journal. The internal nodes of the resulting tree are then labeled with hypernyms for the nouns clustered underneath them, also based on data extracted from the Wall Street Journal. The resulting hierarchy is evaluated by human judges, and future research directions are discussed. ", "conclusion": "We have shown that hypernym hierarchies of nouns can be constructed automatically from text with similar performance to semantic lexicons built automatically for hand-selected hypernyms. With the addition of some improvements we have identified, we believe that these automatic methods can be used to construct truly useful hierarchies. Since the hierarchy is learned from sample text, it could be trained on domainspecific text to create a hierarchy that is more applicable to a particular domain than a general-purpose resource such as WordNet. ", "summary_sents": ["Previous work has shown that automatic methods can be used in building semantic lexicons.", "This work goes a step further by automatically creating not just clusters of related words, but a hierarchy of nouns and their hypernyms, akin to the hand-built hierarchy in WordNet.", "We let three judges evaluate ten internal nodes in the hyponym hierarchy that had at least twenty descendants."]}
{"title": "Unsupervised Construction Of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources", "abstract": "We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships. ", "introduction": "The importance of learning to manipulate monolingual paraphrase relationships for applications like summarization, search, and dialog has been highlighted by a number of recent efforts (Barzilay & McKeown 2001; Shinyama et al 2002; Lee & Barzilay 2003; Lin & Pantel 2001). While several different learning methods have been applied to this problem, all share a need for large amounts of data in the form of pairs or sets of strings that are likely to exhibit lexical and/or structural paraphrase alternations. One approach1 1 An alternative approach involves identifying anchor points--pairs of words linked in a known way--and collecting the strings that intervene. (Shinyama, et al 2002; Lin & Pantel 2001). Since our interest is in that has been successfully used is edit distance, a measure of similarity between strings. The assumption is that strings separated by a small edit distance will tend to be similar in meaning: The leading indicators measure the economy? The leading index measures the economy?. Lee & Barzilay (2003), for example, use Multi Sequence Alignment (MSA) to build a corpus of paraphrases involving terrorist acts. Their goal is to extract sentential templates that can be used in high-precision generation of paraphrase alter nations within a limited domain. Our goal here is rather different: our interest lies in constructing a monolingual broad-domain corpus of pairwise aligned sentences. Such data would be amenable to conventional statistical machine translation (SMT) techniques (e.g., those discussed in Och & Ney 2003).2 In what follows we compare two strategies for unsupervised construction of such a corpus, one employing string similarity and the other associating sentences that may overlap very little at the string level. We measure the relative utility of the two derived monolingual corpora in the context of word alignment techniques developed originally for bilingual text. We show that although the edit distance corpus is well-suited as training data for the alignment algorithms currently used in SMT, it is an incomplete source of information about paraphrase relations, which exhibit many of the characteristics of comparable bilingual corpora or free translations. Many of the more complex alternations that characterize monolingual paraphrase, such as large-scale lexical alternations and constituent reorderings, are not readily learning sentence level paraphrases, including major constituent reorganizations, we do not address this approach here. 2 Barzilay & McKeown (2001) consider the possibility of using SMT machinery, but reject the idea because of the noisy, comparable nature of their dataset. captured by edit distance techniques, which conflate semantic similarity with formal similarity. We conclude that paraphrase research would benefit by identifying richer data sources and developing appropriate learning techniques. ", "conclusion": "Edit distance identifies sentence pairs that exhibit lexical and short phrasal alternations that can be aligned with considerable success. Given a large dataset and a well-motivated clustering of documents, useful datasets can be gleaned even without resorting to more sophisticated techniques Figure 2. Sample human-aligned paraphrase L12 F2 Elaboration 0.83 1.3 Phrasal 0.14 0.69 Spelling 0.12 0.01 Synonym 0.18 0.25 Anaphora 0.1 0.13 Reordering 0.02 0.41 Table 2. Mean number of instances of paraphrase phenomena per sentence (such as Multiple Sequence Alignment, as employed by Barzilay & Lee 2003). However, there is a disparity between the kinds of paraphrase alternations that we need to be able to align and those that we can already align well using current SMT techniques. Based solely on the criterion of word AER, the L12 data would seem to be superior to the F2 data as a source of paraphrase knowledge. Hand evaluation, though, indicates that many of the phenomena that we are interested in learning may be absent from this L12 data. String edit distance extraction techniques involve assumptions about the data that are inadequate, but achieve high precision. Techniques like our F2 extraction strategies appear to extract a more diverse variety of data, but yield more noise. We believe that an approach with the strengths of both methods would lead to significant improvement in paraphrase identification and generation. In the near term, however, the relatively similar performances of F2 and L12-trained models on the F2 test data suggest that with further refinements, this more complex type of data can achieve good results. More data will surely help. One focus of future work is to build a classifier to predict whether two sentences are related through paraphrase. Features might include edit distance, temporal/topical clustering information, information about cross-document discourse structure, relative sentence length, and synonymy information. We believe that this work has potential impact on the fields of summarization, information retrieval, and question answering. Our ultimate goal is to apply current SMT techniques to the problems of paraphrase recognition and generation. We feel that this is a natural extension of the body of recent developments in SMT; perhaps explorations in monolingual data may have a reciprocal impact. The field of SMT, long focused on closely aligned data, is only now beginning to address the kinds of problems immediately encountered in monolingual paraphrase (including phrasal translations and large scale reorderings). Algorithms to address these phenomena will be equally applicable to both fields. Of course a broad-domain SMT-influenced paraphrase solution will require very large corpora of sentential paraphrases. In this paper we have described just one example of a class of data extraction techniques that we hope will scale to this task. Acknowledgements We are grateful to the Mo Corston-Oliver, Jeff Stevenson and Amy Muia of the Butler Hill Group for their work in annotating the data used in the experiments. We have also benefited from discussions with Ken Church, Mark Johnson, Daniel Marcu and Franz Och. We remain, however, responsible for all content. ", "summary_sents": ["We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.", "Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster.", "We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation.", "Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set.", "On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively.", "Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase.", "The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.", "we introduce Microsoft Research Paraphrase Corpus (MSRPC).", "We use Web-aggregated news stories to learn both sentence-level and word-level alignments."]}
{"title": "CDER: Efficient MT Evaluation Using Block Movements", "abstract": "Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks. In many cases though such movements still result in correct or almost correct sentences. In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation. Our measure can be exactly calculated in quadratic time. Furthermore, we will show how some evaluation measures can be improved ", "introduction": "Research in machine translation (MT) depends heavily on the evaluation of its results. Especially for the development of an MT system, an evaluation measure is needed which reliably assesses the quality of MT output. Such a measure will help analyze the strengths and weaknesses of different translation systems or different versions of the same system by comparing output at the sentence level. In most applications of MT, understandability for humans in terms of readability as well as semantical correctness should be the evaluation criterion. But as human evaluation is tedious and cost-intensive, automatic evaluation measures are used in most MT research tasks. A high correlation between these automatic evaluation measures and human evaluation is thus desirable. State-of-the-art measures such as BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) aim at measuring the translation quality rather on the document level1 than on the level of single sentences. They are thus not well-suited for sentence-level evaluation. The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. In this paper, we will present a new automatic error measure for MT \u2013 the CDER \u2013 which is designed for assessing MT quality on the sentence level. It is based on edit distance \u2013 such as the well-known word error rate (WER) \u2013 but allows for reordering of blocks. Nevertheless, by defining reordering costs, the ordering of the words in a sentence is still relevant for the measure. In this, the new measure differs significantly from the position independent error rate (PER) by (Tillmann et al., 1997). Generally, finding an optimal solution for such a reordering problem is NP hard, as is shown in (Lopresti and Tomkins, 1997). In previous work, researchers have tried to reduce the complexity, for example by restricting the possible permutations on the block-level, or by approximation or heuristics during the calculation. Nevertheless, most of the resulting algorithms still have high run times and are hardly applied in practice, or give only a rough approximation. An overview of some better-known measures can be found in Section 3.1. In contrast to this, our new measure can be calculated very efficiently. This is achieved by requiring complete and disjoint coverage of the blocks only for the reference sentence, and not for the candidate translation. We will present an algorithm which computes the new error measure in quadratic time. The new evaluation measure will be investigated and compared to state-of-the-art methods on two translation tasks. The correlation with human assessment will be measured for several different statistical MT systems. We will see that the new measure significantly outperforms the existing approaches. As a further improvement, we will introduce word dependent substitution costs. This method will be applicable to the new measure as well as to established measures like WER and PER. Starting from the observation that the substitution of a word with a similar one is likely to affect translation quality less than the substitution with a completely different word, we will show how the similarity of words can be accounted for in automatic evaluation measures. This paper is organized as follows: In Section 2, we will present the state of the art in MT evaluation and discuss the problem of block reordering. Section 3 will introduce the new error measure CDER and will show how it can be calculated efficiently. The concept of worddependent substitution costs will be explained in Section 4. In Section 5, experimental results on the correlation of human judgment with the CDER and other well-known evaluation measures will be presented. Section 6 will conclude the paper and give an outlook on possible future work. ", "conclusion": "We presented CDER, a new automatic evaluation measure for MT, which is based on edit distance extended by block movements. CDER allows for reordering blocks of words at constant cost. Unlike previous block movement measures, CDER can be exactly calculated in quadratic time. Experimental evaluation on two different translation tasks shows a significantly improved correlation with human judgment in comparison with state-of-the-art measures such as BLEU. Additionally, we showed how word-dependent substitution costs can be applied to enhance the new error measure as well as existing approaches. The highest correlation with human assessment was achieved through linear interpolation of the new CDER with PER. Future work will aim at finding a suitable length penalty for CDER. In addition, more sophisticated definitions of the word-dependent substitution costs will be investigated. Furthermore, it will be interesting to see how this new error measure affects system development: We expect it to allow for a better sentence-wise error analysis. For system optimization, preliminary experiments have shown the need for a suitable length penalty. ", "summary_sents": ["Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks.", "In many cases though such movements still result in correct or almost correct sentences.", "In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation.", "Our measure can be exactly calculated in quadratic time.", "We consider edit distance for word substitution and reordering.", "Our CDER measure is based on edit distance, such as the well-known WER, but allows reordering of blocks."]}
{"title": "Structural Ambiguity And Lexical Relations", "abstract": "We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus. This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning. ", "introduction": "", "conclusion": "", "summary_sents": ["We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus.", "This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning.", "We are the first to show that a corpus-based approach to PP attachment ambiguity resolution can lead to good results.", "We propose one of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions.", "We used a partial parser to extract (v, n, p) tuples from a corpus, where p is the preposition whose attachment is ambiguous between the verb v and the noun n."]}
{"title": "A Syntax-Directed Translator With Extended Domain Of Locality", "abstract": "SD translation schema (synchronous grammar) (string relation) A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language. We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear framework in order to rescore with other fealike language models. We devise a simple-yet-effective algorithm to non-duplicate translations rescoring. Initial experimental results on English-to-Chinese translation are presented. ", "introduction": "The concept of syntax-directed (SD) translation was originally proposed in compiling (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a tree representation that guides the generation of the object code. Following Aho and Ullman (1972), a translation, as a set of string pairs, can be specified by a syntax-directed translation schema (SDTS), which is essentially a synchronous context-free grammar (SCFG) that generates two languages simultaneously. An SDTS also induces a translator, a device that performs the transformation from input string to output string. In this context, an SD translator consists of two components, a sourcelanguage parser and a recursive converter which is usually modeled as a top-down tree-to-string transducer (G\u00b4ecseg and Steinby, 1984). The relationship among these concepts is illustrated in Fig. 1. This paper adapts the idea of syntax-directed translator to statistical machine translation (MT). We apply stochastic operations at each node of the source-language parse-tree and search for the best derivation (a sequence of translation steps) that converts the whole tree into some target-language string with the highest probability. However, the structural divergence across languages often results in nonisomorphic parse-trees that is beyond the power of SCFGs. For example, the S(VO) structure in English is translated into a VSO word-order in Arabic, an instance of complex reordering not captured by any SCFG (Fig. 2). To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree. For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the non-isomorphic tree pair in Fig. 2. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of onelevel context-free productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since an SD translator separates the sourcelanguage analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFGbased Treebank parser but focuses on the extended domain in the recursive converter. Following Galley et al. (2004), we use a special class of extended tree-to-string transducer (zRs for short) with multilevel left-hand-side (LHS) trees.1 Since the righthand-side (RHS) string can be viewed as a flat onelevel tree with the same nonterminal root from LHS (Fig. 2), this framework is closely related to STSGs: they both have extended domain of locality on the source-side, while our framework remains as a CFG on the target-side. For instance, an equivalent zRs rule for the complex reordering in Fig. 2 would be While Section 3 will define the model formally, we first proceed with an example translation from English to Chinese (note in particular that the inverted phrases between source and target): 1Throughout this paper, we will use LHS and source-side interchangeably (so are RHS and target-side). In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al. (1993). Figure 3 shows how the translator works. The English sentence (a) is first parsed into the tree in (b), which is then recursively converted into the Chinese string in (e) through five steps. First, at the root node, we apply the rule r1 which preserves the toplevel word-order and translates the English period into its Chinese counterpart: Then, the rule r2 grabs the whole sub-tree for \u201cthe gunman\u201d and translates it as a phrase: (r2) NP-C ( DT (the) NN (gunman) ) \u2014* qiangshou Now we get a \u201cpartial Chinese, partial English\u201d sentence \u201cqiangshou VP o\u201d as shown in Fig. 3 (c). Our recursion goes on to translate the VP sub-tree. Here we use the rule r3 for the passive construction: which captures the fact that the agent (NP-C, \u201cthe police\u201d) and the verb (VBN, \u201ckilled\u201d) are always inverted between English and Chinese in a passive voice. Finally, we apply rules r\ufffd and r5 which perform phrasal translations for the two remaining subtrees in (d), respectively, and get the completed Chinese string in (e). ", "conclusion": "This paper presents an adaptation of the classic syntax-directed translation with linguisticallymotivated formalisms for statistical MT. Currently we are doing larger-scale experiments. We are also investigating more principled algorithms for integrating n-gram language models during the search, rather than k-best rescoring. Besides, we will extend this work to translating the top k parse trees, instead of committing to the 1-best tree, as parsing errors certainly affect translation quality. ", "summary_sents": ["A syntax-directed translator first parses the source-language input into a parse tree, and then recursively converts the tree into a string in the target-language.", "We model this conversion by an extended tree-to-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility.", "We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.", "The model is then extended to the general log-linear frame work in order to rescore with other features like n-gram language models.", "We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring.", "Initial experimental results on English-to-Chinese translation are presented.", "We study a TSG-based tree-to-string alignment model.", "We define the Extended Tree-to-String Transducer."]}
{"title": "A Maximum Entropy Model For Part-Of-Speech Tagging", "abstract": "This paper presents a statistical model which trains from a corpus annotated with Part-Of- Speech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%). The can be classified as a Entropy model and simultaneously uses many contextual &quot;features&quot; to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems. ", "introduction": "Many natural language tasks require the accurate assignment of Part-Of-Speech (POS) tags to previously unseen text. Due to the availability of large corpora which have been manually annotated with POS information, many taggers use annotated text to &quot;learn&quot; either probability distributions or rules and use them to automatically assign POS tags to unseen text. The experiments in this paper were conducted on the Wall Street Journal corpus from the Penn Treebank project(Marcus et al., 1994), although the model can train from any large corpus annotated with POS tags. Since most realistic natural language applications must process words that were never seen before in training data, all experiments in this paper are conducted on test data that include unknown words. Several recent papers(Brill, 1994, Magerman, 1995) have reported 96.5% tagging accuracy on the Wall St. Journal corpus. The experiments in this paper test the hypothesis that better use of context will improve the accuracy. A Maximum Entropy model is well-suited for such experiments since it cornbines diverse forms of contextual information in a principled manner, and does not impose any distributional assumptions on the training data. Previous uses of this model include language modeling(Lau et al., 1993), machine translation(Berger et al., 1996), prepositional phrase attachment(Ratnaparkhi et al., 1994), and word morphology(Della Pietra et al., 1995). This paper briefly describes the maximum entropy and maximum likelihood properties of the model, features used for POS tagging, and the experiments on the Penn Treebank Wall St. Journal corpus. It then discusses the consistency problems discovered during an attempt to use specialized features on the word context. Lastly, the results in this paper are compared to those from previous work on POS tagging. The Probability Model The probability model is defined over It x T, where fl is the set of possible word and tag contexts, or &quot;histories&quot;, and T is the set of allowable tags. The model's probability of a history h together with a tag t is defined as: where ir is a normalization constant, fp, cu,. , a} are the positive model parameters and { , fk} are known as &quot;features&quot;, where fj (h, t) E {OM. Note that each parameter aj corresponds to a feature fj. Given a sequence of words {w1, , w} and tags {t1, .tn} as training data, define hi as the history available when predicting ti. The parameters {p, ai , \u2022 .. , } are then chosen to maximize the likelihood of the training data using p: This model also can be interpreted under the Maximum Entropy formalism, in which the goal is to maximize the entropy of a distribution subject to certain constraints. Here, the entropy of the distribution p is defined as: where the model's feature expectation is and the observed feature expectation is Ef =Epoi,ti) and where (h1, t1) denotes the observed probability of (hi , ti) in the training data. Thus the constraints force the model to match its feature expectations with those observed in the training data. In practice, I-1 is very large and the model's expectation E fi cannot be computed directly, so the following approximation(Lau et al., 1993) is used: where /3(h1) is the observed probability of the history hi in the training set. It can be shown (Darroch and Ratcliff, 1972) that if p has the form (1) and satisfies the k constraints (2), it uniquely maximizes the entropy H (p) over distributions that satisfy (2), and uniquely maximizes the likelihood L(p) over distributions of the form (1). The model parameters for the distribution p are obtained via Generalized Iterative Sca/ing(Darroch and Ratcliff, 1972). The joint probability of a history h and tag t is determined by those parameters whose corresponding features are active, i.e., those aj such that f (h,t) = 1. A feature, given (h,t), may activate on any word or tag in the history h, and must encode any information that might help predict t, such as the spelling of the current word, or the identity of the previous two tags. The specific word and tag context available to a feature is given in the following definition of a history hi: If the above feature exists in the feature set of the model, its corresponding model parameter will contribute towards the joint probability p(hi,ti) when wi ends with &quot;ing&quot; and when ti =VBG1. Thus a model parameter aj effectively serves as a &quot;weight&quot; for a certain contextual predictor, in this case the suffix &quot;ing&quot;, towards the probability of observing a certain tag, in this case a VBG. The model generates the space of features by scanning each pair (hi ,ti) in the training data with the feature &quot;templates&quot; given in Table 1. Given hi as the current history, a feature always asks some yes/no question about hi, and furthermore constrains ti to be a certain tag. The instantiations for the variables X, Y, and T in Table 1 are obtained automatically from the training data. The generation of features for tagging unknown words relies on the hypothesized distinction that &quot;rare&quot; words' in the training set are similar to unknown words in test data, with respect to how their spellings help predict their tags. The rare word features in Table 1, which look at the word spellings, will apply to both rare words and unknown words in test data. For example, Table 2 contains an excerpt from training data while Table 3 contains the features generated while scanning (h3, t3), in which the current word is about, and Table 4 contains features generated while scanning (h4, 14), in which the current word, well-heeled, occurs 3 times in training data and is therefore classified as &quot;rare&quot;. The behavior of a feature that occurs very sparsely in the training set is often difficult to predict, since its statistics may not be reliable. Therefore, the model uses the heuristic that any feature Condition Features wi is not rare wi = X wi is rare Xis prefix of wi, IXI <4 & ti = T X is suffix of wi, IXI < 4 wi contains number & ti = T wi contains uppercase character & t\u2022 = T wi contains hyphen & ti = T which occurs less than 10 times in the data is unreliable, and ignores features whose counts are less than 10.3 While there are many smoothing algorithms which use techniques more rigorous than a simple count cutoff, they have not yet been investigated in conjunction with this tagger. ", "conclusion": "The Maximum Entropy model is an extremely flexible technique for linguistic modelling, since it can use a virtually unrestricted and rich feature set in the framework of a probability model. The implementation in this paper is a state-of-the-art POS tagger, as evidenced by the 96.6% accuracy on the unseen Test set, shown in Table 11. The model with specialized features does not perform much better than the baseline model, and further discovery or refinement of word-based features is difficult given the inconsistencies in the training data. A model trained and tested on data from a single annotator performs at .5% higher accuracy than the baseline model and should produce more consistent input for applications that require tagged text. ", "summary_sents": ["This paper presents a statistical model which trains from a corpus annotated with Part-Of-Speech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%).", "The model can be classified as a Maximum Entropy model and simultaneously uses many contextual \"features\" to predict the POS tag.", "Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.", "We assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words.", "We release a publicly available maximum entropy tagger."]}
{"title": "D-Tree Grammars", "abstract": "designed to share some of the advantages of TAG while overcoming some its limitations. two composition operations called subsertion and sister-adjunction. The most distinctive feaof that, unlike TAG, there is complete uniformity in the way that the relate lexical items: subsertion always corresponds to complementation and sister-adjunction to modi- Furthermore, unlike TAG, provide a uniform analysis for whmovement in English and Kashmiri, despite the fact that the wh element in Kashmiri appears in sentence-second position, and not sentence-initial position as in English. We define a new grammar formalism, called D-Tree arises from work on Tree- Adjoining Grammars (TAG) (Joshi et al., 1975). A salient feature of TAG is the extended domain of locality it provides. Each elementary structure can be associated with a lexical item (as in Lexicalized (LTAG) (Joshi & 1991)). Properties related to the lexical item (such as subcategorization, agreement, certain types of word order variation) can be expressed within the elementary struc- (Kroch, 1987; Frank, 1992). In addition, remain tractable, yet their generative capacity is sufficient to account for certain syntactic phenomena that, it has been argued, lie beyond Context-Free Grammars (CFG) (Shieber, 1985). TAG, however, has two limitations which provide the motivation for this The first problem (discussed in Section that the of substitution and adjunction do not map cleanly onto the relations of complementation and modification. A second problem (discussed in Section 1.2) has to do with the of provide analyses for certain syntactic phenomena. In developing DTG we have tried to overcome these problems while remaining faithto what we see as the key advantages of particular, its enlarged domain of locality). In Section 1.3 we introduce some of the key features of explain how they are intended to address problems that we have identified with 1.1 Derivations and Dependencies operations of substitution and adjunction relate two lexical items. It is therefore natural to interpret these operations as establishing a direct linguistic relation between the two lexical items, namely a relation of complementation (predicateargument relation) or of modification. In purely CFG-based approaches, these relations are only implicit. However, they represent important linguistic intuition, they provide a uniform interface to semantics, and they are, as Schabes & Shieber (1994) argue, important in order to support statistical parameters in stochastic frameworks and appropriate constraints in many frameworks, complementation and modification are in fact made & Kaplan, 1982) provides a separate functional (f-) structure, and dependency grammars (see e.g. Mel'euk (1988)) use these notions as the principal basis for syntactic representation. We will follow the dependency literature in referring to complementation and modification as syntactic dependency. As observed by Rambow and Joshi (1992), for TAG, the importance of the dependency structure means that not only the derived phrase-structure tree is of interest, but also the operations by which we obtained it from elementary structures. This information is encoded in the derivation tree (Vijay-Shanker, 1987). However, as Vijay-Shanker (1992) observes, the operations are not used uniformly: while substitution is used only to add a (nominal) complement, adjunction is used both for modification and (clausal) complementation. Clausal complementation could not be handled uniformly by substitution because of the existence of syntactic such as in English. Furthermore, there is an inconsistency in 151 the directionality of the operations used for complementation in TAG@: nominal complements are substituted into their governing verb's tree, while the governing verb's tree is adjoined into its own clausal complement. The fact that adjunction and substitution are used in a linguistically heterogeneous manner means that (standard) TAG derivation trees do not provide a good representation of the dependencies between the words of the sentence, i.e., of the predicate-argument and modification structure. adore SUBJ COMP Figure 1: Derivation trees for (1): original definition (left); Schabes & Shieber definition (right) For instance, English sentence (1) gets the derivation structure shown on the left in Figure 1'. spicy hotdogs he claims Mary seems to adore When comparing this derivation structure to the dependency structure in Figure 2, the following problems become apparent. First, both adjectives deon in the derivation structure is daughter of addition, deon does its nominal argument, on the derivation strucis daughter of direction does express the actual dependency), and also daughter of neither is an argument of the other). claim SUBOMP he seem I COMP adore Mary hotdog </LOD spicy small Figure 2: Dependency tree for (1) Schabes & Shieber (1994) solve the first problem 'For clarity, we depart from standard TAG notational practice and annotate nodes with lexemes and arcs with grammatical function. by distinguishing between the adjunction of modifiers and of clausal complements. This gives us the derivation structure shown on the right in Figure 1. While this might provide a satisfactory treatment of modification at the derivation level, there are now three types of operations (two adjunctions and substitution) for two types of dependencies (arguments and modifiers), and the directionality problem for embedded clauses remains unsolved. defining have attempted to resolve these problems with the use of a single operation (that we call subsertion) for handling all complementation and a second operation (called sisteradjunction) for modification. Before discussion these operations further we consider a second problem with TAG that has implications for the design of these new composition operations (in particular, subsertion). 1.2 Problematic Constructions for TAG be used to provide suitable analyses for certain syntactic phenomena, including longdistance scrambling in German (Becker et al., 1991), Romance Clitics (Bleam, 1994), wh-extraction out of complex picture-NPs (Kroch, 1987), and Kashmiri wh-extraction (presented here). The problem in describing these phenomena with TAG arises from the fact (observed by Vijay-Shanker (1992)) that adjoining is an overly restricted way of combining structu- We illustrate the problem by considering Kashon Bhatt (1994). extraction in Kashmiri proceeds as in English, except that the wh-word ends up in sentence-second position, with a topic from the matrix clause in sentence-initial position. This is illustrated in (2a) for a simple clause and in (2b) for a complex clause. (2) rameshan kyaa dyutnay tse RameshERG whatNom gave youoAr What did you give Ramesh? b. rameshan kyaa, chu baasaan what is believeNPertthat kor 'ERG do What does Ramesh believe that I did? Since the moved element does not appear in sentence-initial position, the TAG analysis of English wit-extraction of Kroch (1987; 1989) (in which the matrix clause is adjoined into the embedded clause) cannot be transferred, and in fact no linguistically plausible TAG analysis appears to be available. In the past, variants of TAG have been developed to extend the range of possible analyses. In Multi-Component TAG (MCTAG) (Joshi, 1987), trees are grouped into sets which must be adjoined to- (multicomponent adjunction). However, MCexpressive power since, while syntactic relations are invariably subject to c-command or dominance constraints, there is no way to state that Mary OBJ hotdog claim SUBJ spicyhe I MOD small adore COMP COMP Mary OBJ seem hotdog claim MOD SUBJ spicy small he COMP seamSUBJ 152 two trees from a set must be in a dominance relain the derived tree. Domination et al., 1991) are multicomponent systems that allow for the expression of dominance constraints. However, MCTAG-DL share a further problem with MCTAG: the derivation structures cannot be given a linguistically meaningful interpretation. Thus, they fail to address the first prowe discussed (in Section 1.3 The DTG Approach Vijay-Shanker (1992) points out that use of adjunction for clausal complementation in TAG corresponds, at the level of dependency structure, to substitution at the foot node' of the adjoined tree. However, adjunction (rather than substitution) is used since, in general, the structure that is substituted may only form part of the clausal complement: the remaining substructure of the clausal complement appears above the root of the adjoined tree. Unfortunately, as seen in the examples given in Section 1.2, there are cases where satisfactory analyses cannot be obtained with adjunction. In particular, using adjunction in this way cannot handle cases in which parts of the clausal complement are required to be placed within the structure of the adjoined tree. of subsertion is designed to overcome this limitation. Subsertion can be viewed as a generalization of adjunction in which components of the clausal complement (the subserted structure) which are not substituted can be interspersed within the structure that is the site of the subsertion. Following earlier work (Becker et al., Vijay-Shanker, 1992), a mechanism involving the use of domination links (d-edges) that ensure that parts of the subserted structure that are not substituted dominate those parts that are. Furthermore, there is a need to constrain the way in which the non-substituted components can be interspersed'. This is done by either using appropriate feature constraints at nodes or by means of subsertion-insertion constraints (see Section 2). We end this section by briefly commenting on the of sister-adjunction. In TAG, modification is performed with adjunction of modifier trees that have a highly constrained form. In particular, the foot nodes of these trees are always daughters of the root and either the leftmost or rightmost frontier nodes. The effect of adjoining a these cases the foot node is an argument node of the lexical anchor. 'This was also observed by Rambow (1994a), where integrity constraint (first defined for an of TAG (Becker etal., 1991)) is defined for a MCTAG-DL version called V-TAG. However, this was found to be insufficient for treating both long-distance scrambling and long-distance topicalization in German. V-TAG retains adjoining (to handle topicalization) for this reason. tree of this form corresponds (almost) exactly to the addition of a new (leftmost or rightmost) subtree below the nede that was the site of the adjunction. this reason, we have equipped an operation (sister-adjunction) that does exactly this and more. From the definition of Section 2 it can be seen that the essential aspects of Schabes & Shieber (1994) treatment for modification, including multiple modifications of a phrase, be captured by using this defining Section 2, we discuss, in Section 3, DTG analyses for the English and Kashmiri data presented in this section. Section 4 briefly algorithms. 2 Definition of D-Tree Grammars d-tree is a tree with two types of domination edges (d-edges) and immediate domination edges (i-edges). D-edges and i-edges express domination and immediate domination relations between nodes. These relations are never rescinded when dtrees are composed. Thus, nodes separated by an i-edge will remain in a mother-daughter relationship throughout the derivation, whereas nodes separated by an d-edge can be equated or have a path of any length inserted between them during a derivation. D-edges and i-edges are not distributed arbitrarily in d-trees. For each internal node, either all of its daughters are linked by i-edges or it has a single daughter that is linked to it by a d-edge. Each node is labelled with a terminal symbol, a nonterminal or the empty string. containing n can be decomposed into n + 1 containing only i-edges. D-trees can be composed using two operations: subsertion and sister-adjunction. When a d-tree is subserted into another d-tree component of a is substituted at a frontier nonterminal node (a node) and all components of a that are above the substituted component are inserted into d-edges above the substituted node or placed above the root node. For example, consider the d-trees a and shown in Figure 3. Note that components are shown as triangles. In the composed d-tree 7 the component a(5) is substituted at a substitution node in /3. The components, a(1), a(2), and a(4) of a above a(5) drift up the path in which runs from the substitution node. These are then into in # or above the root of #. In general, when a component some d-tree a is inserted into a d-edge betnodes and two new d-edges are created, the first of which relates and the root node of the second of which relates the frontier 'Santorini and Mahootian (1995) provide additional evidence against the standard TAG approach to modification from code switching data, which can be accounted for by using sister-adjunction. 153 Figure 3: Subsertion node of a(i) that dominates the substituted comto is possible for components above the substituted node to drift arbitrarily far up the d-tree and distribute themselves within domination edges, or above the root, in any way that is compatible with the domination relationships present in the d-tree. a mechanism called constraints control what can appear within d-edges (see below). The second composition operation involving dtrees is called sister-adjunction. When a d-tree a is at a node ri in a d-tree the comd-tree from the addition to fl of a as a new leftmost or rightmost sub-d-tree below that sister-adjunction involves the addition of exactly one new immediate domination edge and that several sister-adjunctions can occur at the same constraints where d-trees can be sister-adjoined and whether they will be rightor left-sister-adjoined (see below). DTG a four tuple G = , VT S, D) the usual nonterminal and termialphabets, a distinguished nonterand a finite set of DTG said to be each d-tree in the grammar has at least one terminal node. The d-trees of a grammar two additional annotations: subsertion-insertion constraints and sister-adjoining constraints. These will be described below, but first we define simultaneously and subsertion-adjoining trees (SAtrees), which are partial derivation structures that can be interpreted as representing dependency in ", "introduction": "We define a new grammar formalism, called D-Tree Grammars (DTG), which arises from work on TreeAdjoining Grammars (TAG) (Joshi et al., 1975). A salient feature of TAG is the extended domain of locality it provides. Each elementary structure can be associated with a lexical item (as in Lexicalized TAG (LTAG) (Joshi & Schabes, 1991)). Properties related to the lexical item (such as subcategorization, agreement, certain types of word order variation) can be expressed within the elementary structure (Kroch, 1987; Frank, 1992). In addition, TAG remain tractable, yet their generative capacity is sufficient to account for certain syntactic phenomena that, it has been argued, lie beyond Context-Free Grammars (CFG) (Shieber, 1985). TAG, however, has two limitations which provide the motivation for this work. The first problem (discussed in Section 1.1) is that the TAG operations of substitution and adjunction do not map cleanly onto the relations of complementation and modification. A second problem (discussed in Section 1.2) has to do with the inability of TAG to provide analyses for certain syntactic phenomena. In developing DTG we have tried to overcome these problems while remaining faithful to what we see as the key advantages of TAG (in particular, its enlarged domain of locality). In Section 1.3 we introduce some of the key features of DTG and explain how they are intended to address the problems that we have identified with TAG. In LTAG, the operations of substitution and adjunction relate two lexical items. It is therefore natural to interpret these operations as establishing a direct linguistic relation between the two lexical items, namely a relation of complementation (predicateargument relation) or of modification. In purely CFG-based approaches, these relations are only implicit. However, they represent important linguistic intuition, they provide a uniform interface to semantics, and they are, as Schabes & Shieber (1994) argue, important in order to support statistical parameters in stochastic frameworks and appropriate adjunction constraints in TAG. In many frameworks, complementation and modification are in fact made explicit: LFG (Bresnan & Kaplan, 1982) provides a separate functional (f-) structure, and dependency grammars (see e.g. Mel'euk (1988)) use these notions as the principal basis for syntactic representation. We will follow the dependency literature in referring to complementation and modification as syntactic dependency. As observed by Rambow and Joshi (1992), for TAG, the importance of the dependency structure means that not only the derived phrase-structure tree is of interest, but also the operations by which we obtained it from elementary structures. This information is encoded in the derivation tree (Vijay-Shanker, 1987). However, as Vijay-Shanker (1992) observes, the TAG composition operations are not used uniformly: while substitution is used only to add a (nominal) complement, adjunction is used both for modification and (clausal) complementation. Clausal complementation could not be handled uniformly by substitution because of the existence of syntactic phenomena such as long-distance wh-movement in English. Furthermore, there is an inconsistency in the directionality of the operations used for complementation in TAG@: nominal complements are substituted into their governing verb's tree, while the governing verb's tree is adjoined into its own clausal complement. The fact that adjunction and substitution are used in a linguistically heterogeneous manner means that (standard) TAG derivation trees do not provide a good representation of the dependencies between the words of the sentence, i.e., of the predicate-argument and modification structure. When comparing this derivation structure to the dependency structure in Figure 2, the following problems become apparent. First, both adjectives depend on hotdog, while in the derivation structure small is a daughter of spicy. In addition, seem depends on claim (as does its nominal argument, he), and adore depends on seem. In the derivation structure, seem is a daughter of adore (the direction does not express the actual dependency), and claim is also a daughter of adore (though neither is an argument of the other). Schabes & Shieber (1994) solve the first problem 'For clarity, we depart from standard TAG notational practice and annotate nodes with lexemes and arcs with grammatical function. by distinguishing between the adjunction of modifiers and of clausal complements. This gives us the derivation structure shown on the right in Figure 1. While this might provide a satisfactory treatment of modification at the derivation level, there are now three types of operations (two adjunctions and substitution) for two types of dependencies (arguments and modifiers), and the directionality problem for embedded clauses remains unsolved. In defining DTG we have attempted to resolve these problems with the use of a single operation (that we call subsertion) for handling all complementation and a second operation (called sisteradjunction) for modification. Before discussion these operations further we consider a second problem with TAG that has implications for the design of these new composition operations (in particular, subsertion). TAG cannot be used to provide suitable analyses for certain syntactic phenomena, including longdistance scrambling in German (Becker et al., 1991), Romance Clitics (Bleam, 1994), wh-extraction out of complex picture-NPs (Kroch, 1987), and Kashmiri wh-extraction (presented here). The problem in describing these phenomena with TAG arises from the fact (observed by Vijay-Shanker (1992)) that adjoining is an overly restricted way of combining structures. We illustrate the problem by considering Kashmiri drawing on Bhatt (1994). Whextraction in Kashmiri proceeds as in English, except that the wh-word ends up in sentence-second position, with a topic from the matrix clause in sentence-initial position. This is illustrated in (2a) for a simple clause and in (2b) for a complex clause. Since the moved element does not appear in sentence-initial position, the TAG analysis of English wit-extraction of Kroch (1987; 1989) (in which the matrix clause is adjoined into the embedded clause) cannot be transferred, and in fact no linguistically plausible TAG analysis appears to be available. In the past, variants of TAG have been developed to extend the range of possible analyses. In Multi-Component TAG (MCTAG) (Joshi, 1987), trees are grouped into sets which must be adjoined together (multicomponent adjunction). However, MCTAG lack expressive power since, while syntactic relations are invariably subject to c-command or dominance constraints, there is no way to state that two trees from a set must be in a dominance relation in the derived tree. MCTAG with Domination Links (MCTAG-DL) (Becker et al., 1991) are multicomponent systems that allow for the expression of dominance constraints. However, MCTAG-DL share a further problem with MCTAG: the derivation structures cannot be given a linguistically meaningful interpretation. Thus, they fail to address the first problem we discussed (in Section 1.1). Vijay-Shanker (1992) points out that use of adjunction for clausal complementation in TAG corresponds, at the level of dependency structure, to substitution at the foot node' of the adjoined tree. However, adjunction (rather than substitution) is used since, in general, the structure that is substituted may only form part of the clausal complement: the remaining substructure of the clausal complement appears above the root of the adjoined tree. Unfortunately, as seen in the examples given in Section 1.2, there are cases where satisfactory analyses cannot be obtained with adjunction. In particular, using adjunction in this way cannot handle cases in which parts of the clausal complement are required to be placed within the structure of the adjoined tree. The DTG operation of subsertion is designed to overcome this limitation. Subsertion can be viewed as a generalization of adjunction in which components of the clausal complement (the subserted structure) which are not substituted can be interspersed within the structure that is the site of the subsertion. Following earlier work (Becker et al., 1991; Vijay-Shanker, 1992), DTG provide a mechanism involving the use of domination links (d-edges) that ensure that parts of the subserted structure that are not substituted dominate those parts that are. Furthermore, there is a need to constrain the way in which the non-substituted components can be interspersed'. This is done by either using appropriate feature constraints at nodes or by means of subsertion-insertion constraints (see Section 2). We end this section by briefly commenting on the other DTG operation of sister-adjunction. In TAG, modification is performed with adjunction of modifier trees that have a highly constrained form. In particular, the foot nodes of these trees are always daughters of the root and either the leftmost or rightmost frontier nodes. The effect of adjoining a 21n these cases the foot node is an argument node of the lexical anchor. 'This was also observed by Rambow (1994a), where an integrity constraint (first defined for an ID/LP version of TAG (Becker etal., 1991)) is defined for a MCTAG-DL version called V-TAG. However, this was found to be insufficient for treating both long-distance scrambling and long-distance topicalization in German. V-TAG retains adjoining (to handle topicalization) for this reason. tree of this form corresponds (almost) exactly to the addition of a new (leftmost or rightmost) subtree below the nede that was the site of the adjunction. For this reason, we have equipped DTG with an operation (sister-adjunction) that does exactly this and nothing more. From the definition of DTG in Section 2 it can be seen that the essential aspects of Schabes & Shieber (1994) treatment for modification, including multiple modifications of a phrase, can be captured by using this operation4. After defining DTG in Section 2, we discuss, in Section 3, DTG analyses for the English and Kashmiri data presented in this section. Section 4 briefly discusses DTG recognition algorithms. ", "conclusion": "DTG, like other formalisms in the TAG family, is lexicalizable, but in addition, its derivations are themselves linguistically meaningful. In future work we intend to examine additional linguistic data, refining aspects of our definition as needed. We will also study the formal properties of DTG, and complete the design of the Earley style parser. ", "summary_sents": ["DTG are designed to share some of the advantages of TAG while overcoming some of its limitations.", "DTG involve two composition operations called subsertion and sister-adjunction.", "The most distinctive feature of DTG is that, unlike TAG, there is complete uniformity in the way that the two DTG operations relate lexical items: subsertion always corresponds to complementation and sister-adjunction to modification.", "Furthermore, DTG, unlike TAG, can provide a uniform analysis for em wh-movement in English and Kashmiri, despite the fact that the wh element in Kashmiri appears in sentence-second position, and not sentence-initial position as in English.", "In the quest of modeling dependency correctly, we expand weak generative capacity and thus end up with much greater parsing complexity."]}
{"title": "Non-Projective Dependency Parsing in Expected Linear Time", "abstract": "We present a novel transition system for dependency parsing, which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input. Adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score. ", "introduction": "Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Attardi, 2006; Titov and Henderson, 2007). A hallmark of many of these models is that they can be implemented very efficiently. Thus, transition-based parsers normally run in linear or quadratic time, using greedy deterministic search or fixed-width beam search (Nivre et al., 2004; Attardi, 2006; Johansson and Nugues, 2007; Titov and Henderson, 2007), and graph-based models support exact inference in at most cubic time, which is efficient enough to make global discriminative training practically feasible (McDonald et al., 2005a; McDonald et al., 2005b). However, one problem that still has not found a satisfactory solution in data-driven dependency parsing is the treatment of discontinuous syntactic constructions, usually modeled by non-projective dependency trees, as illustrated in Figure 1. In a projective dependency tree, the yield of every subtree is a contiguous substring of the sentence. This is not the case for the tree in Figure 1, where the subtrees rooted at node 2 (hearing) and node 4 (scheduled) both have discontinuous yields. Allowing non-projective trees generally makes parsing computationally harder. Exact inference for parsing models that allow non-projective trees is NP hard, except under very restricted independence assumptions (Neuhaus and Br\u00a8oker, 1997; McDonald and Pereira, 2006; McDonald and Satta, 2007). There is recent work on algorithms that can cope with important subsets of all nonprojective trees in polynomial time (Kuhlmann and Satta, 2009; G\u00b4omez-Rodr\u00b4\u0131guez et al., 2009), but the time complexity is at best O(n6), which can be problematic in practical applications. Even the best algorithms for deterministic parsing run in quadratic time, rather than linear (Nivre, 2008a), unless restricted to a subset of non-projective structures as in Attardi (2006) and Nivre (2007). But allowing non-projective dependency trees also makes parsing empirically harder, because it requires that we model relations between nonadjacent structures over potentially unbounded distances, which often has a negative impact on parsing accuracy. On the other hand, it is hardly possible to ignore non-projective structures completely, given that 25% or more of the sentences in some languages cannot be given a linguistically adequate analysis without invoking non-projective structures (Nivre, 2006; Kuhlmann and Nivre, 2006; Havelka, 2007). Current approaches to data-driven dependency parsing typically use one of two strategies to deal with non-projective trees (unless they ignore them completely). Either they employ a non-standard parsing algorithm that can combine non-adjacent substructures (McDonald et al., 2005b; Attardi, 2006; Nivre, 2007), or they try to recover nonprojective dependencies by post-processing the output of a strictly projective parser (Nivre and Nilsson, 2005; Hall and Nov\u00b4ak, 2005; McDonald and Pereira, 2006). In this paper, we will adopt a different strategy, suggested in recent work by Nivre (2008b) and Titov et al. (2009), and propose an algorithm that only combines adjacent substructures but derives non-projective trees by reordering the input words. The rest of the paper is structured as follows. In Section 2, we define the formal representations needed and introduce the framework of transitionbased dependency parsing. In Section 3, we first define a minimal transition system and explain how it can be used to perform projective dependency parsing in linear time; we then extend the system with a single transition for swapping the order of words in the input and demonstrate that the extended system can be used to parse unrestricted dependency trees with a time complexity that is quadratic in the worst case but still linear in the best case. In Section 4, we present experiments indicating that the expected running time of the new system on naturally occurring data is in fact linear and that the system achieves state-ofthe-art parsing accuracy. We discuss related work in Section 5 and conclude in Section 6. ", "conclusion": "We have presented a novel transition system for dependency parsing that can handle unrestricted non-projective trees. The system reuses standard techniques for building projective trees by combining adjacent nodes (representing subtrees with adjacent yields), but adds a simple mechanism for swapping the order of nodes on the stack, which gives a system that is sound and complete for the set of all dependency trees over a given label set but behaves exactly like the standard system for the subset of projective trees. As a result, the time complexity of deterministic parsing is O(n2) in the worst case, which is rare, but O(n) in the best case, which is common, and experimental results on data from five languages support the conclusion that expected running time is linear in the length of the sentence. Experimental results also show that parsing accuracy is competitive, especially for languages like Czech and Slovene where nonprojective dependency structures are common, and especially with respect to the exact match score, where it has the best reported results for four out of five languages. Finally, the simplicity of the system makes it very easy to implement. Future research will include an in-depth error analysis to find out why the system works better for some languages than others and why the exact match score improves even when the attachment score goes down. In addition, we want to explore alternative oracle functions, which try to minimize the number of swaps by allowing the stack to be temporarily \u201cunsorted\u201d. ", "summary_sents": ["We present a novel transition system for dependency parsing, which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input.", "Adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora.", "Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score.", "We present the projective Stack algorithm."]}
{"title": "Distortion Models For Statistical Machine Translation", "abstract": "In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation. We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations. We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used. We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments. ", "introduction": "A language model is a statistical model that gives a probability distribution over possible sequences of words. It computes the probability of producing a given word w1 given all the words that precede it in the sentence. An n-gram language model is an n-th order Markov model where the probability of generating a given word depends only on the last n \u2212 1 words immediately preceding it and is given by the following equation: where k >= n. N-gram language models have been successfully used in Automatic Speech Recognition (ASR) as was first proposed by (Bahl et al., 1983). They play an important role in selecting among several candidate word realization of a given acoustic signal. N-gram language models have also been used in Statistical Machine Translation (SMT) as proposed by (Brown et al., 1990; Brown et al., 1993). The run-time search procedure used to find the most likely translation (or transcription in the case of Speech Recognition) is typically referred to as decoding. There is a fundamental difference between decoding for machine translation and decoding for speech recognition. When decoding a speech signal, words are generated in the same order in which their corresponding acoustic signal is consumed. However, that is not necessarily the case in MT due to the fact that different languages have different word order requirements. For example, in Spanish and Arabic adjectives are mainly noun post-modifiers, whereas in English adjectives are noun pre-modifiers. Therefore, when translating between Spanish and English, words must usually be reordered. Existing statistical machine translation decoders have mostly relied on language models to select the proper word order among many possible choices when translating between two languages. In this paper, we argue that a language model is not sufficient to adequately address this issue, especially when translating between languages that have very different word orders as suggested by our experimental results in Section 5. We propose a new distortion model that can be used as an additional component in SMT decoders. This new model leads to significant improvements in MT quality as measured by BLEU (Papineni et al., 2002). The experimental results we report in this paper are for Arabic-English machine translation of news stories. We also present a novel method for measuring word order similarity (or differences) between any given pair of languages based on word alignments as described in Section 3. The rest of this paper is organized as follows. Section 2 presents a review of related work. In Section 3 we propose a method for measuring the distortion between any given pair of languages. In Section 4, we present our proposed distortion model. In Section 5, we present some empirical results that show the utility of our distortion model for statistical machine translation systems. Then, we conclude this paper with a discussion in Section 6. ", "conclusion": "We presented a new distortion model that can be integrated with existing phrase-based SMT decoders. The proposed model shows statistically significant improvement over a state-of-the-art phrase-based SMT decoder. We also showed that n-gram language modlations. Output 1 is decoding without the distortion model and (s=4, w=8), which corresponds to 0.4104 BLEU score. Output 2 is decoding with the distortion model and (s=3, w=8), which corresponds to 0.4792 BLEU score. The sentences presented here are much shorter than the average in our test set. The average length of the arabic sentence in the MT03 test set is \u2014 24.7. els are not sufficient to model word movement in translation. Our proposed distortion model addresses this weakness of the n-gram language model. We also propose a novel metric to measure word order similarity (or differences) between any pair of languages based on word alignments. Our metric shows that Chinese-English have a closer word order than Arabic-English. Our proposed distortion model relies solely on word alignments and is conditioned on the source words. The majority of word movement in translation is mainly due to syntactic differences between the source and target language. For example, Arabic is verb-initial for the most part. So, when translating into English, one needs to move the verb after the subject, which is often a long compounded phrase. Therefore, we would like to incorporate syntactic or part-of-speech information in our distortion model. ", "summary_sents": ["In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.", "We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.", "We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used.", "We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.", "Our lexicalized distortion model predicts the jump from the last translated word to the next one, with a class for each possible jump length.", "We find that deterministic word reordering is beyond the scope of optimization and cannot be undone by the decoder."]}
{"title": "Characterizing Structural Descriptions Produced By Various Grammatical Formalisms", "abstract": "We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate. In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages. ", "introduction": "Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism. Little attention, however, has been paid to the structural descriptions that these formalisms can assign to strings, i.e. their strong generative capacity. This aspect of the formalism is both linguistically and computationally important. For example, Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB). The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's. We consider properties of the tree sets generated by CFG's, Tree Adjoining Grammars (TAG's), Head Grammars (HG's), Categorial Grammars (CG's), and IG's. We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths. These two properties of the tree sets are not only linguistically relevant, but also have computational importance. By considering derivation trees, and thus abstracting away from the details of the composition operation and the structures being manipulated, we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER, MCS82-07294 and DCR-84-10413, ARO grant DAA 29-84-9-0027, and DARPA grant N00014-85-K0018. We are very grateful to Tony Kroc.h, Michael Pails, Sunil Shende, and Mark Steedman for valuable discussions. formalisms. It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets. This suggests that by generalizing the notion of context-freeness in CFG's, we can define a class of grammatical formalisms that manipulate more complex structures. In this paper, we outline how such family of formalisms can be defined, and show that like CFG's, each member possesses a number of desirable linguistic and computational properties: in particular, the constant growth property and polynomial recognizability. ", "conclusion": "", "summary_sents": ["We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.", "In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties of their derivation trees.", "We find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammar.", "On the basis of this observation, we describe a class of formalisms which we call Linear Context-Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.", "We introduce Linear context-free rewriting system (LCFRS), which is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases."]}
{"title": "A Program For Aligning Sentences In Bilingual Corpora", "abstract": "Researchers in both machine translation (e.g., al., and bilingual lexicography (e.g., Klavans and Tzoulcermann, 1990) have recently become interested in studying parallel texts, texts such as the Canadian Hansards (parliamentary proceedings) which are available in multiple languages (French and English). This paper describes a method for aligning sentences in these parallel texts, based on a simple statistical model of character lengths. The method was developed and tested on a small trilingual sample of Swiss economic reports. A much larger sample of 90 million words of Canadian Hansards has been aligned and donated to the ACL/DCI. ", "introduction": "", "conclusion": "", "summary_sents": ["Researchers in both machine Iranslation (e.g., Brown et al., 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann, 1990) have recently become interested in studying parallel texts, texts such as the Canadian Hansards (parliamentary proceedings) which are available in multiple languages (French and English).", "This paper describes a method for aligning sentences in these parallel texts, based on a simple statistical model of character lengths.", "The method was developed and tested on a small trilingual sample of Swiss economic reports.", "A much larger sample of 90 million words of Canadian Hansards has been aligned and donated to the ACL/DCI.", "We extract pairs of anchor words such as numbers, proper nouns (organiziation, person, title), dates and monetary information.", "We find that the byte length ratio of target sentence to source sentence is normally distributed.", "We demonstrate the effectiveness of a global alignment dynamic program algorithm where the basic similarity score is based on the difference in sentence lengths, measured in characters."]}
{"title": "Efficient Algorithms For Parsing The DOP Model", "abstract": "Excellent results have been reported for Data- Oriented Parsing (DOP) of natural language texts (Bod, 1993c). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to,a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers. ", "introduction": "The Data-Oriented Parsing (DOP) model has a short, interesting, and controversial history. It was introduced by Reinko Scha (1990), and was then studied by Rens Bod. Unfortunately, Bod (1993c, 1992) was not able to find an efficient exact algorithm for parsing using the model; however he did discover and implement Monte Carlo approximations. He tested these algorithms on a cleaned up version of the ATIS corpus, and achieved some very exciting results, reportedly getting 96% of his test set exactly correct, a huge improvement over previous results. For instance, Bod (1993b) compares these results to Schabes (1993), in which, for short sentences, 30% of the sentences have no crossing brackets (a much easier measure than exact match). Thus, Bod achieves an extraordinary 8-fold error rate reduction. Not surprisingly, other researchers attempted to duplicate these results, but due to a lack of details of the parsing algorithm in his publications, these other researchers were not able to confirm the results (Magerman, Lafferty, personal communication). Even Bod's thesis (Bod, 1995a) does not contain enough information to replicate his results. Parsing using the DOP model is especially difficult. The model can be summarized as a special kind of Stochastic Tree Substitution Grammar (STSG): given a bracketed, labelled training corpus, let every subtree of that corpus be an elementary tree, with a probability proportional to the number of occurrences of that subtree in the training corpus. Unfortunately, the number of trees is in general exponential in the size of the training corpus trees, producing an unwieldy grammar. In this paper, we introduce a reduction of the DOP model to an exactly equivalent Probabilistic Context Free Grammar (PCFG) that is linear in the number of nodes in the training data. Next, we present an algorithm for parsing, which returns the parse that is expected to have the largest number of correct constituents. We use the reduction and algorithm to parse held out test data, comparing these results to a replication of Pereira and Schabes (1992) on the same data. These results are disappointing: the PCFG implementation of the DOP model performs about the same as the Pereira and Schabes method. We present an analysis of the runtime of our algorithm and Bod's. Finally, we analyze Bod's data, showing that some of the difference between our performance and his is due to a fortuitous choice of test data. This paper contains the first published replication of the full DOP model, i.e. using a parser which sums over derivations. It also contains algorithms implementing the model with significantly fewer resources than previously needed. Furthermore, for the first time, the DOP model is compared on the same data to a competing model. The DOP model itself is extremely simple and can be described as follows: for every sentence in a parsed training corpus, extract every subtree. In general, the number of subtrees will be very large, typically exponential in sentence length. Now, use these trees to form a Stochastic Tree Substitution Grammar (STSG). There are two ways to define a STSG: either as a Stochastic Tree Adjoining Grammar (Schabes, 1992) restricted to substitution operations, or as an extended PCFG in which entire trees may occur on the right hand side, instead of just strings of terminals and nonterminals. Given the tree of Figure 1, we can use the DOP model to convert it into the STSG of Figure 2. The numbers in parentheses represent the probabilities. These trees can be combined in various ways to parse sentences. In theory, the DOP model has several advantages over other models. Unlike a PCFG, the use of trees allows capturing large contexts, making the model more sensitive. Since every subtree is included, even trivial ones corresponding to rules in a PCFG, novel sentences with unseen contexts Unfortunately, the number of subtrees is huge; therefore Bod randomly samples 5% of the subtrees, throwing away the rest. This significantly speeds up parsing. There are two existing ways to parse using the DOP model. First, one can find the most probable derivation. That is, there can be many ways a given sentence could be derived from the STSG. Using the most probable derivation criterion, one simply finds the most probable way that a sentence could be produced. Figure 3 shows a simple example STSG. For the string xx, what is the most probable derivation? The parse tree has probability i of being generated by the trivial derivation containing a single tree. This tree corresponds to the most probable derivation of xx. One could try to find the most probable parse tree. For a given sentence and a given parse tree, there are many different derivations that could lead to that parse tree. The probability of the parse tree is the sum of the probabilities of the derivations. Given our example, there are two different ways to generate the parse tree each with probability t, so that the parse tree has probability t . This parse tree is most probable. Bod (1993c) shows how to approximate this most probable parse using a Monte Carlo algorithm. The algorithm randomly samples possible derivations, then finds the tree with the most sampled derivations. Bod shows that the most probable parse yields better performance than the most probable derivation on the exact match criterion. Khalil Sima'an (1996) implemented a version of the DOP model, which parses efficiently by limiting the number of trees used and by using an efficient most probable derivation model. His experiments differed from ours and Bod's in many ways, including his use of a different version of the ATIS corpus; the use of word strings, rather than part of speech strings; and the fact that he did not parse sentences containing unknown words, effectively throwing out the most difficult sentences. Furthermore, Sima'an limited the number of substitution sites for his trees, effectively using a subset of the DOP model. Unfortunately, Bod's reduction to a STSG is extremely expensive, even when throwing away 95% of the grammar. Fortunately, it is possible to find an equivalent PCFG that contains exactly eight PCFG rules for each node in the training data; thus it is 0(n). Because this reduction is so much smaller, we do not discard any of the grammar when using it. The PCFG is equivalent in two senses: first it generates the same strings with the same probabilities; second, using an isomorphism defined below, it generates the same trees with the same probabilities, although one must sum over several PCFG trees for each STSG tree. To show this reduction and equivalence, we must first define some terminology. We assign every node in every tree a unique number, which we will call its address. Let A@k denote the node at address k, where A is the non-terminal labeling that node. We will need to create one new nonterminal for each node in the training data. We will call this non-terminal Ak. We will call nonterminals of this form &quot;interior&quot; non-terminals, and the original non-terminals in the parse trees &quot;exterior&quot;. Let aj represent the number of subtrees headed by the node Aaj. Let a represent the number of subtrees headed by nodes with non-terminal A, that is a = E \u2022 a \u2022 How many subtrees does it have? Consider first the possibilities on the left branch. There are bk non-trivial subtrees headed by Bak, and there is also the trivial case where the left node is simply B. Thus there are bk + 1 different possibilities on the left branch. Similarly, for the right branch there are ci + 1 possibilities. We can create a subtree by choosing any possible left subtree and any possible right subtree. Thus, there are ai = (bk + 1)(ci + 1) possible subtrees headed by A@j. In our example tree of Figure 1, both noun phrases have exactly one subtree: npt = np2 = 1; the verb phrase has 2 subtrees: vp3 = 2; and the sentence has 6: si = 6. These numbers correspond to the number of subtrees in Figure 2. We will call a PCFG subderivation isomorphic to a STSG tree if the subderivation begins with an external non-terminal, uses internal nonterminals for intermediate steps, and ends with external non-terminals. For instance, consider the tree taken from Figure 2. The following PCFG subderivation is isomorphic: S = NPA1 VP\u00a92 PN PN VP@2 PN PN V NP. We say that a PCFG derivation is isomorphic to a STSG derivation if there is a corresponding PCFG subderivation for every step in the STSG derivation. We will give a simple small PCFG with the following surprising property: for every subtree in the training corpus headed by A, the grammar will generate an isomorphic subderivation with probability 1/a. In other words, rather than using the large, explicit STSG, we can use this small PCFG that generates isomorphic derivations, with identical probabilities. The construction is as follows. For a node such we will generate the following eight PCFG rules, where the number in parentheses following a rule is its probability. Ai -4 BC (11a3) A -4 BC (11a) \u2014> BkC (bklaj) A \u2014> BkC (bkla) Ai -4 BC, (cilai) A -4 BC, (cila) BkC, (bkCliaj) A BkC, (bkel /a) We will show that subderivations headed by A with external non-terminals at the roots and leaves, internal non-terminals elsewhere have probability 1/a. Subderivations headed by Ai with external non-terminals only at the leaves, internal non-terminals elsewhere, have probability Vai. The proof is by induction on the depth of the trees. For trees of depth 1, there are two cases: Trivially, these trees have the required probabilities. Now, assume that the theorem is true for trees of depth n or less. We show that it holds for trees of depth n +1. There are eight cases, one for each of the eight rules. We show two of them. Let Bak represent a tree of at most depth n with external leaves, headed by Bk, and with internal intermediate non-terminals. Then, for trees such as the probability of the tree is. -1- = -. Simibk ci aj a, larly, for another case, trees headed by A the probability of the tree is*kk-0 = 1. The other six cases follow trivially with similar reasoning. We call a PCFG derivation isomorphic to a STSG derivation if for every substitution in the STSG there is a corresponding subderivation in the PCFG. Figure 4 contains an example of isomorphic derivations, using two subtrees in the STSG and four productions in the PCFG. We call a PCFG tree isomorphic to a STSG tree if they are identical when internal nonterminals are changed to external non-terminals. Our main theorem is that this construction produces PCFG trees isomorphic to the STSG trees with equal probability. If every subtree in the training corpus occurred exactly once, this would be trivial to prove. For every STSG subderivation, there would be an isomorphic PCFG subderivation, with equal probability. Thus for every STSG derivation, there would be an isomorphic PCFG derivation, with equal probability. Thus every STSG tree would be produced by the PCFG with equal probability. However, it is extremely likely that some subtrees, especially trivial ones like If the STSG formalism were modified slightly, so that trees could occur multiple times, then our relationship could be made one to one. Consider a modified form of the DOP model, in which when subtrees occurred multiple times in the training corpus, their counts were not merged: both identical trees are added to the grammar. Each of these trees will have a lower probability than if their counts were merged. This would change the probabilities of the derivations; however the probabilities of parse trees would not change, since there would be correspondingly more derivations for each tree. Now, the desired one to one relationship holds: for every derivation in the new STSG there is an isomorphic derivation in the PCFG with equal probability. Thus, summing over all derivations of a tree in the STSG yields the same probability as summing over all the isomorphic derivations in the PCFG. Thus, every STSG tree would be produced by the PCFG with equal probability. It follows trivially from this that no extra trees are produced by the PCFG. Since the total probability of the trees produced by the STSG is 1, and the PCFG produces these trees with the same probability, no probability is &quot;left over&quot; for any other trees. There are several different evaluation metrics one could use for finding the best parse. In the section covering previous research, we considered the most probable derivation and the most probable parse tree. There is one more metric we could consider. If our performance evaluation were based on the number of constituents correct, using measures similar to the crossing brackets measure, we would want the parse tree that was most likely to have the largest number of correct constituents. With this criterion and the example grammar of Figure 3, the best parse tree would be The probability that the S constituent is correct is 1.0, while the probability that the A constituent is correct is 1, and the probability that the B constituent is correct is Thus, this tree has on average 2 constituents correct. All other trees will have fewer constituents correct on average. We call the best parse tree under this criterion the Maximum Constituents Parse. Notice that this parse tree cannot even be produced by the grammar: each of its constituents is good, but it is not necessarily good when considered as a full tree. Bod (1993a, 1995a) shows that the most probable derivation does not perform as well as the most probable parse for the DOP model, getting 65% exact match for the most probable derivation, versus 96% correct for the most probable parse. This is not surprising, since each parse tree can be derived by many different derivations; the most probable parse criterion takes all possible derivations into account. Similarly, the Maximum Constituents Parse is also derived from the sum of many different derivations. Furthermore, although the Maximum Constituents Parse should not do as well on the exact match criterion, it should perform even better on the percent constituents correct criterion. We have previously performed a detailed comparison between the most likely parse, and the Maximum Constituents Parse for Probabilistic Context Free Grammars (Goodman, 1996); we showed that the two have very similar performance on a broad range of measures, with at most a 10% difference in error rate (i.e., a change from 10% error rate to 9% error rate.) We therefore think that it is reasonable to use a Maximum Constituents Parser to parse the DOP model. The parsing algorithm is a variation on the Inside-Outside algorithm, developed by Baker (1979) and discussed in detail by Lan and Young (1990). However, while the Inside-Outside algorithm is a grammar re-estimation algorithm, the algorithm presented here is just a parsing algorithm. It is closely related to a similar algorithm used for Hidden Markov Models (Rabiner, 1989) for finding the most likely state at each time. However, unlike in the HMM case where the algorithm produces a simple state sequence, in the PCFG case a parse tree is produced, resulting in additional constraints. A formal derivation of a very similar algorithm is given elsewhere (Goodman, 1996); only the intuition is given here. The algorithm can be summarized as follows. First, for each potential constituent, where a constituent is a non-terminal, a start position, and an end position, find the probability that that constituent is in the parse. After that, put the most likely constituents together to form a passe tree, using dynamic programming. The probability that a potential constituent occurs in the correct parse tree, P(X ws...wtIS wi...tv\u201e), will be called g(s,t, X). In words, it is the probability that, given the sentence wi...w., a symbol X generates ws...wt\u2022 We can compute this probability using elements of the Inside-Outside algorithm. First, compute the inside probabilities, e(s,t, X) = P(X Ws.--wt). Second, compute the outside probabilities, f(s,t, X) = P(S Third, compute the matrix g(s,t, X): Once the matrix g(s,t, X) is computed, a dynamic programming algorithm can be used to determine the best parse, in the sense of maximizing the number of constituents expected correct. Figure 5 shows pseudocode for a simplified form of this algorithm. For a grammar with g nonterminals and training data of size T, the run time of the algorithm is 0(Tn2 + gn3 + n3) since there are two layers of outer loops, each with run time at most n, and inner loops, over addresses (training data), nonterminals and n. However, this is dominated by the computation of the Inside and Outside probabilities, which takes time 0(rn3), for a grammar with r rules. Since there are eight rules for every node in the training data, this is 0(Tn3). By modifying the algorithm slightly to record the actual split used at each node, we can recover the best parse. The entry maxc Li, n] contains the expected number of correct constituents, given the model. ", "conclusion": "We have given efficient techniques for parsing the DO? model. These results are significant since the DOP model has perhaps the best reported parsing accuracy; previously the full DOP model had not been replicated due to the difficulty and computational complexity of the existing algorithms. We have also shown that previous results were partially due to an unlikely choice of test data, and partially due to the heavy cleaning of the data, which reduced the difficulty of the task. Of course, this research raises as many questions as it answers. Were previous results due only to the choice of test data, or are the differences in implementation partly responsible? In that case, there is significant future work required to understand which differences account for Bod's exceptional performance. This will be complicated by the fact that sufficient details of Bod's implementation are not available. This research also shows the importance of testing on more than one small test set, as well as the importance of not making cross-corpus comparisons; if a new corpus is required, then previous algorithms should be duplicated for comparison. ", "summary_sents": ["Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993c).", "Unfortunately, existing algorithms are both computationally intensive and difficult to implement.", "Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm.", "In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar.", "We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree.", "Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate.", "This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data.", "We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.", "We give a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set."]}
{"title": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods", "abstract": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints \u2014 that words tend to have one sense per discourse and one sense per collocation \u2014 exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%. ", "introduction": "This paper presents an unsupervised algorithm that can accurately disambiguate word senses in a large, completely untagged corpus.1 The algorithm avoids the need for costly hand-tagged training data by exploiting two powerful properties of human language: Moreover, language is highly redundant, so that the sense of a word is effectively overdetermined by (1) and (2) above. The algorithm uses these properties to incrementally identify collocations for target senses of a word, given a few seed collocations 'Note that the problem here is sense disambiguation: assigning each instance of a word to established sense definitions (such as in a dictionary). This differs from sense induction: using distributional similarity to partition word instances into clusters that may have no relation to standard sense partitions. 'Here I use the traditional dictionary definition of collocation \u2014 &quot;appearing in the same location; a juxtaposition of words&quot;. No idiomatic or non-compositional interpretation is implied. for each sense, This procedure is robust and selfcorrecting, and exhibits many strengths of supervised approaches, including sensitivity to word-order information lost in earlier unsupervised algorithms. ", "conclusion": "In essence, our algorithm works by harnessing several powerful, empirically-observed properties of language, namely the strong tendency for words to exhibit only one sense per collocation and per discourse. It attempts to derive maximal leverage from these properties by modeling a rich diversity of collocational relationships. It thus uses more discriminating information than available to algorithms treating documents as bags of words, ignoring relative position and sequence. Indeed, one of the strengths of this work is that it is sensitive to a wider range of language detail than typically captured in statistical sense-disambiguation algorithms. Also, for an unsupervised algorithm it works surprisingly well, directly outperforming Schiitze's unsupervised algorithm 96.7 % to 92.2 %, on a test of the same 4 words. More impressively, it achieves nearly the same performance as the supervised algorithm given identical training contexts (95.5 % vs. 96.1 %) , and in some cases actually achieves superior performance when using the one-sense-perdiscourse constraint (96.5 % vs. 96.1%). This would indicate that the cost of a large sense-tagged training corpus may not be necessary to achieve accurate word-sense disambiguation. ", "summary_sents": ["This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations.", "The algorithm is based on two powerful constraints - that words tend to have one sense per discourse and one sense per collocation - exploited in an iterative bootstrapping procedure.", "Tested accuracy exceeds 96%.", "We introduce the idea of sense consistency and extend it to operator across related documents.", "We propose the self training, a semi-supervised algorithm which we apply do word sense disambiguation."]}
{"title": "Ranking Algorithms For Named Entity Extraction: Boosting And The Voted Perceptron", "abstract": "This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data. The first approach uses a boosting algorithm for ranking problems. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples. ", "introduction": "Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures. Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al. 1997; Johnson et al. 1999), and boosting algorithms (Freund et al. 1998; Collins 2000; Walker et al. 2001). One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included. A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures. This discriminative property is shared by the methods of (Johnson et al. 1999; Collins 2000), and also the Conditional Random Field methods of (Lafferty et al. 2001). In a previous paper (Collins 2000), a boosting algorithm was used to rerank the output from an existing statistical parser, giving significant improvements in parsing accuracy on Wall Street Journal data. Similar boosting algorithms have been applied to natural language generation, with good results, in (Walker et al. 2001). In this paper we apply reranking methods to named-entity extraction. A state-ofthe-art (maximum-entropy) tagger is used to generate 20 possible segmentations for each input sentence, along with their probabilities. We describe a number of additional global features of these candidate segmentations. These additional features are used as evidence in reranking the hypotheses from the max-ent tagger. We describe two learning algorithms: the boosting method of (Collins 2000), and a variant of the voted perceptron algorithm, which was initially described in (Freund & Schapire 1999). We applied the methods to a corpus of over one million words of tagged web data. The methods give significant improvements over the maximum-entropy tagger (a 17.7% relative reduction in error-rate for the voted perceptron, and a 15.6% relative improvement for the boosting method). One contribution of this paper is to show that existing reranking methods are useful for a new domain, named-entity tagging, and to suggest global features which give improvements on this task. We should stress that another contribution is to show that a new algorithm, the voted perceptron, gives very credible results on a natural language task. It is an extremely simple algorithm to implement, and is very fast to train (the testing phase is slower, but by no means sluggish). It should be a viable alternative to methods such as the boosting or Markov Random Field algorithms described in previous work. ", "conclusion": "", "summary_sents": ["This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.", "The first approach uses a boosting algorithm for ranking problems.", "The second approach uses the voted perceptron algorithm.", "Both algorithms give comparable, significant improvements over the maximum-entropy baseline.", "The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples.", "We describe a mapping from words to word types which groups words with similar orthographic forms into classes."]}
{"title": "Generalized Probabilistic LR Parsing Of Natural Language (Corpora) With Unification-Based Grammars", "abstract": "The first issue to consider is what the analysis will be used for and what constraints this places on its form. The corpus analysis literature contains a variety of proposals, ranging from part-of-speech tagging to assignment of a unique, sophisticated syntactic analysis. Our eventual goal is to recover a semantically and pragmatically appropriate syntactic analysis capable of supporting semantic interpretation. Two stringent requirements follow immediately: firstly, the analyses assigned must determinately represent the syntactic relations that hold between all constituents in the input; secondly, they be drawn from an priori well-formed set of possible syntactic analyses (such as the set defined by a generative grammar). Otherwise, semantic interpretation of the resultant analyses cannot be guaranteed to be (structurally) unambiguous, and the semantic operations defined (over syntactic configurations) cannot be guaranteed to match and yield an interpretation. These requirements immediately suggest that approaches that recover only lexical tags (e.g. de Rose 1988) or a syntactic analysis that is the 'closest fit' to some previously defined set of possible analyses (e.g. Sampson, Haigh, and Atwell 1989), are inadequate (taken alone). Pioneering approaches to corpus analysis proceeded on the assumption that computationally tractable generative grammars of sufficiently general coverage could not be developed (see, for example, papers in Garside, Leech, and Sampson 1987). However, the development of wide-coverage declarative and computationally tractable grammars makes this assumption questionable. For example, the ANLT word and sentence grammar (Grover et al. 1989; Carroll and Grover 1989) consists of an English lexicon of approximately 40,000 lexemes and a 'compiled' fixed-arity term unification grammar containing around 700 phrase structure rules. Taylor, Grover, and Briscoe (1989) demonstrate that an earlier version of this grammar was capable of assigning the correct analysis to 96.8% of a corpus of 10,000 noun phrases extracted (without regard for their internal form) from a variety of corpora. However, although Taylor, and show that the ANLT grammar very wide coverage, they abstract away from issues of lexical idiosyncrasy by formimg equivalence classes of noun phrases and parsing a single token of each class, and they do not address the issues of 1) tuning a grammar to a particular corpus or sublanguage 2) selecting the correct analysis from the set licensed by the grammar and 3) providing reliable analyses of input outside the coverage of the grammar. Firstly, it is clear that vocabulary, idiom, and conventionalized constructions used in, say, legal language and dictionary definitions, will differ both in terms of the range and frequency of words and constructions deployed. Secondly, Church and Patil (1982) demonstrate that for a realistic grammar parsing realistic input, the set of possible analyses licensed by the grammar can be in the thousands. Finally, it is extremely unlikely that any generative grammar will ever be capable of correctly analyzing all naturally occurring input, even when tuned for a particular corpus or sublanguage (if only because of the synchronic idealization implicit in the assumption that the set of grammatical sentences of a language is well formed.) this paper, we describe our to the first and second problems and make some preliminary remarks concerning the third (far harder) problem. Our apto grammar tuning is based on a semi-automatic parsing phase which additions to the grammar are made manually and statistical information concerning the frequency of use of grammar rules is acquired. Using this statistical information and modified grammar, a breadth-first probabilistic parser is constructed. The latter is capable of ranking the possible parses identified by the grammar in a useful (and efficient) manner. However, (unseen) sentences whose correct analysis is outside the coverage of the grammar reri:ain a problem. The feasibility and usefulness of our approach has been investigated in a preliminary way by analyzing a small corpus of 26 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing definitions drawn from the Dictionary of Contemporary English (Procter 1978). This corpus was chosen because the vocabulary employed is restricted (to approximately 2,000 morphemes), average definition length is about 10 words (with a maximum of around 30), and each definition is independent, allowing us to ignore phenomena such as ellipsis. In addition, the language of definitions represents a recognizable sublanguage, allowing us to explore the task of tuning a general purpose grammar. The results reported below suggest that probabilistic information concerning the frequency of occurrence of syntactic rules correlates in a useful (though not absolute) way with the semantically and pragmatically most plausible analysis. In Section 2, we briefly review extant work on probabilistic approaches to corpus analysis and parsing and argue the need for a more refined probabilistic model to distinguish distinct derivations. Section 3 discusses work on LR parsing of natural language and presents our technique for automatic construction of LR parsers for unification-based grammars. Section 4 presents the method and results for constructing a LALR(1) parse table for the ANLT grammar and discusses these in the light of both computational complexity and other empirical results concerning parse table size and construction time. Section 5 motivates our interactive and incremental approach to semi-automatic production of a disambiguated training corpus and describes the variant of the LR parser used for this task. Section 6 describes our implementation of a breadth-first LR parser and compares its performance empirically to a highly optimized chart parser for the same grammar, suggesting that (optimized) LR parsing is more efficient in practice for the ANLT grammar despite exponential worst case complexity results. Section 7 explains the technique we employ for deriving a probabilistic version of the LR parse table from the training corpus, and demonstrates that this leads to a more refined and parse-context\u2014dependent probabilistic model capable of distinguishing derivations that in a probabilistic context-free model would be equally probable. Section 8 describes and presents the results of our first experiment parsing LDOCE noun definitions, and Section 9 draws some preliminary conclusions and outlines ways in which the work described should be modified and extended. 2. Probabilistic Approaches to Parsing In the field of speech recognition, statistical techniques based on hidden Markov mod ", "introduction": "", "conclusion": "", "summary_sents": ["We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques.", "The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis.", "We discuss a fully automatic procedure for constructing an LR parse table from a unification-based grammar formalism, and consider the suitability of alternative LALR(1) parse table construction methods for large grammars.", "The parse table is used as the basis for two parsers; a user-driven interactive system that provides a computationally tractable and labor-efficient method of supervised training of the statistical information required to drive the probabilistic parser.", "The latter is constructed by associating probabilities with the LR parse table directly.", "This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism.", "We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser.", "We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions.", "Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic)frequency of occurrence.", "Our work on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags.", "Our statistical parser is an extension of the ANLT grammar development system."]}
{"title": "Randomized Algorithms And NLP: Using Locality Sensitive Hash Functions For High Speed Noun Clustering", "abstract": "In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic to practically linear in the number of elements to be computed. ", "introduction": "In the last decade, the field of Natural Language Processing (NLP), has seen a surge in the use of corpus motivated techniques. Several NLP systems are modeled based on empirical data and have had varying degrees of success. Of late, however, corpusbased techniques seem to have reached a plateau in performance. Three possible areas for future research investigation to overcoming this plateau include: The above listing may not be exhaustive, but it is probably not a bad bet to work in one of the above directions. In this paper, we investigate the first two avenues. Handling terabytes of data requires more efficient algorithms than are currently used in NLP. We propose a web scalable solution to clustering nouns, which employs randomized algorithms. In doing so, we are going to explore the literature and techniques of randomized algorithms. All clustering algorithms make use of some distance similarity (e.g., cosine similarity) to measure pair wise distance between sets of vectors. Assume that we are given n points to cluster with a maximum of k features. Calculating the full similarity matrix would take time complexity n2k. With large amounts of data, say n in the order of millions or even billions, having an n2k algorithm would be very infeasible. To be scalable, we ideally want our algorithm to be proportional to nk. Fortunately, we can borrow some ideas from the Math and Theoretical Computer Science community to tackle this problem. The crux of our solution lies in defining Locality Sensitive Hash (LSH) functions. LSH functions involve the creation of short signatures (fingerprints) for each vector in space such that those vectors that are closer to each other are more likely to have similar fingerprints. LSH functions are generally based on randomized algorithms and are probabilistic. We present LSH algorithms that can help reduce the time complexity of calculating our distance similarity atrix to nk. Rabin (1981) proposed the use of hash functions from random irreducible polynomials to create short fingerprint representations for very large strings. These hash function had the nice property that the fingerprint of two identical strings had the same fingerprints, while dissimilar strings had different fingerprints with a very small probability of collision. Broder (1997) first introduced LSH. He proposed the use of Min-wise independent functions to create fingerprints that preserved the Jaccard similarity between every pair of vectors. These techniques are used today, for example, to eliminate duplicate web pages. Charikar (2002) proposed the use of random hyperplanes to generate an LSH function that preserves the cosine similarity between every pair of vectors. Interestingly, cosine similarity is widely used in NLP for various applications such as clustering. In this paper, we perform high speed similarity list creation for nouns collected from a huge web corpus. We linearize this step by using the LSH proposed by Charikar (2002). This reduction in complexity of similarity computation makes it possible to address vastly larger datasets, at the cost, as shown in Section 5, of only little reduction in accuracy. In our experiments, we generate a similarity list for each noun extracted from 70 million page web corpus. Although the NLP community has begun experimenting with the web, we know of no work in published literature that has applied complex language analysis beyond IR and simple surface-level pattern matching. ", "conclusion": "NLP researchers have just begun leveraging the vast amount of knowledge available on the web. By searching IR engines for simple surface patterns, many applications ranging from word sense disambiguation, question answering, and mining semantic resources have already benefited. However, most language analysis tools are too infeasible to run on the scale of the web. A case in point is generating noun similarity lists using co-occurrence statistics, which has quadratic running time on the input size. In this paper, we solve this problem by presenting a randomized algorithm that linearizes this task and limits memory requirements. Experiments show that our method generates cosine similarities between pairs of nouns within a score of 0.03. In many applications, researchers have shown that more data equals better performance (Banko and Brill, 2001; Curran and Moens, 2002). Moreover, at the web-scale, we are no longer limited to a snapshot in time, which allows broader knowledge to be learned and processed. Randomized algorithms provide the necessary speed and memory requirements to tap into terascale text sources. We hope that randomized algorithms will make other NLP tools feasible at the terascale and we believe that many algorithms will benefit from the vast coverage of our newly created noun similarity list. ", "summary_sents": ["In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data.", "We apply these algorithms to generate noun similarity lists from 70 million pages.", "We reduce the running time from quadratic to practically linear in the number of elements to be computed.", "We show that by using the LSH nearest neighbors calculation can be done in O(nd) time.", "Our method can produce over 70% accuracy in extracting synonyms."]}
{"title": "An Algebra For Semantic Construction In Constraint-Based Grammars", "abstract": "We develop a framework for formalizing semantic construction within grammars expressed in typed feature struclogics, including The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unificationbased approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability. ", "introduction": "Some constraint-based grammar formalisms incorporate both syntactic and semantic representations within the same structure. For instance, Figure 1 shows representations of typed feature structures (TFSs) for Kim, sleeps and the phrase Kim sleeps, in an HPSG-like representation, loosely based on Sag and Wasow (1999). The semantic representation expressed is intended to be equivalent to r name(x, Kim) \u2227 sleep(e, x).1 Note: A similar approach has been used in a large number of implemented grammars (see Shieber (1986) for a fairly early example). It is in many ways easier to work with than A-calculus based approaches (which we discuss further below) and has the great advantage of allowing generalizations about the syntax-semantics interface to be easily expressed. But there are problems. The operations are only specified in terms of the TFS logic: the interpretation relies on an intuitive correspondence with a conventional logical representation, but this is not spelled out. Furthermore the operations on the semantics are not tightly specified or constrained. For instance, although HPSG has the Semantics Principle (Pollard and Sag, 1994) this does not stop the composition process accessing arbitrary pieces of structure, so it is often not easy to conceptually disentangle the syntax and semantics in an HPSG. Nothing guarantees that the grammar is monotonic, by which we mean that in each rule application the semantic content of each daughter subsumes some portion of the semantic content of the mother (i.e., no semantic information is dropped during composition): this makes it impossible to guarantee that certain generation algorithms will work effectively. Finally, from a theoretical perspective, it seems clear that substantive generalizations are being missed. Minimal Recursion Semantics (MRS: Copestake et al (1999), see also Egg (1998)) tightens up the specification of composition a little. It enforces monotonic accumulation of EPs by making all rules append the EPs of their daughters (an approach which was followed by Sag and Wasow (1999)) but it does not fully spectics in TFSs d to other work on unification based grammar. and abstracts away from the specific feature architecture used in individual grammars, but the essential features of the algebra can be encoded in the hierarchy of lexical and constructional type constraints. Our work actually started as an attempt at rational reconstruction of semantic composition in the large grammar implemented by the LinGO project at (available via Semantics and the syntax/semantics interface have accounted for approximately nine-tenths of the development time of the English Resource Grammar (ERG), largely because the account of semantics within is so underdetermined. In this paper, we begin by giving a formal account of a very simplified form of the algebra and in \u00a73, we consider its interpretation. In \u00a74 to \u00a76, we generalize to the full algebra needed to capture the use of in the LinGO English Resource Grammar (ERG). Finally we conclude with some comparisons to the an ", "conclusion": "We have developed a framework for formally specifying semantics within constraint-based representations which allows semantic operations in a grammar to be tightly specified and which allows a representation of semantic content which is largely independent of the feature structure architecture of the syntactic representation. HPSGs can be written which encode much of the algebra described here as constraints on types in the grammar, thus ensuring that the grammar is consistent with the rules on composition. There are some aspects which cannot be encoded within currently implemented TFS formalisms because they involve negative conditions: for instance, we could not write TFS constraints that absolutely prevent a grammar writer sneaking in a disallowed coindexation by specifying a path into the lzt. There is the option of moving to a more general TFS logic but this would require very considerable research to develop reasonable tractability. Since the constraints need not be checked at runtime, it seems better to regard them as metalevel conditions on the description of the grammar, which can anyway easily be checked by code which converts the TFS into the algebraic representation. Because the ERG is large and complex, we have not yet fully completed the exercise of retrospectively implementing the constraints throughout. However, much of the work has been done and the process revealed many bugs in the grammar, which demonstrates the potential for enhanced maintainability. We have modified the grammar to be monotonic, which is important for the chart generator described in Carroll et al (1999). A chart generator must determine lexical entries directly from an input logical form: hence it will only work if all instances of nonmonotonicity can be identified in a grammar-specific preparatory step. We have increased the generator\u2019s reliability by making the ERG monotonic and we expect further improvements in practical performance once we take full advantage of the restrictions in the grammar to cut down the search space. ", "summary_sents": ["We develop a framework for formalizing semantic construction within grammars expressed in typed feature structure logics, including HPSG.", "The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unification- based approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability.", "The semantic interpretations are expressed using Minimal Recursion Semantics (MRS), which provides the means to represent interpretations with a flat, underspecified semantics using terms of the predicate calculus and generalized quantifiers.", "An MRS consists of a bag of labeled elementary predicates and their arguments, a list of scoping constraints, and a pair of relations that provide a hook into the representation - a label, which must outscope all the handles, and an index."]}
{"title": "Knowledge-Free Induction Of Morphology Using Latent Semantic Analysis", "abstract": "Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction. Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Relying on stemand-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes (&quot;ally&quot; stemming to &quot;all&quot;). We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plusaffix are sufficiently similar semantically. We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system. ", "introduction": "Computational morphological analyzers have existed in various languages for years and it has been said that &quot;the quest for an efficient method for the analysis and generation of word-forms is no longer an academic research topic&quot; (Karlsson and Karttunen, 1997). However, development of these analyzers typically begins with human intervention requiring time spans from days to weeks. If it were possible to build such analyzers automatically without human knowledge, significant development time could be saved. On a larger scale, consider the task of inducing machine-readable dictionaries (MRDs) using no human-provided information (&quot;knowledge-free&quot;). In building an MRD, &quot;simply expanding the dictionary to encompass every word one is ever likely to encounter.. .fails to take advantage of regularities&quot; (Sproat, 1992, p. xiii). Hence, automatic morphological analysis is also critical for selecting appropriate and non-redundant MRD headwords. For the reasons expressed above, we are interested in knowledge-free morphology induction. Thus, in this paper, we show how to automatically induce morphological relationships between words. Previous morphology induction approaches (Goldsmith, 1997, 2000; Mean, 1998; Gaussier, 1999) have focused on inflectional languages and have used statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Several problems can arise using only stem-and-affix statistics: (1) valid affixes may be applied inappropriately (&quot;ally&quot; stemming to &quot;all&quot;), (2) morphological ambiguity may arise (&quot;rating&quot; conflating with &quot;rat&quot; instead of &quot;rate&quot;), and (3) non-productive affixes may get accidentally pruned (the relationship between &quot;dirty&quot; and &quot;dirt&quot; may be lost).1 Some of these problems could be resolved if one could incorporate word semantics. For instance, &quot;all&quot; is not semantically similar to &quot;ally,&quot; so with knowledge of semantics, an algorithm could avoid conflating these two words. To maintain the &quot;knowledge-free&quot; paradigm, such semantics would need to be automatically induced. Latent Semantic Analysis (LSA) (Deerwester, et al., 1990); Landauer, et at., 1998) is a technique which automatically identifies semantic information from a corpus. We here show that incorporating LSA-based semantics alone into the morphology-induction process can provide results that rival a state-ofthe-art system based on stem-and-affix statistics (Goldsmith's Linguistica). lError examples are from Goldsmith's Linguistica Our algorithm automatically extracts potential affixes from an untagged corpus, identifies word pairs sharing the same proposed stem but having different affixes, and uses LSA to judge semantic relatedness between word pairs. This process serves to identify valid morphological relations. Though our algorithm could be applied to any inflectional language, we here restrict it to English in order to perform evaluations against the human-labeled CELEX database (Baayen, et al., 1993). ", "conclusion": "These results suggest that semantics and LSA can play a key part in knowledge-free morphology induction. Semantics alone worked at least as well as Goldsmith's frequency-based approach. Yet we believe that semantics-based and frequency-based approaches play complementary roles. In current work, we are examining how to combine these two approaches. ", "summary_sents": ["Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction.", "Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate.", "Relying on stem-and-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes (\"ally\" stemming to \"all\").", "We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plus-affix are sufficiently similar semantically.", "We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system.", "We generate a list of N candidate suffixes and use this list to identify word pairs which share the same stem.", "We attempt to cluster morphologically related words starting with an unrefined trie search, which contains a parameter of minimum possible stem length and an upper bound on potential affix candidates, that is constrained by semantic similarity in a word context vector space."]}
{"title": "A Global Joint Model for Semantic Role Labeling", "abstract": "We present a model for semantic role labeling that effectively captures the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments. We show how to incorporate these strong dependencies in a statistical joint model with a rich set of features over multiple argument phrases. The proposed model substantially outperforms a similar state-of-the-art local model that does not include dependencies among different arguments. We evaluate the gains from incorporating this joint information on the Propbank corpus, when using correct syntactic parse trees as input, and when using automatically derived parse The gains amount to reduction on all arguments and core arguments for gold-standard parse trees on Propbank. For automatic parse trees, the error reductions are all and core arguments, respectively. We also present results on the CoNLL 2005 shared task data set. Additionally, we explore considering multiple syntactic analyses to cope with parser noise and uncertainty. ", "introduction": "", "conclusion": "", "summary_sents": ["We present a model for semantic role labeling that effectively captures the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments.", "We show how to incorporate these strong dependencies in a statistical joint model with a rich set of features over multiple argument phrases.", "The proposed model substantially outperforms a similar state-of-the-art local model that does not include dependencies among different arguments.", "We evaluate the gains from incorporating this joint information on the Propbank corpus, when using correct syntactic parse trees as input, and when using automatically derived parse trees.", "The gains amount to 24.1% error reduction on all arguments and 36.8% on core arguments for gold-standard parse trees on Propbank.", "For automatic parse trees, the error reductions are 8.3% and 10.3% on all and core arguments, respectively.", "We also present results on the CoNLL 2005 shared task data set.", "Additionally, we explore considering multiple syntactic analyses to cope with parser noise and uncertainty.", "We present a re-ranking model to jointly learn the semantic roles of multiple constituents in the SRL task."]}
{"title": "Soft Syntactic Constraints For Word Alignment Through Discriminative Training", "abstract": "Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree. However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex. We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint. The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser. ", "introduction": "Given a parallel sentence pair, or bitext, bilingual word alignment finds word-to-word connections across languages. Originally introduced as a byproduct of training statistical translation models in (Brown et al., 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks. The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings. These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology. There have been many attempts to incorporate syntax into alignment; we will not present a complete list here. Some methods parse two flat strings at once using a bitext grammar (Wu, 1997). Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with syntactic aligners tend to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft constraint. We add our syntactic features to an already strong flat-string discriminative solution, and we show that they provide new information resulting in improved alignments. ", "conclusion": "We have presented a discriminative, syntactic word alignment method. Discriminative training is conducted using a highly modular SVM for structured output, which allows code reuse between the syntactic aligner and a maximum matching baseline. An ITG parser is used for the alignment search, exposing two syntactic features: the use of inverted productions, and the use of spans that would not be available in a tree-to-string system. This second feature creates a soft phrasal cohesion constraint. Discriminative training allows us to maintain all of the features that are useful to the maximum matching baseline in addition to the new syntactic features. We have shown that these features produce a 22% relative reduction in error rate with respect to a strong flat-string model. ", "summary_sents": ["Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree.", "However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex.", "We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint.", "The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.", "We use dependency structures as soft constraints to improve word alignment in an ITG framework.", "We introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment."]}
{"title": "Improved Alignment Models For Statistical Machine Translation", "abstract": "PP \u2014 31.5 In all experiments, we use the following three error criteria: \u2022 WER (word error rate): The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the target string. This performance criterion is widely used in speech recognition. \u2022 PER (position-independent word error rate): A shortcoming of the WER is the fact that it requires a perfect word order. This is 26 2: for Text and Speech Input: error rate (WER), positionindependent word error rate (PER) and subjective sentence error rate (SSER) with/without preprocessing (147 sentences = 1 968 words of the Verbmobil task). Input Preproc. WER[%] PER[Vo] SSER[%] Single-Word Based Approach Text No 53.4 38.3 35.7 Yes 56.0 41.2 35.3 Speech No 67.8 50.1 54.8 Yes 67.8 51.4 52.7 Alignnient Templates Text No 49.5 35.3 31.5 Yes 48.3 35.1 27.2 Speech No 63.5 45.6 52.4 Yes 62.8 45.6 50.3 particularly a problem for the Verbmobil task, where the word order of the German- English sentence pair can be quite different. As a result, the word order of the automatically generated target sentence can be different from that of the target sentence, but nevertheless acceptable so that the WER measure alone could be misleading. In order to overcome this problem, we introduce as additional measure the positionindependent word error rate (PER). This measure compares the words in the two senthe word order into account. Words that have no matching counterparts are counted as substitution errors. Depending on whether the translated sentence is longer or shorter than the target translation, the remaining words result in either insertion or deletion errors in addition to substitution errors. The PER is guaranteed to be less than or equal to the WER. \u2022 SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary. Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0. A score of 0.0 means that the translation is semantically and syntactically correct, a score of 0.5 means that a sentence is semantically correct but syntactically wrong and a score of 1.0 means that the sentence is semantically wrong. The human examiner was offered the translated sentences of the two approaches at the same As a result we expect a better possibility of reproduction. The results of the translation experiments using the single-word based approach and the alignment template approach on text input and on speech input are summarized in Table 2. The results are shown with and without the use of domain-specific preprocessing. The alignment template approach produces better translation results than the single-word based approach. From this we draw the conclusion that it is important to model word groups in source and target language. Considering the recognition word error rate of 31% the degradation of about 20% by speech input can be expected. The average translation time on an Alpha workstation for a single sentence is about one second for the alignment template appreach and 30 seconds for the single-word based search procedure. Within the Verbmobil project other translation modules based on rule-based, examplebased and dialogue-act-based translation are used. We are not able to present results with these methods using our test corpus. But in the current Verbmobil prototype the preliminary evaluations show that the statistical methods produce comparable or better results than other systems. An advantage of the systhat it is robust and always produces a translation result even if the input of the speech recognizer is quite incorrect. 5 Summary We have described two approaches to perform statistical machine translation which extend the baseline alignment models. The single-word 27 based approach allows for the the possibility of one-to-many alignments. The alignment template approach uses two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words. As a result the context of words has a greater influence and the changes in word order from source to target language can be learned explicitly. An advantage of both methods is that they learn fully automatically by using a bilingual training corpus and are capable of achieving better translation results on a limited-domain task than other example-based or rule-based translation systems. Acknowledgment This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268). ", "introduction": "", "conclusion": "", "summary_sents": ["In this paper, we describe improved alignment models for statistical machine translation.", "The statistical translation approach uses two types of information: a translation model and a language model.", "The language model used is a bigram or general m-gram model.", "The translation model is decomposed into a lexical and an alignment model.", "We describe two different approaches for statistical translation and present experimental results.", "The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words.", "We present results using the Verbmobil task (German-English, 6000-word vocabulary) which is a limited-domain spoken-language task.", "The experimental tests were performed on both the text transcription and the speech recognizer output.", "To obtain the best single alignment, we use a post-hoc algorithm to merge directional alignments.", "We propose a heuristic, where all the aligned phrase pairs (x?, a?, y?) satisfying the following criteria are extracted: (1) x? and y? consist of consecutive words of x and y, and both have length at most k, (2) a? is the alignment between words of x? and y? induced by a, (3) a? contains at least one link, and (4) there are no links in a that have just one end in x? or y?."]}
{"title": "The Necessity Of Parsing For Predicate Argument Recognition", "abstract": "Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time. Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter &quot;chunked&quot; representation of the input can be as effective for the purposes of semantic role identification. ", "introduction": "Over the past decade, most work in the field of information extraction has shifted from complex rule-based, systems designed to handle a wide variety of semantic phenomena including quantification, anaphora, aspect and modality (e.g. Alshawi (1992)), to simpler finite-state or statistical systems such as Hobbs et al. (1997) and Miller et al. (1998). Much of the evaluation of these systems has been conducted on extracting relations for specific semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences. Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates. The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science Institute (Baker et al., 1998) share the goal of documenting the syntactic realization of arguments of the predicates of the general English lexicon by annotating a corpus with semantic roles. Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the effect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. The system first passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentences were automatically parsed, as no hand-annotated parse trees were available for the corpus. While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this effect. Of the systems evaluated for the Message Understanding Conference task, Miller et al. (1998) made use of an integrated syntactic and semantic model producing a full parse tree, and achieved results comparable to other systems that did not make use of a complete parse. As in the FrameNet case, the parser was not trained on the corpus for which semantic annotations were available, and the effect of better, or even perfect, parses could not be measured. One of the differences between the two semantic annotation projects is that the sentences chosen for annotation for Propbank are from the same Wall Street Journal corpus chosen for annotation for the original Penn Treebank project, and thus hand-checked syntactic parse trees are available for the entire dataset. In this paper, we compare the performance of a system based on goldstandard parses with one using automatically generated parser output. We also examine whether it is possible that the additional information contained in a full parse tree is negated by the errors present in automatic parser output, by testing a role-labeling system based on a flat or &quot;chunked&quot; representation of the input. ", "conclusion": "Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk's other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al. (1997), McCallum et al. (2000) and Lafferty et al. (2001). While a more elaborate finite-state system might do better, it is possible that additional features would not be helpful given the small amount of data for each predicate. By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a flat representation of the data. We feel that our results show that statistical parsers, although computationally expensive, do a good job of providing relevant information for semantic interpretation. Not only the constituent structure but also head word information, produced as a side product, are important features. Parsers, however, still have a long way to go. Our results using hand-annotated parse trees show that improvements in parsing should translate directly into better semantic interpretations. Acknowledgments This work was undertaken with funding from the Institute for Research in Cognitive Science at the University of Pennsylvania and from the Propbank project, DoD Grant MDA904-00C-2136. ", "summary_sents": ["Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time.", "Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text.", "In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter \"chunked\" representation of the input can be as effective for the purposes of semantic role identification.", "We note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function.", "We experiment with the set of features: Pred HW, Arg HW, Phrase Type, Position, Path, Voice."]}
{"title": "Evaluation Metrics For Generation", "abstract": "Certain generation applications may profit from the use of stochastic methods. In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models. In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment. This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects. To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment. The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development. ", "introduction": "For many applications in natural language generation (NLG), the range of linguistic expressions that must be generated is quite restricted, and a grammar for a surface realization component can be fully specified by hand. Moreover, in many cases it is very important not to deviate from very specific output in generation (e.g., maritime weather reports), in which case hand-crafted grammars give excellent control. In these cases, evaluations of the generator that rely on human judgments (Lester and Porter, 1997) or on human annotation of the test corpora (Kukich, 1983) are quite sufficient. However, in other NLG applications the variety of the output is much larger. and the demands on the quality of the output are somewhat less stringent. A typical example is NLG in the context of (interlingua- or transfer-based) machine translation. Another reason for relaxing the quality of the output may be that not enough time is available to develop a full grammar. for a new target language in NLG. In all these cases, stochastic methods provide an alternative to hand-crafted approaches to NLG. To our knowledge, the first to use stochastic techniques in an NLG realization module were Langkilde and Knight (1998a) and (1998b) (see also (Langkilde, 2000)). As is the case for stochastic approaches in natural language understanding, the research and development itself requires an effective intrinsic metric in order to be able to evaluate progress. In this paper, we discuss several evaluation metrics that we are using during the development of FERGUS (Flexible Empiricist/Rationalist Generation Using Syntax). FERGUS, a realization module, follows Knight and Langkilde's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a lexicalized syntactic grammar. The metrics are useful to us as relative quantitative assessments of different models we experiment with; however, we do not pretend that these metrics in themselves have any validity. Instead, we follow work done in dialog systems (Walker et al., 1997) and attempt to find metrics which on the one hand can be computed easily but on the other hand correlate with empirically verified human judgments in qualitative categories such as readability. The structure of the paper is as follows. In Section 2. we briefly describe the architecture of FERGUS, and some of the modules. In Section 3 we present four metrics and some results obtained with these metrics. In Section 4 we discuss the for experimental validation of the metrics using human judgments. and present a new metric based on the results of these experiments. In Section 5 we discuss some, of the -many problematic issues related to the use of metrics and our metrics in particular, and discuss on-going work. ", "conclusion": "", "summary_sents": ["Certain generation applications may profit from the use of stochastic methods.", "In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models.", "In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment.", "This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects.", "To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment.", "The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development.", "We propose Simple String Accuracy as a baseline evaluation metric for natural language generation."]}
{"title": "Deep Read: A Reading Comprehension System", "abstract": "paper describes initial work on Read, an automated reading comprehension system that accepts arbitrary text input (a story) and answers questions about it. We have acquired a corpus of 60 and 60 test stories of to grade material; each story is followed by short-answer questions (an answer key was also provided). We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution). This simple system retrieves the sentence containing the answer 30-40% of the time. ", "introduction": "This paper describes our initial work exploring reading comprehension tests as a research problem and an evaluation method for language understanding systems. Such tests can take the form of standardized multiple-choice diagnostic reading skill tests, as well as fill-inthe-blank and short-answer tests. Typically, such tests ask the student to read a story or article and to demonstrate her/his understanding of that article by answering questions about it. For an example, see Figure 1. Reading comprehension tests are interesting because they constitute &quot;found&quot; test material: these tests are created in order to evaluate children's reading skills, and therefore, test materials, scoring algorithms, and human performance measures already exist. Furthermore, human performance measures provide a more intuitive way of assessing the capabilities of a given system than current measures of precision, recall, F-measure, operating curves, etc. In addition, reading comprehension tests are written to test a range of skill levels. With proper choice of test material, it should be possible to challenge systems to successively higher levels of performance. For these reasons, reading comprehension tests offer an interesting alternative to the kinds of special-purpose, carefully constructed evaluations that have driven much recent research in language understanding. Moreover, the current state-of-theart in computer-based language understanding makes this project a good choice: it is beyond current systems' capabilities, but tractable. Our (WASHINGTON, D.C., 1964) - It was 150 years ago this year that our nation's biggest library burned to the ground. Copies of all the written books of the time were kept in the Library of Congress. But they were destroyed by fire in 1814 during a war with the British. That fire didn't stop book lovers. The next year, they began to rebuild the library. By giving it 6,457 of his books, Thomas Jefferson helped get it started. The first libraries in the United States could be used by members only. But the Library of Congress was built for all the people. From the start, it was our national library. Today, the Library of Congress is one of the largest libraries in the world. People can find a copy of just about every book and magazine printed. Libraries have been with us since people first learned to write. One of the oldest to be found dates back to about 800 years B.C. The books were written on tablets made from clay. The people who took care of the books were called &quot;men of the written tablets.&quot; simple bag-of-words approach picked an appropriate sentence 30-40% of the time with only a few months work, much of it devoted to infrastructure. We believe that by adding additional linguistic and world knowledge sources to the system, it can quickly achieve primary-school-level performance, and within a few years, &quot;graduate&quot; to real-world applications. Reading comprehension tests can serve as a testbed, providing an impetus for research in a number of areas: bottlenecks for lexical and world knowledge. In addition, research into collaboration might lead to insights about intelligent tutoring. Finally, reading comprehension evaluates systems' abilities to answer ad hoc, domainindependent questions; this ability supports fact retrieval, as opposed to document retrieval, which could augment future search engines \u2014 see Kupiec (1993) for an example of such work. There has been previous work on story understanding that focuses on inferential processing, common sense reasoning, and world knowledge required for in-depth understanding of stories. These efforts concern themselves with specific aspects of knowledge representation, inference techniques, or question types \u2014 see Lehnert (1983) or Schubert (to appear). In contrast, our research is concerned with building systems that can answer ad hoc questions about arbitrary documents from varied domains. We report here on our initial pilot study to determine the feasibility of this task. We purchased a small (hard copy) corpus of development and test materials (about 60 stories in each) consisting of remedial reading materials for grades 3-6; these materials are simulated news stories, followed by short-answer &quot;5W&quot; questions: who, what, when, where, and why questions.' We developed a simple, modular, baseline system that uses pattern matching (bag-of-words) techniques and limited linguistic processing to select the sentence from the text that best answers the query. We used our development corpus to explore several alternative evaluation techniques, and then evaluated on the test set, which was kept blind. ", "conclusion": "We have argued that taking reading comprehension exams is a useful task for developing and evaluating natural language understanding systems. Reading comprehension uses found material and provides humancomparable evaluations which can be computed automatically with a minimum of human annotation. Crucially, the reading comprehension task is neither too easy nor too hard, as the performance of our pilot system demonstrates. Finally, reading comprehension is a task that is sufficiently close to information extraction applications such as ad hoc question answering, fact verification, situation tracking, and document summarization, that improvements on the reading comprehension evaluations will result in improved systems for these applications. ", "summary_sents": ["This paper describes initial work on Deep Read, an automated reading comprehension system that accepts arbitrary text input (a story) and answers questions about it.", "We have acquired a corpus of 60 development and 60 test stories of 3rd to 6th grade material; each story is followed by short-answer questions (an answer key was also provided).", "We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution).", "This simple system retrieves the sentence containing the answer 30-40% of the time.", "We use a statistical bag-of-words approach, matching the question with the lexically most similar sentence in the story."]}
{"title": "A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing", "abstract": "Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages. ", "introduction": "Dependency-based syntactic parsing has been the focus of intense research efforts during the last decade, and the state of the art today is represented by globally normalized discriminative models that are induced using structured learning. Graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for inference (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010), but other inference methods have been explored especially for non-projective parsing (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009; Martins et al., 2010; Koo et al., 2010). Transitionbased models parameterize the problem by elementary parsing actions and typically use incremental beam search (Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Clark, 2011). Despite notable differences in model structure, graph-based and transition-based parsers both give state-of-theart accuracy with proper feature selection and optimization (Koo and Collins, 2010; Zhang and Nivre, 2011; Bohnet, 2011). It is noteworthy, however, that almost all dependency parsers presuppose that the words of an input sentence have been morphologically disambiguated using (at least) a part-of-speech tagger. This is in stark contrast to the best parsers based on PCFG models, such as the Brown parser (Charniak and Johnson, 2005) and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which not only can perform their own part-of-speech tagging but normally give better parsing accuracy when they are allowed to do so. This suggests that joint models for tagging and parsing might improve accuracy also in the case of dependency parsing. It has been argued that joint morphological and syntactic disambiguation is especially important for richly inflected languages, where there is considerable interaction between morphology and syntax such that neither can be fully disambiguated without considering the other. Thus, Lee et al. (2011) show that a discriminative model for joint morphological disambiguation and dependency parsing outperforms a pipeline model in experiments on Latin, Ancient Greek, Czech and Hungarian. However, Li et al. (2011) and Hatori et al. (2011) report improvements with a joint model also for Chinese, which is not a richly inflected language but is nevertheless rich in part-of-speech ambiguities. In this paper, we present a transition-based model for joint part-of-speech tagging and labeled dependency parsing with non-projective trees. Experiments show that joint modeling improves both tagging and parsing accuracy, leading to state-of-the-art accuracy for richly inflected languages like Czech and German as well as more configurational languages like Chinese and English. To our knowledge, this is the first joint system that performs labeled dependency parsing. It is also the first joint system that achieves state-of-the-art accuracy for non-projective dependency parsing. ", "conclusion": "We have presented the first system for joint partof-speech tagging and labeled dependency parsing with non-projective dependency trees. Evaluation on four languages shows consistent improvements in both tagging and parsing accuracy over a pipeline system with state-of-the-art results across the board. The error analysis reveals improvements in tagging accuracy for syntactically central categories, mainly verbs, with improvement in syntactic accuracy for core grammatical functions as a result. In future work we intend to explore joint models that incorporate not only basic part-of-speech tags but also more fine-grained morphological features. ", "summary_sents": ["Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins.", "We present a transition-based system for joint part-of-speech tagging and labeled dependency parsing with non-projective trees.", "Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-the-art results for all languages.", "we introduce a transition-based system that jointly performed POS tagging and dependency parsing."]}
{"title": "Syntax-Based Alignment Of Multiple Translations: Extracting Paraphrases And Generating New Sentences", "abstract": "We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations. ", "introduction": "In the past, paraphrases have come under the scrutiny of many research communities. Information retrieval researchers have used paraphrasing techniques for query reformulation in order to increase the recall of information retrieval engines (Sparck Jones and Tait, 1984). Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999). And researchers in multi-document text summarization (Barzilay et al., 1999), information extraction (Shinyama et al., 2002), and question answering (Lin and Pantel, 2001; Hermjakob et al., 2002) have focused on identifying and exploiting paraphrases in the context of recognizing redundancies, alternative formulations of the same meaning, and improving the performance of question answering systems. In previous work (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Shinyama et al., 2002), paraphrases are represented as sets or pairs of semantically equivalent words, phrases, and patterns. Although this is adequate in the context of some applications, it is clearly too weak from a generative perspective. Assume, for example, that we know that text pairs (stock market rose, stock market gained) and (stock market rose, stock prices rose) have the same meaning. If we memorized only these two pairs, it would be impossible to infer that, in fact, consistent with our intuition, any of the following sets of phrases are also semantically equivalent: {stock market rose, stock market gained, stock prices rose, stock prices gained } and {stock market, stock prices } in the context of rose or gained; {market rose }, {market gained }, {prices rose } and {prices gained } in the context of stock; and so on. In this paper, we propose solutions for two problems: the problem ofparaphrase representation and the problem of paraphrase induction. We propose a new, finite-statebased representation of paraphrases that enables one to encode compactly large numbers of paraphrases. We also propose algorithms that automatically derive such representations from inputs that are now routinely released in conjunction with large scale machine translation evaluations (DARPA, 2002): multiple English translations of many foreign language texts. For instance, when given as input the 11 semantically equivalent English translations in Figure 1, our algorithm automatically induces the FSA in Figure 2, which represents compactly 49 distinct renderings of the same semantic meaning. Our FSAs capture both lexical paraphrases, such as {fighting, battle}, {died, were killed} and structural paraphrases such as {last week\u2019s fighting, the battle of last week}. The contexts in which these are correct paraphrases are also conveniently captured in the representation. In previous work, Langkilde and Knight (1998) used word lattices for language generation, but their method involved hand-crafted rules. Bangalore et al. (2001) and Barzilay and Lee (2002) both applied the technique of multi-sequence alignment (MSA) to align parallel corpora and produced similar FSAs. For their purposes, they mainly need to ensure the correctness of consensus among different translations, so that different constituent orderings in input sentences do not pose a serious problem. In contrast, we want to ensure the correctness of all paths represented by the FSAs, and direct application of MSA in the presence of different constituent orderings can be problematic. For example, when given as input the same sentences in Figure 1, one instantiation of the MSA algorithm produces the FSA in Figure 3, which contains many \u201cbad\u201d paths such as the battle of last week\u2019s fighting took at least 12 people lost their people died in the fighting last week\u2019s fighting (See Section 4.2.2 for a more quantitative analysis.). It\u2019s still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). But we chose to approach this problem from another direction. As a result, we propose a new syntax-based algorithm to produce FSAs. In this paper, we first introduce the multiple translation corpus that we use in our experiments (see Section 2). We then present the algorithms that we developed to induce finite-state paraphrase representations from such data (see Section 3). An important part of the paper is dedicated to evaluating the quality of the finite-state representations that we derive (see Section 4). Since our representations encode thousands and sometimes millions of equivalent verbalizations of the same meaning, we use both manual and automatic evaluation techniques. Some of the automatic evaluations we perform are novel as well. ", "conclusion": "In this paper, we presented a new syntax-based algorithm that learns paraphrases from a newly available dataset. The multiple translation corpus that we use in this paper is the first instance in a series of similar corpora that are built and made publicly available by LDC in the context of a series of DARPA-sponsored MT evaluations. The algorithm we proposed constructs finite state representations of paraphrases that are useful in many contexts: to induce large lists of lexical and structural paraphrases; to generate semantically equivalent renderings of a given meaning; and to estimate the quality of machine translation systems. More experiments need to be carried out in order to assess extrinsically whether the FSAs we produce can be used to yield higher agreement scores between human and automatic assessments of translation quality. In our future work, we wish to experiment with more flexible merging algorithms and to integrate better the top-down and bottom-up processes that are used to induce FSAs. We also wish to extract more abstract paraphrase patterns from the current representation. Such patterns are more likely to get reused \u2013 which would help us get reliable statistics for them in the extraction phase, and also have a better chance of being applicable to unseen data. ", "summary_sents": ["We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets.", "These FSAs are good representations of paraphrases.", "They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets.", "Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations.", "We describe a syntax-based algorithm that builds word lattices from parallel translations which can be used to generate new para phrases.", "We propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences."]}
{"title": "Assigning Time-Stamps To Event-Clauses", "abstract": "We describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation. We describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references. Evaluations show a performance of 52%, compared to humans. ", "introduction": "Linguists who have analyzed news stories (Schokkenbroek,1999; Bell,1997; Ohtsuka and Brewer,1992, etc.) noticed that \u201cnarratives1 are about more than one event and these events are temporally ordered. Though it seems most logical to recapitulate events in the order in which they happened, i.e. in chronological order, the events are often presented in a different sequence\u201d. The same paper states that \u201cit is important to reconstruct the underlying event order2 for narrative analysis to assign meaning to the sequence in which the events are narrated at the level of discourse structure....If the underlying event structure cannot be reconstructed, it may well be impossible to understand the narrative at all, let alone assign meaning to its structure\u201d. Several psycholinguistic experiments show the influence of event-arrangement in news stories on the ease of comprehension by readers. Duszak (1991) had readers reconstruct a news story from the randomized sentences. According to his experiments readers have a default strategy by which\u2014in the absence of cues to the contrary\u2014they re-impose chronological order on events in the discourse. The problem of reconstructing the chronological order of events becomes more complicated if we have to deal with separate news stories, written at different times and describing the development of some situation, as is the case for multidocument summarization. By judicious definition, one can make this problem easy or hard. Selecting only specific items to assign time-points to, and then measuring correctness on them alone, may give high performance but leave much of the text unassigned. We address the problem of assigning a time-point to every clause in the text. Our approach is to break the news stories into their constituent events and to assign timestamps\u2014either time-points or time-intervals\u2014to these events. When assigning time-stamps we analyze both implicit time references (mainly through the tense system) and explicit ones (temporal adverbials) such as \u2018on Monday\u2019, \u2018in 1998\u2019, etc. The result of the work is a prototype program which takes as input set of news stories broken into separate sentences and produces as output a text that combines all the events from all the articles, organized in chronological order. ", "conclusion": "Bell (1997) notices \u201cmore research is needed on the effects of time structure on news comprehension. The hypothesis that the noncanonical news format does adversely affect understanding is a reasonable one on the basis of comprehension research into other narrative genres, but the degree to which familiarity with news models may mitigate these problems is unclear\u201d. This research can greatly improve the performance of time-stamper and might lead to a list of machine learning rules for time detection. In this paper we made an attempt to not just analyze and decode temporal expressions but to apply this analysis throughout the whole text and assign time-stamps to such type of clauses, which later could be used as separate sentences in various natural language applications, for example in multidocument summarization. text number of manually number of time point percentage of number created event-clauses correctly assigned to correct manually created clauses assignment target 1 7 6 85.71 target 2 27 20 74.07 target 3 5 4 80.00 target 4 28 26 92.85 target 5 33 30 90.91 target 6 58 37 63.79 Total 158 123 77.85 ", "summary_sents": ["We describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation.", "We describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references.", "Evaluations show a performance of 52%, compared to humans.", "We infer time values based on the most recently assigned date of the date of the article."]}
{"title": "Online Large-Margin Training Of Dependency Parsers", "abstract": "We present an effective training algorithm for linearly-scored dependency parsers that implements online largemargin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements. ", "introduction": "Research on training parsers from annotated data has for the most part focused on models and training algorithms for phrase structure parsing. The best phrase-structure parsing models represent generatively the joint probability P(x, y) of sentence x having the structure y (Collins, 1999; Charniak, 2000). Generative parsing models are very convenient because training consists of computing probability estimates from counts of parsing events in the training set. However, generative models make complicated and poorly justified independence assumptions and estimations, so we might expect better performance from discriminatively trained models, as has been shown for other tasks like document classification (Joachims, 2002) and shallow parsing (Sha and Pereira, 2003). Ratnaparkhi\u2019s conditional maximum entropy model (Ratnaparkhi, 1999), trained to maximize conditional likelihood P(y|x) of the training data, performed nearly as well as generative models of the same vintage even though it scores parsing decisions in isolation and thus may suffer from the label bias problem (Lafferty et al., 2001). Discriminatively trained parsers that score entire trees for a given sentence have only recently been investigated (Riezler et al., 2002; Clark and Curran, 2004; Collins and Roark, 2004; Taskar et al., 2004). The most likely reason for this is that discriminative training requires repeatedly reparsing the training corpus with the current model to determine the parameter updates that will improve the training criterion. The reparsing cost is already quite high for simple context-free models with O(n3) parsing complexity, but it becomes prohibitive for lexicalized grammars with O(n5) parsing complexity. Dependency trees are an alternative syntactic representation with a long history (Hudson, 1984). Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al., 2002) and machine translation (Ding and Palmer, 2005). Yet, they can be parsed in O(n3) time (Eisner, 1996). Therefore, dependency parsing is a potential \u201csweet spot\u201d that deserves investigation. We focus here on projective dependency trees in which a word is the parent of all of its arguments, and dependencies are non-crossing with respect to word order (see Figure 1). However, there are cases where crossing dependencies may occur, as is the case for Czech (Haji\u02c7c, 1998). Edges in a dependency tree may be typed (for instance to indicate grammatical function). Though we focus on the simpler non-typed case, all algorithms are easily extendible to typed structures. The following work on dependency parsing is most relevant to our research. Eisner (1996) gave a generative model with a cubic parsing algorithm based on an edge factorization of trees. Yamada and Matsumoto (2003) trained support vector machines (SVM) to make parsing decisions in a shift-reduce dependency parser. As in Ratnaparkhi\u2019s parser, the classifiers are trained on individual decisions rather than on the overall quality of the parse. Nivre and Scholz (2004) developed a history-based learning model. Their parser uses a hybrid bottom-up/topdown linear-time heuristic parser and the ability to label edges with semantic types. The accuracy of their parser is lower than that of Yamada and Matsumoto (2003). We present a new approach to training dependency parsers, based on the online large-margin learning algorithms of Crammer and Singer (2003) and Crammer et al. (2003). Unlike the SVM parser of Yamada and Matsumoto (2003) and Ratnaparkhi\u2019s parser, our parsers are trained to maximize the accuracy of the overall tree. Our approach is related to those of Collins and Roark (2004) and Taskar et al. (2004) for phrase structure parsing. Collins and Roark (2004) presented a linear parsing model trained with an averaged perceptron algorithm. However, to use parse features with sufficient history, their parsing algorithm must prune heuristically most of the possible parses. Taskar et al. (2004) formulate the parsing problem in the large-margin structured classification setting (Taskar et al., 2003), but are limited to parsing sentences of 15 words or less due to computation time. Though these approaches represent good first steps towards discriminatively-trained parsers, they have not yet been able to display the benefits of discriminative training that have been seen in namedentity extraction and shallow parsing. Besides simplicity, our method is efficient and accurate, as we demonstrate experimentally on English and Czech treebank data. ", "conclusion": "", "summary_sents": ["We present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996).", "The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements.", "We have achieved parsers with O(n3) time complexity without the grammar constant.", "We use the prefix of each word form instead of word form itself as features.", "Our dependency parser achieves accuracy as good as Charniak (2000) with speed ten times faster than Collins (1997) and four times faster than Charniak (2000)."]}
{"title": "Class-Based N-Gram Models Of Natural Language", "abstract": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. ", "introduction": "", "conclusion": "", "summary_sents": ["We address the problem of predicting a word from previous words in a sample of text.", "In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words.", "We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.", "We propose a window method introducing the concept of semantic stickiness of two words as the relatively frequent close occurrence between them (less than 500 words distance)."]}
{"title": "Hedge Trimmer: A Parse-And-Trim Approach To Headline Generation", "abstract": "and Abstracts for Nice Summaries, In Workon Automatic Philadelphia, PA, pp. 9-14. Edmundson, H. (1969). \u201cNew methods in automatic of the 16(2). Grefenstett, G. (1998). Producing intelligent telegraphic text reduction to provide an audio scanning serfor the blind. In Notes of the AIII Spring on Intelligent Text Summarization, ", "introduction": "In this paper we present Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story by removing constituents from a parse tree of the first sentence until a length threshold has been reached. Linguistically-motivated heuristics guide the choice of which constituents of a story should be preserved, and which ones should be deleted. Our focus is on headline generation for English newspaper texts, with an eye toward the production of document surrogates\u2014for cross-language information retrieval\u2014and the eventual generation of readable headlines from speech broadcasts. In contrast to original newspaper headlines, which are often intended only to catch the eye, our approach produces informative abstracts describing the main theme or event of the newspaper article. We claim that the construction of informative abstracts requires access to deeper linguistic knowledge, in order to make substantial improvements over purely statistical approaches. In this paper, we present our technique for producing headlines using a parse-and-trim approach based on the BBN Parser. As described in Miller et al. (1998), the BBN parser builds augmented parse trees according to a process similar to that described in Collins (1997). The BBN parser has been used successfully for the task of information extraction in the SIFT system (Miller et al., 2000). The next section presents previous work in the area of automatic generation of abstracts. Following this, we present feasibility tests used to establish the validity of an approach that constructs headlines from words in a story, taken in order and focusing on the earlier part of the story. Next, we describe the application of the parse-and-trim approach to the problem of headline generation. We discuss the linguistically-motivated heuristics we use to produce results that are headlinelike. Finally, we evaluate Hedge Trimmer by comparing it to our earlier work on headline generation, a probabilistic model for automatic headline generation (Zajic et al, 2002). In this paper we will refer to this statistical system as HMM Hedge We demonstrate the effectiveness of our linguistically-motivated approach, Hedge Trimmer, over the probabilistic model, HMM Hedge, using both human evaluation and automatic metrics. ", "conclusion": "We have shown the effectiveness of constructing headlines by selecting words in order from a newspaper story. The practice of selecting words from the early part of the document has been justified by analyzing the behavior of humans doing the task, and by automatic evaluation of a system operating on a similar principle. We have compared two systems that use this basic technique, one taking a statistical approach and the other a linguistic approach. The results of the linguistically motivated approach show that we can build a working system with minimal linguistic knowledge and circumvent the need for large amounts of training data. We should be able to quickly produce a comparable system for other languages, especially in light of current multi-lingual initiatives that include automatic parser induction for new languages, e.g. the TIDES initiative. We plan to enhance Hedge Trimmer by using a language model of Headlinese, the language of newspaper headlines (M\u00e5rdh 1980) to guide the system in which constituents to remove. We Also we plan to allow for morphological variation in verbs to produce the present tense headlines typical of Headlinese. Hedge Trimmer will be installed in a translingual detection system for enhanced display of document surrogates for cross-language question answering. This system will be evaluated in upcoming iCLEF conferences. ", "summary_sents": ["This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline.", "We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story.", "In addition, we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches.", "Our approach focuses on extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length."]}
{"title": "Discriminative Training And Maximum Entropy Models For Statistical Machine Translation", "abstract": "We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach. ", "introduction": "We are given a source (\u2018French\u2019) sentence fJ1 = f1, ... , fj, ... , fJ, which is to be translated into a target (\u2018English\u2019) sentence eI1 = e1, ... , ei, ... , eI. Among all possible target sentences, we will choose the sentence with the highest probability:1 The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. According to Bayes\u2019 decision rule, we can equivalently to Eq. 1 perform the following maximization: This approach is referred to as source-channel approach to statistical MT. Sometimes, it is also referred to as the \u2018fundamental equation of statistical MT\u2019 (Brown et al., 1993). Here, Pr(eI1) is the language model of the target language, whereas Pr(fJ1 |eI1) is the translation model. Typically, Eq. 2 is favored over the direct translation model of Eq. 1 with the argument that it yields a modular approach. Instead of modeling one probability distribution, we obtain two different knowledge sources that are trained independently. The overall architecture of the source-channel approach is summarized in Figure 1. In general, as shown in this figure, there may be additional transformations to make the translation task simpler for the algorithm. Typically, training is performed by applying a maximum likelihood approach. If the language model Pr(eI1) = p\u03b3(eI1) depends on parameters \u03b3 and the translation model Pr(fJ1 |eI1) = p\u03b8(fJ1 |eI1) depends on parameters \u03b8, then the optimal parameter values are obtained by maximizing the likelihood on a parallel training corpus fS1 , eS1 (Brown et al., 1993): We obtain the following decision rule: instead of Eq. 5 (Och et al., 1999): State-of-the-art statistical MT systems are based on this approach. Yet, the use of this decision rule has various problems: Here, we replaced p\u02c6\ufffd(fJ1 |ei) by p\u02c6\ufffd(ei|fJ1 ). From a theoretical framework of the sourcechannel approach, this approach is hard to justify. Yet, if both decision rules yield the same translation quality, we can use that decision rule which is better suited for efficient search. As alternative to the source-channel approach, we directly model the posterior probability Pr(ei|fJ1 ). An especially well-founded framework for doing this is maximum entropy (Berger et al., 1996). In this framework, we have a set of M feature functions hm(ei, fJ1 ), m = 1, ... , M. For each feature function, there exists a model parameter am, m = 1, ... , M. The direct translation probability is given the following two feature functions: This approach has been suggested by (Papineni et al., 1997; Papineni et al., 1998) for a natural language understanding task. We obtain the following decision rule: Hence, the time-consuming renormalization in Eq. 8 is not needed in search. The overall architecture of the direct maximum entropy models is summarized in Figure 2. Interestingly, this framework contains as special case the source channel approach (Eq. 5) if we use and set A1 = A2 = 1. Optimizing the corresponding parameters A1 and A2 of the model in Eq. 8 is equivalent to the optimization of model scaling factors, which is a standard approach in other areas such as speech recognition or pattern recognition. The use of an \u2018inverted\u2019 translation model in the unconventional decision rule of Eq. 6 results if we use the feature function log Pr(eI1|fJ1 ) instead of log Pr(fJ1 |eI1). In this framework, this feature can be as good as log Pr(fJ1 |eI1). It has to be empirically verified, which of the two features yields better results. We even can use both features log Pr(eI1|fJ1 ) and log Pr(fJ1 |eI1), obtaining a more symmetric translation model. As training criterion, we use the maximum class posterior probability criterion: This corresponds to maximizing the equivocation or maximizing the likelihood of the direct translation model. This direct optimization of the posterior probability in Bayes decision rule is referred to as discriminative training (Ney, 1995) because we directly take into account the overlap in the probability distributions. The optimization problem has one global optimum and the optimization criterion is convex. Typically, the probability Pr(fJ1 |eI1) is decomposed via additional hidden variables. In statistical alignment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is introduced as a hidden variable: ", "conclusion": "We have presented a framework for statistical MT for natural languages, which is more general than the widely used source-channel approach. It allows a baseline MT system to be extended easily by adding new feature functions. We have shown that a baseline statistical MT system can be significantly improved using this framework. There are two possible interpretations for a statistical MT system structured according to the sourcechannel approach, hence including a model for Pr(ei) and a model for Pr(fi Iei). We can interpret it as an approximation to the Bayes decision rule in Eq. 2 or as an instance of a direct maximum entropy model with feature functions log Pr(ei) and log Pr(fi |ei). As soon as we want to use model scaling factors, we can only do this in a theoretically justified way using the second interpretation. Yet, the main advantage comes from the large number of additional possibilities that we obtain by using the second interpretation. An important open problem of this approach is the handling of complex features in search. An interesting question is to come up with features that allow an efficient handling using conventional dynamic programming search algorithms. In addition, it might be promising to optimize the parameters directly with respect to the error rate of the MT system as is suggested in the field of pattern and speech recognition (Juang et al., 1995; Schl\u00a8uter and Ney, 2001). ", "summary_sents": ["We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.", "All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables.", "This approach allows a baseline machine translation system to be extended easily by adding new feature functions.", "We show that a baseline statistical machine translation system is significantly improved using this approach."]}
{"title": "Immediate-Head Parsing For Language Models", "abstract": "We present two language models based upon an \u201cimmediate-head\u201d parser \u2014 our name for a parser that conditions events below a constituent head of While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammarbased language model. For the better of our two models these improvements are 24% and 14% respectively. We also suggest that improvement of the underlying parser should significantly improve the model\u2019s perplexity and that even in the near term there is a lot of potential for improvement in immediatehead language models. are what we will call parsers in that all of the properties of the immedescendants of a constituent assigned probabilities that are conditioned on the lexical of For example, in Figure 1 the probability the into np pp conditioned on head of the \u201cput\u201d, as are the choices of the under the i.e., \u201cball\u201d (the head of and \u201cin\u201d (the head of the It is the experience of the statistical parsing community that immediate-head parsers are the most accurate we can design. It is also worthy of note that many of these [1,3,6,7] are that is, for a try to find the parse by Equation 1: = arg (1) This is interesting because insofar as they comthese parsers define a language-model in that they can (in principle) assign a probability to all possible sentences in the language by com ", "introduction": "", "conclusion": "We have presented two grammar-based language models, both of which significantly improve upon both the trigram model baseline for the task (by 24% for the better of the two) and the best previous grammar-based language model (by 14%). Furthermore we have suggested that improvement of the underlying parser should improve the model\u2019s perplexity still further. We should note, however, that if we were dealing with standard Penn Tree-bank Wall-StreetJournal text, asking for better parsers would be easier said than done. While there is still some progress, it is our opinion that substantial improvement in the state-of-the-art precision/recall figures (around 90%) is unlikely in the near future.3 However, we are not dealing with standard tree-bank text. As pointed out above, the text in question has been \u201cspeechified\u201d by removing punctuation and capitalization, and \u201csimplified\u201d by allowing only a fixed vocabulary of 10,000 words (replacing all the rest by the symbol \u201cUNK\u201d), and replacing all digits and symbols by the symbol \u201cN\u201d. We believe that the resulting text grossly underrepresents the useful grammatical information available to speech-recognition systems. First, we believe that information about rare or even truly unknown words would be useful. For example, when run on standard text, the parser uses ending information to guess parts of speech [3]. Even if we had never encountered the word \u201cshowboating\u201d, the \u201cing\u201d ending tells us that this is almost certainly a progressive verb. It is much harder to determine this about UNK.4 Secondly, while punctuation is not to be found in speech, prosody should give us something like equivalent information, perhaps even better. Thus significantly better parser performance on speechderived data seems possible, suggesting that highperformance trigram-less language models may be within reach. We believe that the adaptation of prosodic information to parsing use is a worthy topic for future research. Finally, we have noted two objections to immediate-head language models: first, they complicate left-to-right search (since heads are often to the right of their children) and second, they cannot be tightly integrated with trigram models. The possibility of trigram-less language models makes the second of these objections without force. Nor do we believe the first to be a permanent disability. If one is willing to provide sub-optimal probability estimates as one proceeds left-to-right and then amend them upon seeing the true head, left-to-right processing and immediatehead parsing might be joined. Note that one of the cases where this might be worrisome, early words in a base noun-phrase could be conditioned upon a head which comes several words later, has been made significantly less problematic by our revised definition of heads inside noun-phrases. We believe that other such situations can be brought into line as well, thus again taming the search problem. However, this too is a topic for future research. ", "summary_sents": ["We present two language models based upon an \u201cimmediate-head\u201d parser \u2014 our name for a parser that conditions all events below a constituent c upon the head of c.", "While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology.", "The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model.", "For the better of our two models these improvements are 24% and 14% respectively.", "We also suggest that improvement of the underlying parser should significantly improve the model\u2019s perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models.", "The model presented identifies both syntactic structural and lexical dependencies that aid in language modeling.", "These contexts include syntactic structure such as parent and grandparent category labels as well as lexical items such as the head of the parent or the head of a sibling constituent."]}
{"title": "Pseudo-Projectivity A Polynomially Parsable Non-Projective Dependency Grammar", "abstract": "", "introduction": "Dependency grammar has a long tradition in syntactic theory, dating back to at least Tesniere's work from the thirties.' Recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not. One problem that has posed an impediment to more wide-spread acceptance of dependency grammars is the fact that there is no computationally tractable version of dependency grammar which is not restricted to projective analyses. However, it is well known that there are some syntactic phenomena (such as wh-movement in English or clitic climbing in Romance) that require nonprojective analyses. In this paper, we present a form of projectivity which we call pseudoprojectivity, and we present a generative stringrewriting formalism that can generate pseudoprojective analyses and which is polynomially parsable. The paper is structured as follows. In Section 2, we introduce our notion of pseudoprojectivity. We briefly review a previously proposed formalization of projective dependency grammars in Section 3. In Section 4, we extend this formalism to handle pseudo-projectivity. We informally present a parser in Section 5. ", "conclusion": "", "summary_sents": ["The pseudo-projective grammar we propose can be parsed in polynomial time and captures non-local dependencies through a form of gap-threading, but the structures generated by the grammar are strictly projective."]}
{"title": "Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales", "abstract": "address the wherein rather than simply decide whether a review is \u201cthumbs up\u201d or \u201cthumbs down\u201d, as in previous sentiment analysis work, one must determine an author\u2019s evaluation with respect to a multi-point scale (e.g., one to five \u201cstars\u201d). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, \u201cthree stars\u201d is intuitively closer to \u201cfour stars\u201d than to \u201cone star\u201d. We first evaluate human performance at task. Then, we apply a based on a labeling formulation of the problem, that alters a given-ary classifier\u2019s output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem. ", "introduction": "There has recently been a dramatic surge of interest in sentiment analysis, as more and more people become aware of the scientific challenges posed and the scope of new applications enabled by the processing of subjective language. (The papers collected by Qu, Shanahan, and Wiebe (2004) form a representative sample of research in the area.) Most prior work on the specific problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative (Turney, 2002; Pang, Lee, and Vaithyanathan, 2002; Dave, Lawrence, and Pennock, 2003; Yu and Hatzivassiloglou, 2003). But it is often helpful to have more information than this binary distinction provides, especially if one is ranking items by recommendation or comparing several reviewers\u2019 opinions: example applications include collaborative filtering and deciding which conference submissions to accept. Therefore, in this paper we consider generalizing to finer-grained scales: rather than just determine whether a review is \u201cthumbs up\u201d or not, we attempt to infer the author\u2019s implied numerical rating, such as \u201cthree stars\u201d or \u201cfour stars\u201d. Note that this differs from identifying opinion strength (Wilson, Wiebe, and Hwa, 2004): rants and raves have the same strength but represent opposite evaluations, and referee forms often allow one to indicate that one is very confident (high strength) that a conference submission is mediocre (middling rating). Also, our task differs from ranking not only because one can be given a single item to classify (as opposed to a set of items to be ordered relative to one another), but because there are settings in which classification is harder than ranking, and vice versa. One can apply standard-ary classifiers or regression to this rating-inference problem; independent work by Koppel and Schler (2005) considers such methods. But an alternative approach that explicitly incorporates information about item similarities together with label similarity information (for instance, \u201cone star\u201d is closer to \u201ctwo stars\u201d than to \u201cfour stars\u201d) is to think of the task as one of metric labeling (Kleinberg and Tardos, 2002), where label relations are encoded via a distance metric. This observation yields a meta-algorithm, applicable to both semi-supervised (via graph-theoretic techniques) and supervised settings, that alters a given -ary classifier\u2019s output so that similar items tend to be assigned similar labels. In what follows, we first demonstrate that humans can discern relatively small differences in (hidden) evaluation scores, indicating that rating inference is indeed a meaningful task. We then present three types of algorithms \u2014 one-vs-all, regression, and metric labeling \u2014 that can be distinguished by how explicitly they attempt to leverage similarity between items and between labels. Next, we consider what item similarity measure to apply, proposing one based on the positive-sentence percentage. Incorporating this new measure within the metriclabeling framework is shown to often provide significant improvements over the other algorithms. We hope that some of the insights derived here might apply to other scales for text classifcation that have been considered, such as clause-level opinion strength (Wilson, Wiebe, and Hwa, 2004); affect types like disgust (Subasic and Huettner, 2001; Liu, Lieberman, and Selker, 2003); reading level (Collins-Thompson and Callan, 2004); and urgency or criticality (Horvitz, Jacobs, and Hovel, 1999). ", "conclusion": "", "summary_sents": ["We address the rating-inference problem, wherein rather than simply decide whether a review is thumbs up or thumbs down, as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five \"stars\").", "This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, three stars is intuitively closer to four stars than to one star.", "We first evaluate human performance at the task.", "Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels.", "We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.", "We created a sentiment-annotated dataset consisting of movie reviews to train a classifier for identifying positive sentences in a full length review."]}
{"title": "Statistical Machine Translation With Scarce Resources Using Morpho-Syntactic Information", "abstract": "In statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models. In particular, existing statistical systems for machine translation often treat different inflectedforms of the same lemma as if they were independent ofone another. The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms. We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words. In addition, we introduce sentence-level restructuring transformations which aim at the assimilation of word order in related sentences. We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation. The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested: We were able to reduce the amount of bilingual training data to less than 10% of the original corpus, while losing only 1.6% in translation quality. The improvement of the translation results is demonstrated on two German-English corpora taken from the Uerbmobil task and the Nespole! task. ", "introduction": "", "conclusion": "", "summary_sents": ["In statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models.", "In particular, existing statistical systems for machine translation often treat different inflected forms of the same lemma as if they were independent of one another.", "The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms.", "We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words.", "In addition, we introduce sentence-level restructuring transformations which aim at the assimilation of word order in related sentences.", "We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation.", "The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested: We were able to reduce the amount of bilingual training data to less than 10% of the original corpus, while losing only 1.6% in translation quality.", "The improvement of the translation results is demonstrated on two German-English corpora taken from the Verbmobil task and the Nespole! task.", "We decompose German words into a hierarchical representation using lemmas and morphological tags, and use a MaxEnt model to combine the different levels of representation in the translation model.", "We describe a method that combines morphologically split verbs in German, and also reorders questions in English and German."]}
{"title": "Robust Bilingual Word Alignment For Machine Aided Translation", "abstract": "We have developed a new program called aligning parallel text, text such as the Canadian Hansards that are available in two or more languages. The program takes the of 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints usa version of Brown Model 2 (Brown et al., 1993), modified and extended to deal robustness issues. tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of over More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. ", "introduction": "Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query. Landauer and Littman used SVD analysis (or Latent Semantic Indexing) on the Canadian Hansards, parliamentary debates that are published in both English and French, in order to estimate a kind of soft thesaurus. They then showed that these estimates could be used to retrieve documents appropriately in the bilingual condition where the query and the document were written in different languages. We have been most interested in the terminology application. How does Microsoft, or some other software vendor, want &quot;dialog box,&quot; &quot;text box,&quot; and &quot;menu box&quot; to be translated in their manuals? Considerable time is spent on terminology questions, many of which have already been solved by other translators working on similar texts. It ought to be possible for a translator to point at an instance of &quot;dialog box&quot; in the English version of the Microsoft Windows manual and see how it was translated in the French version of the same manual. Alternatively, the translator can ask for a bilingual concordance as shown in Figure 1. A PCbased terminology reuse tool is being developed to do just exactly this. The tool depends crucially on the results of an alignment program to determine which parts of the source text correspond with which parts of the target text. In working with the translators at AT&T Language Line Services, a commercial translation service, we discovered that we needed to completely redesign our alignment programs in order to deal more effectively with texts supplied by Language Line's customers. All too often the texts are not available in electronic form, and may need to be scanned in and processed by an OCR (optical character recognition) device. Even if the texts are available in electronic form, it may not be worth the effort to clean them up by hand. Real texts are not like the Hansards; real texts are much smaller and not nearly as clean as the ideal texts that have been used in previous studies. To deal with these robustness issues, Church (1993) developed a character-based alignment method called char_align. The method was intended as a replacement for sentence-based methods (e.g., (Brown et al., 1991a; Gale and Church, 1991b; Kay and Rosenschein, 1993)), which are very sensitive to noise. This paper describes a new program, called word_align, that starts with an initial &quot;rough&quot; alignment (e.g., the output of char_a/ign or a sentence-based alignment method), and produces improved alignments by exploiting constraints at the word-level. The alignment algorithm consists of two steps: (1) estimate translation probabilities, and (2) use these probabilities to search for most probable alignment path. The two steps are described in the following section. ", "conclusion": "Compared with other word alignment algorithms (Brown et al., 1993; Gale and Church, 1991a), word_align does not require sentence alignment as input, and was shown to produce useful alignments for small and noisy corpora. Its robustness was achieved by modifying Brown et al. 's Model 2 to handle an initial &quot;rough&quot; alignment, reducing the number of parameters and introducing a dependency between alignments of adjacent words. Taking the output of char_align as input, word_align produces significantly better, wordlevel, alignments on the kind of corpora that are typically available to translators. This improvement increased the rate of constructing bilingual terminology lexicons at AT&T Language Line Services by a factor of 2-3. In addition, the alignments may also be helpful to developers of lexicons for machine translation systems. Word_align thus provides an example how a model such as Brown et al. 's Model 2, that was originally designed for research in statistical machine translation, can be modified to achieve practical, though less ambitious, goals in the near term. ", "summary_sents": ["We have developed a new program called word_align for aligning parallel text, text such as the Canadian Hansards that are available in two or more languages.", "The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown el al.'s Model 2 (Brown et al., 1993), modified and extended to deal with robustness issues.", "Word_align was tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992).", "The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone.", "More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology.", "We show that knowledge of target-text length is not crucial to the model's performance."]}
{"title": "Bootstrapping Path-Based Pronoun Resolution", "abstract": "We present an approach to pronoun resolution based on syntactic paths. Through a simple bootstrapping procedure, we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities. This path information enables us to handle previously challenging resolution instances, and also robustly addresses traditional syntactic coreference constraints. Highly coreferent paths also allow mining of precise probabilistic gender/number information. We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier. Significant gains in performance are observed on several datasets. ", "introduction": "Pronoun resolution is a difficult but vital part of the overall coreference resolution task. In each of the following sentences, a pronoun resolution system must determine what the pronoun his refers to: In (1), John and his corefer. In (2), his refers to some other, perhaps previously evoked entity. Traditional pronoun resolution systems are not designed to distinguish between these cases. They lack the specific world knowledge required in the second instance \u2013 the knowledge that a person does not usually explicitly need his own support. We collect statistical path-coreference information from a large, automatically-parsed corpus to address this limitation. A dependency path is defined as the sequence of dependency links between two potentially coreferent entities in a parse tree. A path does not include the terminal entities; for example, \u201cJohn needs his support\u201d and \u201cHe needs their support\u201d have the same syntactic path. Our algorithm determines that the dependency path linking the Noun and pronoun is very likely to connect coreferent entities for the path \u201cNoun needs pronoun\u2019s friend,\u201d while it is rarely coreferent for the path \u201cNoun needs pronoun\u2019s support.\u201d This likelihood can be learned by simply counting how often we see a given path in text with an initial Noun and a final pronoun that are from the same/different gender/number classes. Cases such as \u201cJohn needs her support\u201d or \u201cThey need his support\u201d are much more frequent in text than cases where the subject noun and pronoun terminals agree in gender/number. When there is agreement, the terminal nouns are likely to be coreferent. When they disagree, they refer to different entities. After a sufficient number of occurrences of agreement or disagreement, there is a strong statistical indication of whether the path is coreferent (terminal nouns tend to refer to the same entity) or non-coreferent (nouns refer to different entities). We show that including path coreference information enables significant performance gains on three third-person pronoun resolution experiments. We also show that coreferent paths can provide the seed information for bootstrapping other, even more important information, such as the gender/number of noun phrases. ", "conclusion": "We have introduced a novel feature for pronoun resolution called path coreference, and demonstrated its significant contribution to a state-of-theart pronoun resolution system. This feature aids coreference decisions in many situations not handled by traditional coreference systems. Also, by bootstrapping with the coreferent paths, we are able to build the most complete and accurate table of probabilistic gender information yet available. Preliminary experiments show path coreference bootstrapping can also provide a means of identifying pleonastic pronouns, where pleonastic neutral pronouns are often followed in a dependency path by a terminal noun of different gender, and cataphoric constructions, where the pronouns are often followed by nouns of matching gender. ", "summary_sents": ["We present an approach to pronoun resolution based on syntactic paths.", "Through a simple bootstrapping procedure, we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities.", "This path information enables us to handle previously challenging resolution instances, and also robustly addresses traditional syntactic coreference constraints.", "Highly coreferent paths also allow mining of precise probabilistic gender/number information.", "We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier.", "Significant gains in performance are observed on several datasets.", "Given an automatically parsed corpus, we extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate antecedent, and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent.", "We show that learned gender is the most important feature in their pronoun resolution systems.", "We achieve achieve state-of-the-art noun gender classification performance, and we make the database of the obtained noun genders available online.", "We build a statistical model from paths that include the lemma of the intermediate tokens, but replace the end nodes with noun, pronoun, or pronoun-self for nouns, pronouns, and reflexive pronouns, respectively."]}
{"title": "Recognising Textual Entailment With Logical Inference", "abstract": "We use logical inference techniques for recognising textual entailment. As the performance of theorem proving turnsout to be highly dependent on not read ily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment. Finally, we use machine learning to combine these deep semantic analysis techniques with simpleshallow word overlap; the resulting hy brid model achieves high accuracy on the RTE testset, given the state of the art. Ourresults also show that the different techniques that we employ perform very dif ferently on some of the subsets of the RTE corpus and as a result, it is useful to use the nature of the dataset as a feature. ", "introduction": "Recognising textual entailment (RTE) is the task to find out whether some text T entails a hypothesis H. This task has recently been the focus of a challenge organised by the PASCAL network in 2004/5.1 In Example 1550 H follows from T whereas this is not the case in Example 731. 1All examples are from the corpus released as part of the RTE challenge. It is downloadable from http://www.pascal-network.org/Challenges/RTE/. The exam ple numbers have also been kept. Each example is marked for entailment as TRUE if H follows from T and FALSE otherwise. The dataset is described in Section 4.1. Example: 1550 (TRUE) T: In 1998, the General Assembly of the Nippon Sei Ko Kai (Anglican Church in Japan) voted to accept female priests. H: The Anglican church in Japan approved the ordination of women. Example: 731 (FALSE) T: The city Tenochtitlan grew rapidly and was the center of the Aztec?s great empire. H: Tenochtitlan quickly spread over the island, marshes, and swamps. The recognition of textual entailment is without doubt one of the ultimate challenges for any NLPsystem: if it is able to do so with reasonable accuracy, it is clearly an indication that it has some thor ough understanding of how language works. Indeed, recognising entailment bears similarities to Turing?s famous test to assess whether machines can think, as access to different sources of knowledge and theability to draw inferences seem to be among the primary ingredients for an intelligent system. Moreover, many NLP tasks have strong links to entailment: in summarisation, a summary should be en tailed by the text; paraphrases can be seen as mutualentailment between T and H; in IE, the extracted in formation should also be entailed by the text.In this paper, we discuss two methods for recog nising textual entailment: a shallow method relyingmainly on word overlap (Section 2), and deep se mantic analysis, using state-of-the-art off-the-shelf inference tools, namely a theorem prover and amodel builder (Section 3). These tools rely on Dis course Representation Structures for T and H as well as lexical and world knowledge. To our knowledge, few approaches to entailment currently use theorem provers and none incorporate model building (see 628 Section 5 for a discussion of related work). Both methods are domain-independent to increasetransferrability and have not been tailored to any par ticular test suite. In Section 4 we test their accuracy and robustness on the RTE datasets as one of the few currently available datasets for textual inference. We also combine the two methods in a hybrid approach using machine learning. We discuss particularly the following questions:? Can the methods presented improve significantly over the baseline and what are the per formance differences between them? Does thehybrid system using both shallow and deep se mantic analysis improve over the individual use of these methods? How far does deep semantic analysis suffer from a lack of lexical and world knowledge and how can we perform logical inference in the face of potentially large knowledge gaps?? How does the design of the test suite affect per formance? Are there subsets of the test suitethat are more suited to any particular textual en tailment recognition method? ", "conclusion": "Relying on theorem proving as a technique for de termining textual entailment yielded high precision but low recall due to a general lack of appropriate background knowledge. We used model building as an innovative technique to surmount this problem toa certain extent. Still, it will be unavoidable to incor porate automatic methods for knowledge acquisition to increase the performance of our approach. Future work will be directed to the acquisition of targeted paraphrases that can be converted into background knowledge in the form of axioms. Our hybrid approach combines shallow analysis with both theorem proving and model building and achieves high accuracy scores on the RTE dataset compared to other systems that we are aware of. The results for this approach also indicate that (a) the choice of entailment recognition methods might have to vary according to the dataset design and/or application and (b) that a method that wants to achieve robust performance across different datasetsmight need the integration of several different entail ment recognition methods as well as an indicator of design methodology or application. Thus, although test suites establish a controlledway of assessing textual entailment detection sys tems, the importance of being able to predict textual entailment in NLP might be better justified usingtask-based evaluation. This can be achieved by in corporating them in QA or summarisation systems.Acknowledgements We would like to thank Mirella Lapata and Malvina Nissim as well as three anonymous review ers for their comments on this paper. We are also grateful to Valentin Jijkoun and Bonnie Webber for discussion and Steve Clark and James Curran for help on using the CCG-parser. ", "summary_sents": ["We use logical inference techniques for recognising textual entailment.", "As the performance of theorem proving turns out to be highly dependent on not readily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment.", "Finally, we use machine learning to combine these deep semantic analysis techniques with simple shallow word overlap; the resulting hybrid model achieves high accuracy on the RTE test set, given the state of the art.", "Our results also show that the different techniques that we employ perform very differently on some of the subsets of the RTE corpus and as a result, it is useful to use the nature of the dataset as a feature.", "It is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs \"no entailment\" for almost all pairs.", "Our system is based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource."]}
{"title": "Corpus Based PP Attachment Ambiguity Resolution With A Semantic Dictionary", "abstract": "This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity. We propose a new supervised learning method for PPattachment based on a semantically tagged corpus. Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags. We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods. ", "introduction": "", "conclusion": "", "summary_sents": ["This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity.", "We propose a new supervised learning method for PP- attachment based on a semantically tagged corpus.", "Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags.", "We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods.", "we developed a customized, explicit WSD algorithm as part of their decision tree system."]}